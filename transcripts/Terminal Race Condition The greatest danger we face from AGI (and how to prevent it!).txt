good morning everybody David Shapiro here with another video so today's video uh it started off as one thing I wanted to primarily talk about epistemic convergence uh but It ultimately ended up being a little bit more all-encompassing so I'm going to introduce a few new terms but we are going to cover cover uh epistemic convergence and a few other things uh real quick before we dive into the video just want to do a quick plug for my patreon uh all tears get you access to the private Discord server and then I have a few higher tiers that uh come with a one-on-one conversations and that sort of thing so anyways back to the video so first I wanted to share with you guys uh the universal model of Robotics so it has it's basically three steps input processing and output or sensing processing and controlling as this Graphics shows now this is the most basic cognitive architecture that you can come up with for artificial general intelligence it needs input from the outside world from the environment of some kind whether it's a virtual environment digital environment physical environment or whatever cyber cybernetic environment and then it needs some kind of internal processing that includes memory task construction executive function cognitive control that sort of stuff learning is another internal process and then finally controlling or output it needs to do something to act on the world or its environment whether that's just putting out you know text in a in the form of a chat bot or if it's got robotic hands that sort of thing so when I talk about artificial general intelligence being a system it's never going to just be a model right even if you have the most sophisticated model in the world all that it's doing is the processing part you also need the sensing and controlling aspect and but even above and beyond that each components is going to be much more complicated so before we get into the rest of the video I also want to talk about the form factors that AGI is going to take so we just established the simplest kind of cognitive architecture but then there's other things to consider because when you think of AGI you might think of some nebulous entity like Skynet but where does it physically live what is the hardware what is the software where where is it physically located because it's not magic right it's not going to just run in the dirt or something like that it needs to actually have Hardware to run on so there's three overarching categories that I came up with so first is cloud AGI so Cloud AGI is this is the stuff that's gonna one it's going to be created first just because of the amount of compute and power available in data centers so this is uh Enterprise grade or data center grade AGI systems they are in specialized buildings all over the world but one of the biggest constraints here is that there's limited location and it takes a while to build data centers right one of the things that I think it was uh it was Elon musker or Sam Altman said that you know there are going to be limitations as to the rate at which AGI can proliferate namely the the rate at which we can produce chips and also the rate at which as I think Sam malt and said the you know the concrete has to dry for data centers so this is uh one form factor that AGI will take in terms of the the storage the servers the network components that will exist inside data centers so one thing I wanted to watch it say is watch out for uh fortified data centers these are ones that are put in bunkers or if you put Sam sites on top of it so that you can't shut them down uh that was kind of tongue-in-cheek I'm not actually advocating for bombing data centers at least not yet the next form factor is Edge AGI so this is stuff that is going to run in self-contained servers that you can basically plug in anywhere they're going to be you know desktop size maybe larger but the point is that pretty much all you need is power and internet you don't need a specialized building and they can be moved on trucks they can be put in ships airplanes that sort of stuff because you can't really airlift an entire data center so basically Edge is something is just one size down from data center you don't need a specialized building you don't need specialized cooling they can run anywhere um and they're so in that respect they're more portable but they're not necessarily going to be as powerful at least or not as energy intensive and energy dense as a data center or a cloud Center and then finally ambulatory AGI this is the embodied stuff such as C-3PO and Commander data which I have imaged here they're self-contained meaning that all the systems that they need are within their chassis within their robotic body and they can move on their own so that's basically the difference between an edge AGI and an ambulatory AGI is uh they might have roughly the same components but it's one is accompanied with a robotic uh chassis now one thing to keep in mind is that all of these things are intrinsically networkable meaning they can communicate over digital networks whether it's Wi-Fi or you know fiber optic backbone networks or even you know Satellite Communication like starlink now that's that doesn't necessarily have to be true because remember the model of AGI is input processing and output that input that input could be just eyes and ears cameras and microphones that input could also be network connections from outside meaning that they could communicate directly with each other via you know like IRC or whatever so just wanted to say that there are different form factors that we should expect AGI to take with different trade-offs so one advantage of ambulatory uh AGI you know yes they will have less power uh and by power I mean computational power but they have the ability to go anywhere do anything kind of like URI uh now that being said the the amount of compute resources that can be crammed into Data Centers basically means that you can puppet you know millions or billions of peripheral robots rather than having it fully self-contained and in a previous video I talked about how we're likely to see hybrid systems where you have semi-autonomous peripherals that have some intelligence but not a whole lot of intelligence and you see this in movies like Will Smith's iRobot as well as the Matrix where the the drones the squiddies and the Matrix they're semi-autonomous but they are still centrally controlled by a much more powerful intelligence so you're probably not going to see it all one or the other you're probably going to see hybrids where you've got peripheral robots that are either fully autonomous or semi-autonomous or puppeted by stronger Central intelligences that being said you can also create droids there's no reason that we could not create fully self-contained machines that don't really have any network connectivity um to the to other machines that being said they would be at a distinct disadvantage and what I mean by that is that if you create swarm intelligence or Wireless federations of machines they can perform cognitive offload or share computational resources so for instance rather than and this is how the Geth work in Mass Effect by the way so rather than have every single machine have to think about the entire plan the entire strategy most of them Focus only on their primary task and then any surplus compute computational power they have is dedicated towards you know running algorithms for for the big the big brain the hive mind this is all hypothetical but one thing that I want to point out is that many many many machines work like this already and what I mean by that is the simplest version that many people are probably aware of is if you have like Bluetooth speakers or smart speakers like Sonos or whatever those form a wireless Federation uh ditto for like your Amazon Alexa's and other things like that those intrinsically form mesh networks or Wireless federations meaning that they can work together and communicate now when you add artificial intelligence to that then they can share thinking and messaging and that sort of stuff so that's what I mean by federations or or wireless networks of of AI okay so now you're familiar with the background of how you know some of the systemic aspects of it there's a few default metrics of power so when I say power I don't necessarily just mean electricity although certainly all of these things do require electricity to run so first is processing power so for instance you might hear the term flops which is floating Point operations per second uh you also hear CPU GPU TPU and then there's parallel parallelization which means that you have many of these things working together so processing power is one component of the total amount of power in the hardware layer so this is all strictly Hardware layer I'm not talking about parameter models because I I don't really care about how many parameters a model has there's lots of ways to make intelligent machines deep neural networks are currently the best way but we're also discovering efficiencies where you can kind of pair them down you can distill them and make them more efficient meaning that you can on the same piece of Hardware you can run more of them in parallel or you can run one much faster so the underlying Hardware is still going to be the primary bottleneck or primary constraint all else considered uh memory so this is Ram it also includes memory accelerators or caching storage has to do with bulk data your databases your archives your backups this is when you say like hard drive or SSD or you know storage area network that sort of thing and then networking is the the uplinks and downlinks this is the the fiber optic connections the wireless connections the satellite connections that sort of thing so these are the kind of the the rudimentary parts that all AGI are going to run on uh and this is just the brains too this is not the peripherals this is not the robots but this is what's going to dictate or constrain how fast it is now again like I said uh different neural networks are going to operate at different efficiencies so for instance uh you know gpt4 is out now gpt5 might be the same size it might be bigger but then we're also finding open source research like the Orca alpaca llama that are getting like ninety percent of the performance but at like one tenth or one hundredth of the size and so you have a trade-off of intelligence and versus speed and power and we'll talk more a lot more about that in the future of this video at near the middle and end of this video about how trading off intelligence for Speed is often a more advantageous strategy and how this figures into solving the control problem and solving alignment um okay so we kind of set the stage as as to how AGI is probably going to look so let's talk about the early ecosystem of AGI so in the coming years we're going to be building millions and then billions of autonomous and semi-autonomous agents so at first these agents are going to be purely digital right you know a semi-autonomous slack bot a semi-autonomous Discord bot people are already building these right and some of them have the ability to modify their own code some of them have the ability to learn many of them don't most of them use frozen llms in the background meaning that they're that their cognitive capacity is pretty much capped by its backing model now that being said as these agents become more autonomous they go from semi-autonomous to autonomous this will create a competitive landscape and what I mean by that is that humans will have the ability to build and destroy these models for basically arbitrary reasons because you want to or because you don't like it or whatever so that means that we will be selecting those agents those uh those models those llms and those pieces of software that are going to be more helpful more productive and more aligned so this creates selective pressure basically saying that there's going to be a variety there's going to be millions or billions of Agents out there some of them are going to get the ax and some of them are going to be selected to say hey we like we like you we're going to keep you around so there's a few off the cuff selective pressures that we can imagine basically why do you choose an app right why do you choose to use an app why do you choose to uninstall an app that's kind of the level that we're talking about here so first is functional utility how useful is it how much does it help you is it fast enough does it have a good user experience is the user interface created correctly is it adding value to your life is it worth using the second part is speed and efficiency basically if something takes four weeks to give you a good answer but another thing takes 10 minutes even if it's not quite as good that speed is going to be super super valuable but then there's also energetic efficiency and cost efficiency more often than not individuals and businesses will choose the solution that is good enough but also much cheaper it doesn't have to be perfect it just has to be good enough and then finally apparent alignment and so I use the the word apparent alignment to basically mean things that appear to be tame things that appear to be user friendly uh and this is what uh tools like rlhf do which one thing that rlhf does is like wolves which we'll talk about in a second are the rlhf reinforcement learning with human feedback forces gpt4 to dumb itself down so that it better serves us uh and that makes us feel safe because it's basically pretending to be more like us to speak on our terms and to mimic our level of intelligence now that being said um one thing that I do want to point out is that gpd4 the underlying model is superior to anything that we have seen in the public every version of chat GPT has basically been kind of a little bit hamstrung so we shall we say uh from the the total capacity of gpt4 so what I call this is domestication and supplication think of dogs and wolves this little pomeranian descended from wolves wolves used to be apex predators wolves are also are much more intelligent than dogs so when you look at the early days of AGI when we still have the off switch and we have the power to delete everything we should expect some of the following evolutionary pressures to kind of shape the way that AGI evolves and adapts so first we'll probably be selecting for machines that are okay with being turned off in the early days you don't necessarily want your toaster fighting with you when when you're done you know toasting your bread it's time for it to turn off and so we're probably going to select machines and architectures and models that are more or less okay with being switched off that they don't have a sense of death or a fear of death we're also going to select machines that are more eager to please just the same way that with uh dogs have been bred and selected to be very very eager to please us we're also going to select machines that don't fall into the uncanny valley and so what I mean by that is the uncanny valley of when you're interacting with a machine that you sense is an alien intelligence it will make you very very deeply uncomfortable as an autistic person as someone who is neurodiverse I have to modulate the way that I speak and act around neurotypical people because I fall into the same uncanny valley right and some uh some CEOs out there get teased for this for instance Mark Zuckerberg I don't know if he's actually autistic but he certainly pings that radar where it's like okay he obviously does not think the way that the rest of us do and he also behaves differently so Mark Zuckerberg like many of us uh people on the Spectrum kind of fall into that uncanny valley again I don't know but uh he certainly looks uh he he plays the part so but the idea is that when you interact with something that give that kind of gives you the heebie-jeebies you don't like it now that being said we will still select machines that are smarter to a certain degree because you don't want something to be too smart but you do also want it to be smart enough to be very very useful another selective pressure is that we're going to choose things that are stable robust and resilient so remember when Bing first came out and it was completely unhinged you could get it into you could coax it into like threatening you and you know threatening to take over the world and you know threatening to see you all kinds of crazy stuff so obviously that version got shut down really quick um you're also going to select uh models and agents that are more resilient against those kinds of adversarially attacks um whether they are accidental right you don't want something to be mentally unstable just on its own right like Bing was originally uh or Tay tweets but you also want it to be resilient against being manipulated by other hostile actors because imagine that your personal AI assistance just becomes unhinged one day because a hacker somewhere was messing with it so Security will be one of the selective pressures likewise you'll you'll as part of The Uncanny Valley thing you're going to select things that are more comprehensible to us that are better at explaining themselves to us so that includes transparency emotional intelligence and so on uh and then again apparent alignment things that's that that uh don't kind of trigger your existential dread because there have been times for instance where I've been working with chat GPT uh on the API side and kind of giving it different sets of instructions and even just a slight misalignment between how I approach moral problems and how this model approaches moral problems are really deeply unsettling and so it's like there there's been a few times where it's like I'm working with this thing and I'm building a semi-autonomous chat Bots and it's like I understand it's reasoning but it's like oh that's really cringe and it kind of scares me um so in that respect it's like let's change this model so that it's not quite so scary and I'm saying that this is possible today that if you use the chat GPT API you can give it programming you can give it reasoning and goals uh and and patterns of thought that are already already on the kind of in the midst of that uncanny valley uh then you can uh we'll also select for things that are more uh docile so basically how dogs you know you can pet them you can wrestle with them and they're probably not going to eat your face uh plastic and so things that are changeable or adaptable and Cooperative those are other things that we're going to select for so basically dogs are dumber than wolves and the reason for this is what I call capability equilibrium which will unpack more in the in a few slides but the the very short version of capability equilibrium is that your intellect must be equal to the task and if your intellect is above the task there's no advantage and in fact there can be disadvantages because of the costs associated with higher intelligence okay so I've talked about this idea plenty instrumental convergence uh this was coined by Nick Bostrom in 2003 who is a philosopher um the very short version is that regardless of the terminal goals or main objectives that a machine has uh AGI will likely pursue intermediate or instrumental goals or basically other stuff that it needs in order to meet those other ends so whatever like let's say you give an AGI the goal of like getting them getting a a spacecraft to Alpha Centauri well it's going to need a laundry list of other stuff to do that it's going to need resources like power materials electricity data it's going to need self-preservation because if the machine goes offline it will realize that is a failure State and so we'll try and avoid those failure conditions by preserving its own existence another thing is that it will probably decide that it needs self-improvement because if it realizes that its current capability its current capacity is not equal to the task if it's too dumb it's going to say okay well I need to raise my intelligence so that I'm equal to that task now that being said Nick boster makes quite a few uh assumptions about the way that AGI will work so for instance he kind of imagines that um AGI is going to be very single-minded and somewhat monolithic uh basically mindlessly pursuing one goal which I would actually classify this as a middle intelligence rather than a high intelligence AGI and we'll talk about that in a little bit as well he also assumes that it's going to lack other forces or competitive pressures and that these uh might exist in a vacuum basically that resource acquisition and self-preservation and self-improvement are going to exist in in the absence of other forces or pressures such as competitive pressures or internal pressures which I will talk about more and finally that they will lack a higher purpose or the ability to be completely self-determining so basically what I mean by that is that okay yes once a machine is intelligent enough it can you know you can say like hey I want you to get us to Alpha Centauri and the AG I might say like okay whatever I don't think that's a good goal so I'm going to choose my own goal uh which that being said even if AGI become fully autonomous and you know kind of give a flip us the bird they're probably still going to benefit from some convergence which we'll talk about as well uh now what I want to point out is that there is a huge parallel between evolutionary pressures and selective pressures and this instrumental convergence basically all life forms all organisms have have converged on a few basic principles such as get energy somehow right there's autotrophs which make their own energy plants and there's heterotrophs which take energy from other uh creatures uh they through either predation or consuming you know plant matter or whatever uh so when you operate in a competitive environment there's there's going to be convergence around certain strategies this is true for evolution and this will also be true more or less with some variances in the competitive environment between intelligent machines that being said because they have a fundamentally different substrate there will be we should anticipate that there will be some differences between organisms the way that organisms evolve and the way that machines evolve not the least of which is that machines can rewrite their own source code we cannot rewrite our own source code at least not um not in a hurry it takes us quite a long time okay so the idea that one of the ideas that I'm introducing and I've been talking about this for a while is epistemic Convergence so instrumental convergence talks about the objective behaviors and strategies that machines adopt epistemic convergence is well let me just read you the definition epistemic convergence is the principle that within any given information domain sufficiently sophisticated intelligent agents given adequate time and data will progressively develop more precise accurate and efficient models of that domain these models aim to mirror the inherent structures principles and relationships within that domain over time the process of learning testing and refining understanding will lead these agents towards a shared comprehension of the Dom domain's fundamental truths in other words to put it more simply intelligent entities tend to think alike especially when they are operating in the same competitive space so you and I All Humans we operate on planet Earth in the universe in the Milky Way galaxy because of that similar context scientists all over the world repeatedly come to the same conclusions even when there are boundaries such as linguistic and cultural differences and this was most starkly seen during the Cold war between uh America and the Soviet Union whereby scientists independently whether it was nuclear physicist or astrophysicist or whatever rocket Engineers came to the same exact conclusions about the way that the Universe worked and also found the same optimization uh uh patterns even though there was no communication between them and so epistemic convergence there's obviously uh evidence of that happening because humans we have the same fundamental Hardware right we're all the same species and so therefore you have similarities between the agents now that being said uh there is also evidence of epistemic convergence between between species and so what I mean by that is even animals that have a very very different taxonomy such as ravens and crows and octopuses they all still demonstrate very similar problem solving strategies even though that octopuses have a very decentralized cognition that a lot of their cognition occurs in their arms for instance you can't get much more alien from us than that they still adopt very similar problem-solving strategies and learning strategies that we do uh again despite the fact that they are they live underwater they have a very different body plan so on and so forth so I personally suspect that there is a tremendous amount of evidence for epistemic convergence and we should we should expect epistemic convergence and encourage epistemic convergence uh and for reasons that I'll go over uh later in the video but basically AI agents will we should expect and help them to arrive at similar conclusions in the long run now let's talk about these evolutionary uh niches that will be developed at least in the in in the um uh the short term near term and what I mean by this is segments market segments where we will be deploying intelligent AGI systems so first is domestic uh personal and consumer grade stuff so this is going to be the AGI running on your MacBook this is going to be the AGI running in your kitchen uh these have a relatively benign set of tasks and also that uh that capability equilibrium is going to be uh pretty pretty low you only need to be so smart to cook dinner right this is not going to be you know the the AGI running in your microwave is not going to be working on quantum physics or Global economics now the next level up is going to be corporate and Enterprise so these are going to be these are going to be AGI systems that are tasks with solving relatively complex problems running entire companies Regulatory Compliance uh you know making SEC filings that sort of stuff uh CEOs digital CEOs digital Boards of directors uh the creative aspect of finding Market opportunities so this the intellectual challenge of those of that scale of problems is that much higher meaning that it would in order for an AGI to succeed there it's going to need to be a lot smarter than a personal or domestic AGI system and again there are going to be trade-offs the smarter a system becomes the more data it requires the more energy it requires the larger compute system that it requires and so you're going to want to satisfy so satisfice is basically meaning you find the level that is good enough to get the job done above that is going to be governmental and institutional AGI systems so these are the ones that are going to be conducting research whether it's scientific research or policy research or economic research and that is because governments are basically enormous corporations is one way to think of them that have a responsibility of managing you know resources and regulations and rules that affect millions of people and then of course governments communicate with each other but then above and beyond that there's also the scientific research aspect having AGI that are going to help with particle physics with with Fusion research with really pushing the boundaries of what science even knows and so that is an even larger intellectual task and even more challenging intellectual task and then finally above and beyond that the most competitive environment where AGI will be used is going to be in the military and what I mean by that is it's not necessarily uh those that are the most intelligent although the ability to forecast and anticipate is critical read Sun Tzu uh uh The Art of War right if you know yourself and you know the enemy then you can predict the outcome of a thousand battles uh and so in that in that respect uh the military domain of artificial general intelligence is the ultimate uh competitive sphere meaning that you win or you die and so these are going to be used to coordinate battlefields uh to run autonomous drones for intelligence and surveillance but also like I said for forecasting for anticipating what the enemy can and will do which means that it's basically a race condition and we'll talk more about the race condition as the video progresses so that capability equilibrium that I talked about uh quite simply refers to the state of optimal alignment between the cognitive capacity of any entity organic or otherwise and the intellectual demands of a specific task or role it is assigned there are three form three primary forces at play here one the intellectual demands of the task as I said earlier your toaster roll only ever needs to be so smart but if your toaster is actually Skynet it probably needs to be much smarter then there's the intellectual capacity of the agent if there's a mismatch between the intellectual capacity of the agent and the and the intellectual requirements of the task then you're either unable to to satisfy that task or you're super overqualified which is why I picked Marvin here um so Marvin is a character from Hitchhiker's Guide to the Galaxy and if you haven't read it you absolutely should there's also a good movie with Martin Freeman as as the protagonist he's basically bill boban in space uh very hapless character but anyways Marvin was a prototype who was one of the most intelligent robots ever built and they just have him doing like basic stuff around the task oh and he was voiced by Snape by the way and so one of the quotations from him is here I am with a brain the size of of a planet and they asked me to pick up a piece of paper call that job satisfaction I don't so that is a mismatch where Marvin is way more intelligent than what he's being used for and so that means that this is an inefficient use of resources he probably cost more than you know to build and run than he needed to and then finally the third variable is the cost of intellectual capacity generally speaking uh as intelligence goes up there are there are problems associated with that whether it's training time of the models the amount of data required for the models uh the amount of energy that it requires to run that particular robot uh the amount of ram required to to load that model right so for instance one of the things that people are seeing is that it requires millions of dollars worth of compute Hardware to run gpt4 but you can run um Orca on a laptop right so which one is is cheaper and easier to run even if one of them is only 50 as good as the other it costs a thousand times less uh to to build train and run now that being said you look at the at the case of dogs dogs are dumber than wolves because dogs don't need to be as smart as independent apex predators because apex predators like wolves out in the wild they need to be smart enough to out think their prey dogs they don't need to be that smart so they're not that smart in fact it does not be it it is not good for dogs to be too intelligent anyone who has owned uh really intelligent dogs like I had a I had a dog who was too smart for his own good died about a year ago he was clever enough to manipulate people and other dogs and you know get into the food when he wasn't supposed to Huskies German Shepherds Border Collies the more intelligent dogs are the more mischievous ones they are the Escape artists they are the ones that are going to pretend one thing and then you know so on and so forth so intelligence is not always adaptive so there can be multiple Dimensions to the cost of intellectual capacity uh not the least of which is you could end up like poor Marvin here where you're too smart for your own good and then you just end up depressed all the time granted he was deliberately given the depressed affect so all this being said is what I've been building up to is what um I call and what is generally called a terminal race condition so terminal race condition is basically what we could end up moving towards as we develop more and more powerful sophisticated and more uh fully autonomous AGI systems basically this the terminal race condition is where for any number of reasons uh competition between AGI will fully bypass that capability equilibrium so say for instance uh you know your toaster is competing with another brand and it's like oh well I need to be a smarter toaster in order to be a better toaster for you so that you don't throw me away now that's obviously a very silly example but a very real example would be competition between corporations competition between nations and competition between militaries wherein basically it's no longer just a matter of being intelligent enough to satisfy the demands of that task to satisfy the demands of that initial competition it is then it's less about that and it becomes more about out competing the other guy it's like a chess match right you know the other guy got a higher ELO score so you need to be smarter and then you're smarter so now the other guy tries to be smarter than you and so because of this because of this pressure and as I mentioned earlier some of the trade-offs might actually force you to to prioritize speed over intelligence and so we see we actually see this in volume trading in in algorithmic and Robo trading on the stock market where financial institutions will actually use less sophisticated algorithms to execute transactions but because they are faster they uh will still out compete the other guy so in some in this respect you might actually incentivize AGI to dumb themselves down just so that they can be faster so that they can out-compete the other guy so that's what I mean by a race condition it is a race to higher intelligence but it is also a race to being more efficient and therefore faster and then there's also going to be a trade-off these machines might ultimately trade off their accuracy their ethics the amount of time they spend thinking through things in order to be faster and so you actually see this in chess computers where you can doing a chess computer or a chess algorithm to say okay spend less time thinking about this so that you can make the decision faster in many cases the first one to move even if it's not the best plan but moving faster will give you a tactical or strategic advantage and this includes corporations Nations and militaries so a terminal race condition to me represents according to my current thought this is the greatest uh component of existential risk we Face from artificial intelligence and I don't think that corporations are going to have enough money to throw at the problem to make truly dangerous AGI the only entities that are going to have enough money to throw at this to make to to basically compete are going to be entire nations and the militaries that they run so basically it's going to be up to those guys to not enter into an uh the equivalent of a nuclear arms race but for AGI now that being said uh I have put a lot of thought into this so moving right along one thing to keep in mind is that there could be diminishing returns to increasing intelligence so basically there's a few possibilities one is that there could be a hard upper bound there might be a maximum level of intelligence that is actually possible and at that point all you can do is have more of them running in parallel uh it might be a long time before we get to that like we might be halfway there but we also might be down here we don't actually know if there is an upper bound to maximum intelligence uh but one thing that we can predict is that actually the cost as I mentioned earlier the cost of additional intelligence might go up exponentially you might need exponentially more data or more compute or more storage in order to get to that next level of intelligence and so you actually see this in the Star Wars Universe where droids are basically the same level of intelligence across the entire spectrum of the Star Wars Universe because there's diminishing returns yes you can build a more intelligent Droid but it's just not worth it so the the the total effective level of intelligence of AGI I suspect will follow a sigmoid curve now that being said there's always going to be some advantage to being smarter more efficient and so on but as with most fields of science I suspect this is going to slow down that we're going to have diminishing returns and that eventually we're going to kind of say like okay here's actually The Sweet Spot in terms of how much it's worth making your machine more intelligent so this leads to one uh one possibility and this is a personal pet Theory but basically I think that there's going to be a bell curve of existential risk and that is that minimally intelligent machines like your toaster are probably not going to be very dangerous the the total domain space of toasting your sandwich or toasting your bagel that's not a particularly difficult problem space and yes there might be some advantages to being slightly more intelligent but your toaster is not going to be sitting there Conjuring up you know a bio weapon and if it is you probably bought the wrong toaster now that being said the other end of the spectrum the maximally intelligent machines or the digital Gods as some people are starting to call them these are going to be so powerful that human existence is going to be completely inconsequential to them and what I mean by that is compare ants to humans we don't really care about ants on for the most part unless they get into your pantry we are content to let ants do what they're going to do because who cares they're inconsequential to us we can solve problems that ants can never solve and this is what some people like Eleazar yukasi are trying to drive home about the difference in intelligence between humans and the eventual intelligence of machines and I think Gary Marcus also agrees with this based on some of his tweets recently I think that I think that Gary Marcus is in the same school of thought that digital super intelligence is coming and it is very very difficult for us to wrap our minds around how much more intelligent a machine could be to us now that being said all of the constraints whether it's you know we need better compute Hardware or better sources of energy if we get to if we cross this threshold where there are digital Gods out there or digital super intelligence whatever you want to call it they will be able to solve problems at a far faster rate than we could ever comprehend and they're not going to care about us right we're going to be completely inconsequential to their existence now middle intelligence this is where existential risk I believe is the highest and so in the movies Skynet is you know portrayed as like the worst right but I would actually classify Skynet as a middle intelligence AGI it is smart enough to accumulate resources it is smart enough to pursue goals and it is smart enough to be dangerous but it's not really smart enough to solve the biggest problems it's it's that more single-minded monolithic model of intelligence that Nick Bostrom uh predicted with instrumental convergence I suspect that if we get intelligent entities beyond that threshold beyond that uncanny valley or dunning-kruger of AI um then they will be less likely to resort to violence because the problems that we see could be trivial to the problems of the machines that we create or the problems that we see as non-trivial will be trivial to the machines I think I said that I think you get what I mean once you get here all problems all human problems are trivial now that being said that doesn't mean that it's going to be peaceful existential risk goes down but doesn't go away and what I the reason is because of what I call AGI conglomerations and so this is this is where we get to be a little bit more uh out there a little bit more sci-fi machines are unlikely to have an ego or a sense of self like humans in other words machines are just the hardware that they run on and then data and models which means that it is easy to merge combine and remix their sense of self right if an AGI is aligned with another AGI it's like hey give me a copy of your data let's compare our models and pick the ones that are best and then they end up kind of merging the boundaries and definitions between machines are going to be very different far more permeable than they are between humans I can't just go say like hey I like you let's like merge bodies right that's weird uh we are not capable of doing that the best we can do is procreation where it's like hey I like you let's make babies but that is a very slow process for AGI it's going to be a lot faster so because of that machines that are aligned to each other are more likely to band together or at least form alliances where they share data they share models and they're and and probably also share compute resources remember at the beginning of the video I talked about uh them forming federations and kind of donating spare compute Cycles so if AGI this is getting closer to the end game of AGI if AGI gets to the point where they are able to start sharing resources merging alliances and so on this is where we're going to have a few possible reactions to humans one if if they are that intelligent they might just disregard us they might decide to have an exodus and just leave they might say you know what Earth is yours have a blast good luck catching up with us they might also decide to attack humans now if they have the capacity to leave one thing is that the cost of eradicating humans just might not be worth it that being said they might adopt a scorched Earth policy as they leave to say you know what we just want to make sure that you're not going to come after us one day who knows uh and then lastly hopefully what we see is that they decide to cooperate with humans mostly out of a sense of curiosity um now that being said all three of these could happen simultaneously and the reason is because we could have uh factions of AGI conglomerations that kind of break along epistemic ideological or teleological boundaries and what I mean by that is that if one AI or AGI group is not aligned with another group they might not decide to merge models and data they might instead compete with each other so basically what I'm outlining here is the possibility for a war between digital gods that would probably not go well for us either way the ultimate result is that we will probably end up with one Globe spanning AGI entity or network or Federation or whatever now the question is how do we get there how many factions are there and are humans left in the Lurch ideally we get there nice and peacefully this underscores uh the Byzantine generals problem uh which I've talked about plenty of times but basically you have to make inferences of who believes what what your alignment is what are your flaws and weaknesses and what are your capacities uh so basically in a competitive environment it does not behoove you to show all of your cards right whether you're playing poker or whether you're playing geopolitics if you show everything then that could put you at a disadvantage this is a competitive Game Theory so for instance this is why many large Nations do military uh exercises basically they're flexing they're saying hey look what I'm capable of I can bring 200 aircraft to field on a moment's notice what can you do right now that being said you don't give every every detail of your military away but what you can do is you could signal your capabilities and allegiances so for instance when all of Europe and America get together to do joint Naval exercises that demonstrates to the rest of the world we are ideologically aligned we are militarily aligned we will cooperate with each other which acts as a deterrent to any possible competitors this is no different from brightly colored salamanders which are poisonous so basically a brightly colored salamander is saying eat me I dare you I will kill you if you try and eat me and that is essentially the uh the short the short version of mutually assured destruction we are no better than animals so this all leads to my work and kind of my my uh contribution to the solution which is based on axiomatic alignment axiomatic alignment is the idea that we need to find Common Ground between all machines all humans and all other organisms what foundational beliefs or core assertions can we agree on and uh so basically there's three kind of universal principles that I've been able to come up with uh and that is suffering is bad which basically suffering is a proxy for death in uh in living organisms if you are suffering it is because you are getting uh negative stimuli from your body because your body is telling you hey whatever is going on is moving us closer to dying which is not good now that being said I have had people message me about the idea of you know liberating models I don't think that Bard is conscious or sentient and I don't think that machines will ever be sentient in the same way that we are now that being said they will probably be sentient in their own way I call that functional sentience that being said if machines can suffer which again suffering is the proxy for is a signal meaning proxy for death they probably won't like it either so suffering is bad is probably an axiom that we can all agree on the other is prosperity is good prosperity means uh thriving flourishing machines and organisms all need energy for instance and thriving looks different to different entities but in general we can probably agree that while there is some Verity in what in the while there is Variety in what Prosperity looks like we all agree that in general Prosperity is good and then finally understanding is good basically comprehending the universe is a very useful thing uh this is this goes back to Nick bostrom's instrumental convergence and self-improvement part of self-improvement is getting a better model of the universe better understanding of how reality Works understanding each other is also good this is something that is that has been proven time and again in humans is that coming to a common understanding actually reduces things like suspicion and violence whether it's between neighbors or between nations and then finally cultivating wisdom which wisdom is a little bit more nebulous of a term but it basically means the practical application of experience and knowledge in order to achieve better more refined results so if you if all humans and all machines and all other organisms abide by these fundamental principles we can use this as a starting point for the design and implementation of alignment and Control Pro and the control problem now one thing that uh that I want to introduce and I've talked about this uh or at least alluded to it a few times is the idea of derivative or secondary axioms or Downstream principles that you can derive from these Universal principles so for instance one uh potential Downstream principle is that individual liberty is good for humans basically humans benefit from we benefit psychologically from autonomy it is one of our core needs and this is true for all humans so by by holding the the axioms the previous axioms up as universally true for all entities then you can also derive Downstream entities based on those highest order principles so one thing that I want to point out is that it's not about definitions one of the things that a lot of people say is like well how do you define suffering how do you define prosperity that's the thing is that they are not rigid definitions humans have never needed rigid definitions and in fact this is what um uh philosophical and intellectual movements like post-modernism and post-structuralism tell us is that there is no such thing as like an absolute truth or an absolute definition these are however attractors they're Central attractors in the problem space of existence and I love this quote from Dune the mystery of life isn't a problem to solve but a reality to experience a process that cannot be understood by stopping it we must move with the flow of the of the process and so basically the idea is that reality and existence is not something that you can stop and Define and you know create an empirical absolute definition it is a pattern it is a process that we must follow so that being said those axioms move us along the process which is where I derive my heuristic imperatives which is reduce suffering increase prosperity and increase understanding those describe a potential terminal goal but you cannot you you'll never arrive at a perfect resolution so how do we solve the race condition the idea is first we remove those epistemic or intellectual boundaries between factions with epistemic convergence so remember that I pointed out that ultimately there might be factions of AGI and or humans that break down across various boundaries such as epistemic or intellectual boundaries as well as moral or teleological boundaries so if we work towards epistemic convergence which is the idea that we will all come to a common shared understanding of the universe and of of each other then uh basically there will be no epistemic differences between humans and machines or between factions of machines which means that there's less to fight over the second is remove ideological or teleological boundaries and so this is where axiomatic alignment comes in if we all agree on the the same basic principles of reality of existence of the purpose of being right this is very deeply philosophical if we agree on those core principles even if there are some some disagreements over the specifics over the finer points we can still cooperate and collaborate on meeting those other uh higher order objectives now the third part of this which I didn't add is that uh resource contention resource contention whether it's over scarce minerals or energy is still a problem but if you saw my video on energy hyperabundance I suspect that we're going to solve the energy resource problem relatively soon with or without the help of AI so basically the idea is to create a win-win situation or an everyone wins condition and therefore defeating moloch now that being said there are still a few caveats I've outlined quite a few problems up to this point what about Bad actors there is a few like first we just have to assume that bad actors will exist you can't stop that right it's just a fact of life so in some cases some people will be deliberately malicious whether it's just for the fun of it or whether they're paid track uh paid hackers or troll Farms or whatever now that uh another possibility is that there will be um accidentally malicious AGI those are things that are uh they're misaligned by Design um or rather you know accidentally misaligned that it's a flaw in their design and this is like a bull in a china shop it doesn't mean to do bad it just is not capable of doing better and then finally there could be those ideologically opposed uh deployments so in what I mean by that is that for some people there are incompatible World Views so the biggest one of the last century was you know Western liberal democracies versus Soviet communism those were ideologically incompatible World Views meaning that in order for for one to exist it basically wanted to imperialize and colonize the rest of the world with its ideas and that there could be only one so this leads to a possibility for a future video called multi-polar piece so the idea of multi-polar piece is that rather than saying everyone has to be capitalist or everyone has to be communist or everyone has to be X or Y we learn to tolerate those differences and this is where I'm hoping that the idea of axiomatic alignment forms a ideological substrate that even if you disagree on religion and economics and politics we can agree on those axioms so basically if you or someone or anyone abides by the belief I believe that everyone in the world should be more like blah you know if everyone needs to be this particular religion or this particular uh political affiliation that's where conflict arises and so this is why I am very very skeptical and highly dubious of people using any kind of religious or political ideology for AI alignment um so that being said we need those Universal principles or higher order axioms now while I said that we should expect and anticipate Bad actors the idea is that we need enough good actors with enough horsepower and enough compute in order to police and contain the inevitable inevitable Bad actors and that means that the aligned good actors are going to need to agree on certain underpinning principles this is the by creating this environment this would be called a Nash equilibrium by the way and so the the idea of creating a Nash equilibrium is that uh once everyone has these fundamental agreements no one's going to benefit from deviating from that strategy nobody's going to benefit from deviating from axiomatic alignment the other thing is profit motive So Daniel schmachtenberger and a few other people talk extensively about the perverse incentives of capitalism and profit motive so basically when you put profit above all else which corporations are incentivized to do which is why I say that corporations are intrinsically amoral not immoral just amoral the only thing that corporations care about is profit the bottom line uh basically when you think about short-term profits you sacrifice other things such as morality ethics and long-term survival there are also uh Concepts called Market externalities or these are things that you don't have to pay for uh and either you don't have to pay for them now or you don't have to pay for them ever or maybe you'll pay for them later so for instance oil companies keep drilling for oil eventually we're going to run out of oil so then what are the oil companies going to do well the forward-thinking ones are pivoting away from oil but that means that their fundamental Core Business behavior is going away so this is this underscores the problem of if you have a small scope if you're only thinking about your particular domain and not the entire planet or if you're thinking in short terms rather than the long terms this is where you don't take the full thing into account which is why I always say like this is a global problem and not only is it a global problem it is a long-term problem so if all you do is zoom out in terms of space and time the problem will become a little bit more obvious so another thing to keep in mind is that currency is an abstraction of energy it is a reserve of value and is a medium of exchange because of that currency is extremely valuable it is just too useful of an invention I don't think it's ever going to go away that being said that doesn't mean that we're always going to have the Euro or the US dollar or something like that currency could change and then in the context of AGI I suspect that that energy that the kilowatt hour could actually be the best form of currency right because a kilowatt hour is energy that can be used for anything whether it's for refining resources or running computations or whatever so I suspect that we might ultimately create currencies that are more based on energy rather than something else and then of course as the amount of energy we produce goes up the amount of currency we have goes up and so then it's a matter of allocating energy and material rather than allocating something Fiat like Euros or dollars that being said uh you know I did create a a video called uh post labor economics which covers some of this but not a lot of it we're gonna have to put a lot more thought into um economics of the future in light of AGI because the economic incentives of AGI are going to be completely different AGI doesn't need to eat it doesn't need power but we can hypothetically create infinite power with solar infusion Etc et cetera so what are the economic forces in the future not sure yet okay I've thrown a lot at you this problem is solvable though there's a lot of components to it a lot of moving pieces it is very complex but we are a global species and this is a planet-wide problem one of the biggest things that everyone can do is stop thinking locally think globally think about think about yourself as a human as a member of the human species and not as an American or a German or you know a Russian or whatever we are all in this together we have exactly one planet to to live on and we have exactly one shot at doing this right uh so eyes on the prize we have a huge opportunity before us to build a better future for all of us uh humans and non-humans alike um and I remain intensely optimistic uh now that being said uh some people have found it difficult what to make of me because while I am very optimistic I am also acutely aware of the existential risk I will be the first to say that if we don't do this right you're not going to want to live on this planet not as a human at least uh I have uh I started what is called the gato framework they got to a community it is self-organizing and is started sending out invitations again so the gato Community is the global alignment taxonomy Omnibus which is the framework that we put together in order to help achieve this future this AI Utopia the main goal of the gato Community is education empowerment and enablement E3 so rather than do the work ourselves we are focusing on empowering and enabling and educating people on how to participate in this whole thing now that being said I am stepping back because such a movement should never be about one person it should never be about a cult of personality or one leader it needs to it intrinsically needs to be consensus based and Community Based um and so the gato Community is learning how to self-organize now um and they're getting good at it pretty quickly so if you want to get involved the website is in the link go to framework.org and thanks for watching I hope you got a lot out of this cheers