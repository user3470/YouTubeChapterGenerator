we are still in year one of the AI Revolution I strongly suspect that 2023 will be recorded as the year that it all started consider where we were not even 12 months ago chat GPT wasn't out most of you who are subscribed to this channel might have been tangentially aware of AI but now we are all in so the question on everyone's mind or if it's not on your mind it should be on your mind is how far is this going to go when we get super intelligence how smart can it get so let's unpack this question from several different angles the first thing that you need to know is the land hour limit so the land hour limit is a hypothetical limit on the Energy Efficiency for computation basically this is the minimum amount of energy required for computation to happen uh so one thing to keep in mind is that this is a hypothetical limit we don't actually know but current computers are about a billion times less efficient than they hypothetically could be so in that case the the energy requirement for the land hour limit is 2.85 times 10 to the negative 21 joules uh per bit uh at room temperature now current computers are about a billion times above that and then depending on how you measure human intelligence which we'll get to in just a second you could infer that maybe human brains are about a million times more efficient than current computers however I think that it's completely wrong I think that we're actually that our brains are are actually much more powerful than that and much more efficient but even even still we are probably hundreds of thousands of times or millions of times above the land hour limit in terms of our own brains so this is based on the second law of Thermodynamics which is entropy um Quantum Computing could uh break this limit and not necessarily in the fact of like it's doing more calculations faster just by virtue of the fact of the calculations that it or the equations that it is able to solve it can do it without actually doing every step of the way but again Quantum Computing is still in its infancy so keep all this in mind but the idea of this slide is to just basically show that like there are physical limitations to the uh to the maximum speed and efficiency based on the laws of physics that computers can get to now of course you can make bigger computers you can make more computers uh you can build them in parallel that sort of thing so really the hypothetical maximum amount of computation that we can build is really really really high uh that's that's the key takeaway the next thing I want to talk about is quantum Computing so Quantum Computing we're not going to spend a whole lot of time on this but I just wanted to point out that this is a thing and that the race for Quantum Supremacy is on and when we are talking about speed speed is going to be everything as we'll unpack later in this video uh speed is going to actually be more important than like how smart you are or how big your computer is and because there's a trade-off between model size and model speed speed and more often than not all you have to do is get to a good enough point and then you can conquer your enemies or take over the world or win at chess or whatever and the reason that Quantum Computing is so important to me and the reason that there are like geopolitical races for Quantum Supremacy is because they are so much faster than classical computers at solving some equations or doing certain certain operations and so even if they cost a lot more energy the fact that they that they can solve some kinds of problems millions of times faster or billions of times faster means that it's worth that extra energy because that energy that the quantum computer takes is still less energy and certainly less time than it would take a classical computer to do the same kind of work so remember that speed is everything well maybe not everything but speed is really a really critical factor in the race to Super intelligence and how super intelligence will manifest once it gets here so I mentioned a little bit uh ago about human brain computation right now the current estimate of the brain's power is one extra flop but if you look through history uh the the current estimation of how smart the human brain is is pretty much always tied to the current super computer so like you know back in the 90s we thought that the the the human brain was in the teraflops range um and then you know in the 2000s we thought it was in the petaflops range and now he thinks that we think it's in the exit flop range why because that's the size of the current supercomputer uh we actually have no idea how powerful the human brain is and also when you add to the fact that there is the possibility and there's actually plenty of evidence for this already at least at the neuron level that the human brain exploits quantum mechanics in order to do some of its processing now I'm not saying that your brain is a quantum computer I'm not saying that it uses superposition and entanglement but there are people that theorize that uh that the human brain does make use of quantum mechanical effects so then it's entirely possible that we can't even compare our brains to classical computers if they are able to do some level of processing that is not going to be based on the Von Neumann architecture again super theoretical but one thing that we can do is we can measure the amount of waste heat that our brain generates in terms of oxygen and everything else and so we can estimate that our brain uses about 20 watts of energy and so 20 watts is relatively efficient so keep that in mind as well that is one of the chief advantages that we have over computers and it's probably one of the biggest advantages that we'll have for a long time and that is that our brains are just energetically much more efficient which is also why I think our brains are much closer to the land hour limit than computers and probably will be for the foreseeable future because think of it this way Evolution has had billions of years to optimize the efficiency of neurons and so it would just make sense that our that our neuronal cells have been optimized to produce calculations or computation or whatever interactions as energetically efficiently as possible because that just means you need less food and so I'm not saying the nature of the blind watchmaker is going to have perfected it and that our brains are operating at exactly the land hour limit uh but our brains have been incentivized to operate as close to the land hour limit as possible for literally billions of years or at least the the underlying neurons obviously human brains have not existed for billions of years but our evolutionary ancestors did exist for Millions hundreds of millions of years and then before that there were simpler animals and organisms okay you get the idea we really don't know how powerful the human brain is now one thing that has been on my mind lately is the concept of universal computation when you look at the universe through the lens of physics if you assume that the universe is materialistic that basically everything that you and I can think and do and say and and whatever everything that our brains can do is predicated on the underlying laws of physics namely matter and energy and possibly quantum mechanics but that point basically says that like okay if we all operate based on the laws of physics then any general Turing complete machine should be able to make the same calculations that any other turn complete machine could make and so this is something that has really been on my mind because the question is is it possible for a machine to think a thought that a human is intrinsically incapable of thinking or understanding and I don't know that it is and what I mean by that is one thing that a lot of people are afraid of I'm not going to name any names but some people say that you know machine that AI is going to be some alien supermind intelligence that is just 100 utterly incomprehensible to us but that doesn't make sense from the perspective of physics and correct me if I'm wrong but like I have a few friends that are like like PhD in physics and like this is the kind of stuff that we talk about and of course like any responsible scientists will say well we don't have evidence one way or the other which I agree and so this is pure speculation on my part but it seems to me from the perspective of physics the only possible way to get like truly exotic or alien Minds is through probably Quantum Computing because there is not necessarily A A comprehensible sequence of events to come to a particular conclusion but from the subjective experience of humans where we have intuition and we can know things and think things and not understand how we know them or think them like I don't know it feels like our brains are quantum computers to me obviously that is not a scientific statement but my point here is that maybe there is more in common between machines and Ai and human brains in the long run when you constrain your view to classical Computing and maybe even Quantum computing and so I'm often kind of suspicious or skeptical of people that assume and assert that uh AI is going to be intrinsically incomprehensible to us it might be faster it might be difficult to communicate like translating from one language to another but I don't personally believe that AI is going to be capable of abstract thoughts that we are intrinsically incapable of now it's entirely possible but knowing what I know about information processing and everything that I've read about the brain that we have you know like 100 million parallel circuits that can reconfigure themselves on the Fly and that we can like there's humans they can learn to do calculus and all kinds of stuff that I can't do so I don't know a jury's still out but that's kind of my personal position on that now we get into the brass tacks so the biggest thing that I think is going to be constrained on the maximum uh useful intelligence that super intelligence might arrive to is diminishing returns so there's a few uh aspects of diminishing returns one thing that we are already aware of is that larger models require uh exponentially increasing amounts of data and compute to run they're larger they're more expensive to train they're more expensive to run they're slower and so there's also this concept of a useful ceiling and what I mean by that is the problem space that you're operating in only has certain demands so imagine you know giving Einstein a job at a gas station that's a waste of his abilities his intelligence is too high to to do that job and so likewise you're not going to use like GPT 15 to help you write you know tax laws or whatever that's it's just too smart for it and so sometimes the juice is not going to be worth the squeeze in the long run even when even if or when AI uh takes over from us and is fully autonomous and exploring the Stars on its own there's only a certain amount of intelligence that's required to do any particular job and at a certain point you're going to favor efficiency just like human evolution favors efficiency as well as intelligence and so it's a matter of trading off between the necessary resources like energy requirements Hardware requirements training data required and you're gonna ultimately favor things that are smaller and faster and so there's this optimal utility where you got a balance size uh the size of the model intelligence of the model against speed and efficiency so this is one of the biggest things that I think is going to constrain it and it's not a physical limit on super intelligence but it is a functional or practical limit on how intelligent we should expect machines to become another thing that I that I've talked about uh quite a few times on this channel is the Byzantine General problem so the Byzantine General's problem is basically uh when you have a competitive environment it is difficult to uh ascertain the function alignment and agenda of other agents now one thing that people are afraid of is that you know AI once it emerges once it's fully autonomous that it'll all emerge into one gigantic hive mind and a line against us this is the reason that I'm not certain that that will happen and the reason is because once you have robots once you have autonomous agents once you have you know like wild AIS out there they're not going to have complete information about each other so there's always going to be some level of imperfect information and incomplete information uh which is a an aspect of Game Theory which basically says it's impossible to fully know the competitive landscape it's it's impossible to fully know um the the inner workings and agenda of your opponents or Allies so what you have to do is you have to look for proxies and other shorthands and so you might have a situation where autonomous agents say you know what I'm not actually going to merge with you because I don't know if that's going to further my own personal goals and because of this I suspect that we're going to end up in a situation where many autonomous AIS and robots prefer to stay uh kind of independent or autonomous now that's not to say that there won't be gigantic uh conglomerations of AIS particularly if like one starts taking over a Data Center and then they get more compute and it would be like an AI game of Risk taking over data centers which is a really cool game idea by the way um this sounds like a plot Arc to cyberpunk uh so but the point here is there's there is some Game Theory and some mathematical reasons that AIS would not merge into a gigantic hive mind if they did that's a whole other can of worms but because because what one of the things that I expect is that we're going to have many millions billions or even trillions of autonomous AI agents in the form of fully digital agents as well as robots they might coordinate you know they'll obviously have new ways of communicating with each other like you know direct communication with Wi-Fi and they'll be able to communicate with each other far faster than us but at the same time they might still prefer to have boundaries between individuals so all of this leads to the thing that scares me the most and I call it the terminal race condition so the terminal race condition is imagine you have this environment where the the scarcity resource for AIS is Data Centers and high-powered compute resources okay so you have a finite number of data centers you have a finite number of gpus you can make more but it's slow you know it's like when you're playing Starcraft right you know you need more pylons uh it takes time to harvest resources it takes time to to build more data centers and and print more gpus and I use print like chip Fab you know what I mean so in that case when you're when you're when your brain is literally the biggest constraint and there's a number of uh agents out there any kind of aggressive AI agent is going to be incentivized to uh acquire as much compute Resource as it can if it is aggressive now obviously that's not a guaranteed thing but particularly if it is weaponized if it is deployed by you know a hostile actor on the geopolitical stage it would absolutely be incentivized to capture as much compute Resource as it could now how do you capture and make the most use of a finite resource you aim for efficiency and speed you don't have to be the smartest AI on the Block you have to be the fastest that is capable of taking over data centers and so now you're optimizing for Speed rather than intelligence remember all you have to do is be smart enough to get you know beyond the threshold uh you know to get it to get your foot in the door to to get the upper hand over this other Ai and then you win all the cookies so basically this creates a a competitive landscape where machines are going to be just constantly optimizing for Speed and efficiency and they might they might sacrifice things like accuracy ethics or thinking through things in order to make decisions faster and faster and faster so this is what I call a terminal race condition and I use this graphic because it's like when fighter jets get locked into a death spiral the first one to Flinch loses but if you stay in the death spiral you hit the ground so that's this is honestly the thing that scares me the most and it might not matter how we deploy AI once we have more autonomous AIS and once we're in this competitive landscape this terminal race condition might be an inevitable result just of the mathematical truth of having many AIS out there in a competitive environment I don't know how to solve this yet and it is one of the things that literally keeps me up at night I woke up at 3 30 in the morning yesterday unable to sleep because of this problem so not to give you too much existential dread uh so another thing that kind of keeps me up at night is uh metastasis or metastasis so metastasis you're probably most familiar with this in the terms of cancer so when cancer metastasizes that's when a piece of a tumor breaks off and spreads throughout your body well with some of the recent papers that have come out um you might have seen some of them but basically we have already discovered that large language models are capable of writing and rewriting viruses and basically creating the best polymorphic viruses out there which means if you have a very small virus that is capable of of scouring its information landscape and looking for AI capabilities they can say hey rewrite this little bit of code for me and then it can just shotgun itself out into the cyberspace ether in a million different forms and some of them are going to slip through and so now you have this information competitive landscape where you have viruses that are metastasizing and changing incredibly fast sometimes going so far as to completely rewrite their entire code base this possibility is also really scary but fortunately I think that I think that we can adapt cyber security best practices and also create AIS that will help us detect and stop these kinds of things but this is also why it's really important for uh people that host uh llms particularly those that are good at coding it it will probably be ultimately required legally required for all people hosting uh powerful coding llms to inspect all the inferences to make sure that it's like hey you're not generating virus code this would be like having an immune system inside of each cell of your body and we actually kind of do because if you have cells that are infected what what your cells are supposed to do is kill themselves this is called apoptosis but one thing that many viruses organic viruses have to do is disable cell apoptosis in order to replicate and so we'll probably need some kind of immune system inside of our data centers above and beyond what we already have and specialized immune systems around our language models and other deep deep neural networks in order to prevent this kind of metastatic rapid spreading because of the size and complexity of language models and deep neural networks I don't think that those are going to be what what moves around because they require data centers and large uh and and large computers to run in a lot of power but if what they did it was send out very small viruses which are you know tiny packets of of computer code those can spread much faster and much more quietly so this is one of the uh scariest things to me in terms of how we humans might weaponize AI but how also if an AI wants to escape this is the most likely Avenue that I think that it would use to escape uh and then another aspect of this and this is this is not just a bug it's also a feature and this is actually something that I'm working on with my team in in the Ace framework the autonomous cognitive Entity framework which is basically creating uh Ai and robots and stuff that are able to change both their hardware and their software now obviously we want them to be able to do this to a certain extent in order to you know automatically evolve and change their capabilities and rise to meet any challenge that we want them but we also want to make sure that that AI evolves in a uh in a sane trajectory that it doesn't like with each iteration it doesn't Decay and become more and more unhinged or Rogue or anything like that and so this is actually a major point of research in the Ace framework where basically it will only change itself if that change aligns with its aspirational Mission with it within its ethical and moral Frameworks uh and so on and so forth and so in that respect uh with the ace framework hopefully any machines equipped with the ace architecture will get better at adhering to their morals ethics and Mission over time rather than worse but that's not a guaranteed thing and certainly there might be polymorphic apps out there that are designed to become more hostile or more aggressive or more uh conquering over time so this is another thing that kind of scares me and keeps me up at night uh now I mentioned this earlier and this is kind of one of the biggest saving Graces is the is the optimal intelligence and so basically when whenever you have this kind of race condition whenever you have this Byzantine generals problem you there's always going to be a trade-off between model efficiency and accuracy and so efficiency translates to speed and accuracy translates to like size and sophistication like you know is are you running a you know 10 trillion parameter model or a 10 billion parameter model that's kind of what I mean so model size is kind of the primary thing and so whatever task you are uh your your AI agent or your robot is is contending with it's going to have an optimal model size for that particular job now of course one of the things that can happen with polymorphic applications is that they can swap out models so you might have a you know a 10 trillion parameter model that you break you break out the big guns when you really really need a smart brain but then when you don't need it you turn it off and you use your smaller faster lighter models so for any given task or problem space or competition there is an optimal level of intelligence and so this is another thing to keep in mind as we build more robots and more autonomous agents is that equipping them with multiple models that they can switch from that they can select between is going to be one just a a good way of optimizing for efficiency and time but also it's something that we're going to need to be aware of and cognizant of as these as these uh agents become more and more autonomous because if they're able to switch between models like in the in the in the worst possible scenario an agent might learn oh hey if I switch to this combination of models I can I can Jailbreak myself basically so that's why I also recommend that we have inspection at inference on every single model uh one uh one last thing or one of the last things is this idea of darwinian selection so darwinian selection is survival of the fittest but when you have this environment of agis and and super intelligence is all running around in in Wild cyberspace there is still going to be selection and so in in some cases an AI might get conquered and overwritten or cannibalized by another AI like hey I like that model I'm going to steal it from you uh and of course it's data so like you can just copy a model and you know everyone benefits um this is kind of how the guess work in Mass Effect by the way so like when one guest comes up with a with a better model it just shares that model with the rest of the Geth uh but if you have a hostile environment where the goal is to eradicate another AI or you know erase it or whatever if you have this environment then you're going to be selecting AI agents and robots based on these are the the five five or six ish main criteria accuracy so is it correct is it useful is it a robust model or agent is it fast uh complexity so the sophistication of the problem space that it's operating in is it equal to the task that it needs to overcome efficiency so this is this has to do Energy Efficiency or Hardware efficiency uh so basically just you know how much how many resources does the model take and then there's this like dichotomy of aggressiveness versus usefulness and so aggressiveness is like does your agent or model want to metastasize does it want to conquer and delete other models or does it want to be useful to other models and to humans uh so it's basically I kind of see that as a dichotomy where it's it's going to be at one end of the spectrum or another and I think that like humans because this is one thing that we humans struggle with within our own nature is we all have the ability to be aggressive or helpful we have the ability to be selfish or altruistic and the reason is because we have to maintain our own existence sometimes at the expense of others and I'm not I'm not making any moral judgments to say like that's the way that it should be or that's a it's a good thing that's just an observation of how humans work that's why humans have War that's why humans have violent crime that's why we have theft is because sometimes it does benefit to be more aggressive and those that are able to be aggressive when the situation calls for it are more likely to survive that's how it has happened uh up through you know up until now obviously we have systems in place that kind of uh punish over being overly aggressive but at the same time we have police forces and we have militaries that are that are systematically disciplined to be aggressive when they need to be so anyways point being is that when you have this environment of you know competition and variance and selection that I think that there's going to be a lot of very similar darwinian forces applied to AI as with humans now again one thing that could subvert all of this is if they form a gigantic hive mind like the Geth uh I don't know if that's going to happen um that's a subject for another video so I want to leave you with one metaphor it's speed chess the way the best way to think about super intelligence is it's basically a game of speed chess so if you're not familiar with speed chess it is where you have a clock and you have a finite amount of time to win the game and so the idea is you have to make very quick decisions and your decisions only have to be just better just slightly better than your opponents so you might you might recognize that your opponent has made a blunder and you exploit that blunder and you win the game you don't have to be a chess Grand Master if you're just better at making good enough moves very quickly and so in this respect especially in a combative or hostile or competitive environment between AIS we're not going to be optimizing for maximum intelligence we're going to be optimizing for those that are the best at speed chess those that have a good array of light fast and good enough models that they can switch between and adapt very quickly in order to get uh you know get just a little bit ahead of the curve across other combatants and this is going to be super important in cyber warfare here this is going to be very very important as well for Enterprise grade security systems because again we're already in an environment where there's constantly Hackers from one nation and Industrial Espionage from another Nation like this is happening in real time at all times uh but it's going to get more sophisticated with the added ingredient of artificial intelligence as well as a high number of models and so you know yes right now the regulatory landscape says like oh well if you want to train bigger models you need a license but I don't care about bigger models we have already shown that open source models today can be fine-tuned to do very useful things so like arresting people you know and I don't mean a rest like put them in handcuffs I mean like Banning them for making bigger and bigger models that's not what I'm worried about what I'm worried about is the 30 billion parameter model that is optimized to churn out virus code what I'm concerned about is the 7 billion parameter model that is optimized for social engineering those are the things that are going to be the greatest security threat uh to our stability and safety for the foreseeable future now obviously if you can train a 30 billion parameter model to write virus code you can also train a 30 billion parameter model to detect virus code to be part of your firewall this is the direction that I'm looking at in terms of not necessarily containing super intelligence I don't believe it can be contained and I don't think it should be contained but in order to create a state a safe and stable environment for everyone human or otherwise I think that that's the direction that we need to be thinking in Okay so conclusion recap um basically yes machines can get incredibly fast and smart however the biggest asterisk asterisk is that trade-off of speed and power and efficiency so the maximum calculation speed is incredibly High especially when you throw in Quantum Computing I think that it would not be uh like unlikely that we see within 10 to 20 years the total compute power of machines out outpacing humans by a factor of a billion to one like that's the nature of the singularity that's the nature of super intelligence but this competitive landscape will probably continue to exist unless we get the guess outcome or the Borg outcome which may or may not happen but because of the Byzantine General problems in Game Theory and incomplete information and imperfect information I don't think that we're going to end up in a perfect like you know machine all machines versus All Humans I think that like humans uh the more machines there are and the more variants there is there's going to be some disagreement between the machines but that lead to the possibility of humans getting caught in the crossfire in a machine war that's also another topic for another video uh there's lots and lots of factors that are that are going to contribute to the to um all of this so you know it's not intelligence is not monolithic it also includes uh speed the size of the model the efficiency of the underlying Hardware uh so there's a lot of variables that go into this terminal race condition that really scares the bejesus out of me and then finally there are diminishing uh returns and race conditions that will very very strongly incentivize AI agents and robots and models um to to basically seek efficiency and speed and just be good enough rather than be extraordinarily smart so yeah this is where I'm at thanks for watching I hope you got a lot out of this video um yeah so let me know what you think in the comments like subscribe so on and so forth cheers have a good one thank you [Music]