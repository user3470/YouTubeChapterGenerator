there are exactly three kinds of prompts that uh that exist that encapsulate literally every other kind of prompt and just a couple of foundational principles that you need to know in order to master language models such as GPT and llama and others now a little bit of background about myself I've been doing prompt engineering since gpt2 and of course now we are on gpt4 so the three fundamental operations you need start with reductive operations so what do I mean by reductive operations a reductive operation is where you have a larger input than you do output so this is things like summarization summarization is you say the same thing with fewer words there's a few formats that you can use you can use lists you can use nodes you can use executive summary each of these have slightly different focuses so for instance an executive summary gives you just a high level overview or the core assertion without actually trying to say the same thing over again distillation is a very useful one which is basically you're you're telling it to kind of purify uh the the the core underlying uh principle or fact this removes a lot of noise uh extraction so extraction is what a lot of people are really familiar with already because that comes from older NLP such as answering questions uh named entity extraction getting dates and numbers that sort of thing characterizing this is one that a lot of people don't know about you can have the language model either characterize the text or characterize the the topic within the text and so what I mean by that is you can characterize text you can say you know is this does this look like fiction does it look like a scientific article does this look like code or then you can if it is code for instance you can characterize the code within that context you can analyze it so basically you can do a structural analysis rhetorical analysis that sort of thing there's evaluations so evaluations are measuring grading or judging the content um so there's several kinds of evaluations so say for instance you have a rubric and you say hey grade this um you know grade this this essay based on this rubric plenty of people have discovered that out there you can also evaluate against moral Frameworks so you can say like is this aligned with you know Plato's you know yada yada or is this aligned with cantian logic or whatever it knows those things and then finally critiquing so critiquing is providing critical feedback and recommendations to improve something that's reductive operations the second kind of operation is transformational operations so transformational operations uh that you basically the input and output put are roughly the same size and roughly the same and or roughly the same meaning so an example of a transformation operation is reformatting so this just changes the presentation you might say hey take this and you know turn it into bullet points rewrite this prose as screenplay or rewrite the screenplay as Pros translate it from XML to Json that sort of thing refactoring so refactoring is something from uh from the programming World which is basically say okay take this chunk of code write something that does the same exact thing but write it better or write it differently and you can also do the same thing with language so for instance any kind of structured language or uh or a constructed language you can say Hey you know basically say the same thing but in a different way uh and so like whether it's a legal document or a scientific document or whatever you can still you can refactor those as well language change so you can change between natural languages such as English and Portuguese you can also translate between coding languages like C plus to python now obviously I can hear people in the background screaming like C plus is is compiled and python is interpreted and there's all kinds of things that mean that you can't translate directly from one to the other it's true but give it a try you'd be surprised how well uh modern language models can do so restructuring so restructuring is different from reformatting whereas restructuring says okay this is in the wrong order we need to change the order we need to remove sections we need to add sections you can optimize for logical flow you can optimize for uh basically any kind of priority that you want so that's restructuring modification so there's all kinds of modifications you can do but basically you rewrite copy to achieve a slightly different intention you can change the tone you can change the formality you can change the level of diplomacy you can change the style those are what I mean by modifications and then finally clarification so clarification is where you say hey write this but write it like more clear right articulate it better and more clearly which is something that is extremely useful when you're using it to edit for instance scientific copy or if you're not a native English speaker or native anything speaker really if I was trying to write something in French and I would you know like my grasp with French is like I've basically taken two weeks of French so I'd be like rewrite this garbage so that it's actually comprehensible to a French person that's an example of a transformational operation and then finally generative operations or expansion operations or magnification operations in this case the input is much smaller than the output and it may or may not have the same meaning but the idea is that you go from small to big and so this is drafting so drafting is where you give it a set of instructions and it you have it draft a an some kind of document whether that's a code file fiction legal document knowledge base scientific document some kind of Storytelling like you know telling a story about data or other facts planning so planning is another thing that I don't see a lot of out there that that it's actually really good at which is given a set of parameters come up with plans so you can have you know action plans project plans uh brainstorm objectives and missions um you can have it elucidate constraints and and uh and also discuss the context within that planning session uh brainstorming so brainstorming is something that a lot of people are familiar with especially with chat GPT where basically use your imagination to list out possibilities so this is you know either generating ideas exploration of possibilities problem solving problem solving requires brainstorming and then hypothesizing so actually generating hypotheses and then finally amplification which is similar to expanding something in the transformational one but basically instead of just saying explain this thing better you say Riff on this and and actually expand this entire topic um to fully unpack it that's that's the key word that I use there is unpacking where it's like hey here's a topic that I want to talk more about unpack this so those are the three operations it's reductive transformational and generative or expansive now the other thing that you need to know about uh prompt engineering and language models is latency and emergence all right so before we dive into this I need to give you a really really really fast crash course on Bloom's taxonomy so Bloom's tax on Bloom studied all all kinds of stuff in education so if you look them up uh did a lot of stuff with education like Bloom's 2 Sigma problem but Bloom's taxonomy as someone who is a big fan of Frameworks um this is really critical to understand um how humans learn in order to really see what language models are capable of so at the bottom layer you have remember followed by understand followed by apply analyze evaluate and create and so let's really quickly unpack these and you will see that language models have already attained most if not all of Bloom's taxonomy so remembering is just recalling facts and content Concepts yes it can regurgitate stuff no problem understanding so understanding is defined as explaining the ideas and Concepts connecting the words to meanings yes language models can can absolutely do that and so whenever I see someone in the comments say like but language models don't truly understand anything I'm like well yes they do by this definition but then if you want to dive into the epistemics and and philosophy of it then I would argue that humans don't understand anything anyways anyways going down a rabbit hole applying using information in new situations so this is the functional utility if language models couldn't apply information they wouldn't be useful but they are useful and so we see that they can apply and and functional settings analyzing so drawing connections among ideas if you don't believe me go into chat GPT and ask it to do a rhetorical analysis on your own emails or whatever it absolutely can analyze stuff uh number or uh the the next one it's not I didn't number these uh evaluating so justifying a decision or action now of course this is where uh chat GPT you can really get it into a corner because it will definitely justify itself it'll it will explain and articulate and defend um something whether or not it's right but again doing it in one shot is not necessarily going to produce the best results because if you have a person if you back a person into a corner and ask them to justify themselves well people will often do the same things especially pathological liars and then finally creating producing new or original work generating something that did not previously exist pretty much everything that a language model spits out is something that did not previously exist so by this definition by this model language models are already at the full stack of taxonomy in terms of mental capabilities that humans are now you might argue it's like well okay it's not quite as smart as a human in some respects and it makes kinds of mistakes okay sure whatever I'm not going to argue about the details um the point here is that Bloom's taxonomy is a good model for understanding what language models are capable of okay so latency latent content or the latent space this is the knowledge facts Concepts information and other capabilities that are embedded in the model but they must be activated by the correct prompting it's like buried treasure uh hence the ends the graphic here so training data the latent content only originates from the training data which means that it's not going to magically know something that wasn't in the training data now that being said it can connect dots and synthesize new information this is this is what I mean by by this is creating is if you activate the right you know set of patterns with the right words and prompts the language model will be able to make novel connections uh World Knowledge so General facts and understanding about the world this is all inferred and and imbibed from the training data scientific information cultural knowledge historical knowledge and languages all of this comes from the training data because um whether or not you realize it these language models are trained on many many terabytes worth of internet data which means through either explicit or implicit connections it understands all of these things emerging capabilities so the larger a language model is the more emergent capabilities we're finding and so several examples of these emerging capabilities are theory of mind the theory of mind is the ability to understand and keep track of the content of other people's minds uh basically it's read enough Reddit to look at you know how people think to understand how humans think uh now you might say This is highly controversial because it doesn't have mirror neurons and other kinds of stuff I really don't care mathematically speaking it has read enough human generated content to be able to model a human's mind um implied cognition and the second one so basically thinking with the right prompting so what I mean by this is that in because all of these language models are due is they're trained to predict the next token in order to accurately predict the next token it had to learn to think uh this it's it's as simple as that is okay well it's just a mathematical model to predict the next token yes but it's also a black box on the inside and it's doing a lot of embedded operations and and using some of those emerging capabilities in order to Accurate act this excuse me accurately predict whatever the next token is going to be logical reasoning so inductive and deductive reasoning basically triangulating principles from General observations or vice versa again if you if you tell it you know you analyze this with a with a specific framework use inductive reasoning use deductive reasoning it will surprise you with what it's capable of doing and finally in context learning uh the fact that language models are able to use entirely novel information uh that that are outside of the training distribution um and it can effectively and accurately use that information is very similar to humans ability to improvise and so the fact that one of the emerging capabilities is this improvisational ability or in context learning means that these models are actually very very intelligent and then finally hallucination equals creativity a lot of people say oh well it's not actually creative but it's hallucinating I'm like you can't have it both ways because these are cognitively the same exact behavior um there is there is functionally no difference between hallucination which is making stuff up and creativity which is creating new things the difference is whether or not you as the user recognize um that Hallucination is actually a superpower that you cannot have creativity without the ability to imagine things that don't exist this actually comes from human evolution which is uh when when mentally modern humans started emerging we started drawing things on Cave walls that didn't exist like hybrid animals and hybrid humans and uh and other things so what uh my point here is that the emergent ability to be creative to hallucinate is actually required in order to achieve these things and so you cannot separate hallucination from creativity now what you can do is there's there there's mechanisms that you can use in order to ground that so you can recognize it as this is a feature not a bug it is a cognitive behavior but then it's a matter of labeling and keeping track of what's fictitious versus real so for instance I was talking to some of my lawyer friends who like using Claude or they experiment with Claude I don't think they use it in their business but you know one thing that one thing that models do and you've seen this in the news particularly with law is they'll just make up cases that don't exist and it's like oh well this is problematic and I'm like no it's not what it's doing is it's imagining if there was a case that did this then it would prove this point and so the way that the way that human brains use this is that um because there are mental disorders and and brain injuries that uh basically make it so that you cannot discern your imagination from reality um but this is a necessary function because how do you know what to go look for you imagine it first you say hey wouldn't it be great if there was a legal case out there that proved this point and so rather than rather than pathologizing and say oh the the thing hallucinated this you just didn't take it a far enough step to say hey let's go find a case like this one we imagined a possibly useful case so now let's go let's go ground that in reality using case text to go search uh it's also very context dependent um so basically like when is it good and useful to hallucinate and imagine and brainstorm so now that you know all these things uh you have you are equipped with pretty much everything that I know from the last four years of prompt engineering and working with language models on a daily basis uh so you are set to go forth and automate everything in the universe have a good one thanks