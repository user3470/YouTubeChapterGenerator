two days ago meta AI dropped this really interesting paper LM infinite and it hasn't gotten much traction and I'm not really sure why so I read it and decided to make a quick video about it in order to unpack and tell you about this paper and why I think it's pretty cool so here's the paper uh it's pre-print so keep that in mind it has not been fully reviewed and accepted yet but two days ago August 30th uh this paper dropped and its name and origin were interesting it's called LM infinite simple on-the-fly length generalization for large language models and it was produced primarily by meta AI So Meta formerly Facebook under Mark Zuckerberg you know meta has done a lot of really interesting stuff with uh you know llama and they're also kind of more big into open source which is a really interesting move uh so the long story short is this paper proposes two primary Innovations the first is a Lambda shaped attention mask which constrains the number of tokens that it needs to attend to and this addresses the issue of high dilution over time particularly with large context windows and if that doesn't make any sense don't worry we're going to unpack it in just a second and then the second is that it bounds the relative distance during attention to a fixed value which is basically just capping the value I looked at the math and it just like it basically just pads the there or creates a smaller window in order to constrain the math so that the so that the the the attention logets don't get too big all the all the math and formulas are in the paper if you're curious but the basically what this does is this presents a plug-and-play solution uh to open source language models where all you have to do is tweak some of the math functions about how it pays attention and then you end up with uh one much better performance over long context Windows particularly long context windows that are much longer than what it was even trained on and some of the tests that they did were passkey retrieval on Long context and so passkey retrieval is a really good test of attention because there's a very very specific unique string of characters that it needs to fetch and with dilution it might kind of forget those so dilution is the phenomenon that you've probably noticed in language models where once the conversation gets longer it kind of seems to forget what was at the beginning of the conversation even if it's in the context window so that's the problem that this solves so a way to analogize this way to think of it is that without this innovation without the Lambda mask and without the the bounded attention size imagine that your entire memory is a gigantic cluttered Warehouse that this is your working memory and that you've got to keep track of literally every word in this gigantic warehouse and it's just you know however many tokens you give it that's how many it's going to keep track of the mathematical complexity goes up logarithmically I think or factorially I don't remember exactly anyways point being is that the more you have to keep track of the more mathematical complexity there is and the more memory it takes so this is similar to human memory so I'm analogizing language model the internal representation of language models to human working memory now on a physical level they're not identical at all this is just an analogy so that you can kind of get an intuition as to what this Innovation is doing so if you have if you're trying to hold too many things in your mind to work on it once this is cluttered and overloaded working memory which impairs your reasoning and functioning and it slows you down on the con on the other hand constrained cleaner working memory gives you better performance and so in this respect it's almost kind of like a a garbage collection mechanism for language models and if you're not familiar with garbage collection we'll go over it in just a little bit so instead of having a gigantic cluttered Warehouse to sift through all the tokens in a large context window this Innovation mathematically constrains that working space to more like a tidy well-organized closet there's less to keep track of it's mathematically simpler and and then to continue the analogy which one would you rather be responsible for organizing would you rather be responsible for organizing a nice tidy closet with clean labeled shelves or a gigantic Warehouse with stacks of boxes it's pretty obvious which one is going to be easier to operate in and this is this this analogy is why llms that have this constrained working memory tend to perform better and it prevents that dilution and degeneration because it says you know what instead of trying to keep track of all the tokens just keep track of the most important ones and let go of the rest and one thing that I that I observed is uh if you just have one room it's pretty it's pretty impossible to get lost and so that's kind of that that lost sensation is what you notice when uh you know particularly on longer chats like Claude with its hundred thousand token window it gets so lost in the space of what you're talking about because it has it doesn't have the mental ability to kind of zoom in and remember which specific bits are relevant and so in uh to to further the comparison to human memory imagine you're reading a novel you imagine you're reading a series of novels you're reading Lord of the Rings which is a million I think it's 1.2 million words or something like that um so you're reading Lord of the Rings and there's something that happens you know later in the in the you know near the the end of the books and it refers back to something that happened in the first book so that was a million a million tokens ago but you can remember very precisely what that one thing was because your working memory is constrained and once that once that bit of information that happened a long time ago is brought back into memory you kind of get rid of the rest so it's about it's about maintaining a level of salience and relevance now that's again this is not a perfect metaphor but um given my knowledge of Psychology and Neuroscience this helped me build an intuition for what this paper is presenting so another thing to keep in mind is these are easy tweaks in this paper they tested it on three different open source models and they had the same generalized results generalized Improvement just by changing the math uh Within These these three models so it's just little tweaks it wasn't a brand new attention mechanism they didn't have to even retrain the models it is a drop in solution to automatically improve any open source model it could work on closed Source models but obviously like you know they don't have access to gpt3 or gpt4 to you know pull it open and try it they don't have access to Claude but because of how easy this mathematical Improvement is I suspect that future updates for all the flagship models out there you know chat gbt and Claude I suspect that they will integrate these kinds of improvements uh relatively soon and so the takeaway is that on the really long conversations that you have with these chat Bots where there's a huge amount of context it's going to have a lot more precise memory which is going to give it get it kind of across that uncanny valley that it's currently stuck in where like hey you know as a human I can read this whole conversation and remember the Salient bits but you seem to get lost or you seem to forget the detail that I told you last time so another thing to make it more relevant to computer science I also analogize this to garbage collection so garbage collection if you're not familiar with this is a process that many compiled languages use or interpreted languages python has automatic garbage collection in The Interpreter uh but garbage collection looks for unused memory that the the program has allocated so as you're running a program it's uh you know calling up variables and lists and arrays and other constructs uh and then sometimes those get orphaned you stop using them and but if you don't uh if you're not careful about it those will accumulate in memory and so this is what's called a memory leak and so garbage collection keeps the run time running lean and efficient by just by banishing by fully deleting from memory anything that's not being used anymore so while again the the way that this mechanism works is not anything remotely like garbage collection functionally uh it is it is analogous to garbage collection so I want to drive that home like calling this garbage collection is a total misnomer and I'm aware of that but because garbage collection is something that exists in computer science already I figured it would be a useful kind of saying hey it's kind of like this thing but in this other domain um so here's a way to compare it the Lambda mask privileges retaining the newer local context it actually has two legs um so there's there's two two uh primary mechanisms at the Lambda mask uses to make decisions but I'm not going to get into the details uh because that'll just bore you um anyway so the Lambda mask privileges retaining the newer local context while masking away older tokens so masking older tokens is similar to garbage collection and that you remove it from memory so you see you see where I get the idea from uh and the other thing is garbage collectors as I mentioned prioritize collecting older unreferenced data first so basically you clean up the space by just getting rid of the stuff that you don't want it's like Marie condo this token does not spark Joy get rid of it I'm really ashamed that I just used that metaphor uh anyways the point is both of them aim to create free space and reduce uh clutter and memory without losing the critical information okay so what uh as I said one thing that I expect is that because this is such an easy drop-in solution you're going to see uh just this kind of transparent Improvement um in pretty much all language models probably relatively soon because again like it's a really quick tweak uh and now that's that's short term but long term what do these kinds of algorithms uh algorithmic improvements mean for language model and language technology well like I said earlier one of the primary constraints that we see is just that you know that that forgetfulness that language models have where they don't really remember what is the most important bit of a conversation or a large context to pay attention to and so while this doesn't get it quite to the level of precision of human recall where like you know hey you know like Bob remember that thing that we were talking about three years ago and you can instantly recall that into memory and and hold that that you know episodic memory in isolation it doesn't quite solve it that way we probably still need like search and recall and and fetch and databases and Vector search for some of those kinds of functions but as you're reading a long document whether it's you know a 200 Page uh criminal filing a class action lawsuit or a novel it's more like the ability to recall like oh you know three chapters ago this character said this to that other character that's the kind of level of precision that we're looking at um and that's very similar to that passkey retrieval from a 32 000 context window so imagine that you read a password at the beginning of a twenty thousand page document and then by the end of the document you can read you can remember that passkey that's what this allows so that is that is a uh you could you could say that that could be superhuman level of recall um all those there are plenty of people that do that would have the ability to say that password is important I'm going to keep that in memory even as I remember or as I keep reading the rest of this document so some other examples as to where I see this being really helpful reading and writing fiction like I said keeping track of the the most Salient tokens within uh you know a field of right now the the most that they tested was thirty two thousand uh but these were this was a test on thirty two thousand token context windows with language models that were trained on 2000 so there you have a factor of 8X no almost 20x sorry uh you have a factor of almost 20x Improvement so in the future six months from now 12 months from now when we have uh more and more models that have hundred thousand five hundred thousand one million context uh token Windows uh being very commonplace I suspect that that means that like no matter how long the conversation is you say hey remember when we talked about this thing you know X you know many months ago and it'll be able to recall exactly that right memory um scientific research so a lot of people are plugging uh a lot of scientific papers into things like Claude and other tools so imagine that you have a context window that can ingest 100 scientific research papers and is able to pick out and remember the most Salient details from each of those papers to easily keep them all in memory and kind of triangulate say okay this is the pattern that I see and I'm able to recall exactly which line in each of those is relevant to put put this into context for Enterprise and business and another thing that this could be really helpful is Imagine you've got you know customer records or internal knowledge base articles and rather than have to read each one and even like split them up and chunk them you just say here's the entire knowledge base read it tell me what's relevant to this problem right here so this is going to greatly greatly accelerate and simplify search and recall and it's going to really increase uh retrieval accuracy particularly on large documents so while this one Innovation might not carry us fully to to this realization this is just one Innovation and remember we're getting like something like 50 llm papers per day at least just on archive sometimes more so this is just another drop in the bucket but it's to me it's a really encouraging sign that we have not fully explored this space so this is taking a big step back when you can just pick this low hanging fruit and you get this level of improvement you get a 10x Improvement on context window retrieval just from changing the math a little bit we are still establishing the first principles of language technology so we don't know where the ceiling is like we just we just hit escape velocity on language models and we have no idea how high this can go so I'm actually getting chills thinking about how exciting this is and what this implies for the future of language technology because that level of memory sophistication I didn't I personally only six months ago if you if you had told me that you could you could sift through and keep track of 32 000 tokens with this level of precision with only a slight mathematical tweak I would have been like well that completely blows everything that I think about the way that these models are going out of the water and the reason that I say that is because I noticed that dilution pattern uh that that happens and I thought that Ah that's just not going to be a solvable problem we need external memory systems to handle uh complex memory tasks and language models but now I am thinking that is less and less going to be true so with all that being said thanks for watching I hope you got a lot out of this and I hope you're as excited as I am and you see that we are just at the ground floor and we have no idea how high this elevator is going to go so thanks for watching take care and yeah see you next time