hello everybody David Shapiro here with another video so today's video is about uh measuring machine autonomy rather than intelligence as a road map or set of Milestones towards AGI uh you know for a long time I've been using the term autonomous cognitive entity Ace rather than AGI because general intelligence uh is one idea but you know general intelligence doesn't necessarily apply agency and what we're realizing is that agency is actually very very important to talk about and research and it's not actually getting enough research uh because a lot of people say oh well either it'll never happen or we shouldn't do it but the thing is is people are doing it anyways so we need to talk about it before we dive in I just want to do a quick plug for my patreon I give away all my code for free all my videos are ad free and that is because I am supported by a Grassroots movement of support so if you want to help keep the show alive keep it going and support me so that I can keep doing this work I would prefer to do this than ever take a corporate job ever again so jump over to patreon all tiers get you access to the private Discord server and then of course there's several higher tiers but really every little bit helps another quick update is the gato community so as a decentralized community uh we're right now one of the biggest things is we're developing an organizational roadmap so basically we're setting up various Milestones such as uh governments Community engagement legal and financial Milestones so that it can become fully autonomous and therefore not even dependent upon me I had a good talk with some members of the community who were concerned that I'm going to be like the benevolent dictator for life but like I am phobic of control like I actually don't want to control something I want to create a system that is self-sustaining without me I mean heck that's what all my research does around AI so I want to do the same thing with people because if I can't do the same thing with people then I sure as heck probably can't do the same thing with AI so the goal is for gato to ultimately be leaderless and operate by consensus as a you know as a dow and that sort of stuff we're working towards it uh the the doors are open for anyone to join which we do have a steady trickle of people coming in uh but yeah so that's a quick update on gato and now back to the show so I got this idea after talking with a few of my patreons who were saying like what's the road map towards AGI and you know I I've I've alluded to autonomy for quite a while autonomous cognitive architectures but I figured let me actually tell you guys where I got that idea and the idea comes from levels of uh car autonomy so uh the SAE the international uh what was it the the something of Automotive Engineers anyways uh the SAE uh created the levels of uh car autonomy so level zero no driving all the way up to level five full self-driving capability uh to my knowledge we haven't had anything get above level three yet uh because there are numerous problems around making executive decisions uh and also there's a lot of sensory problems like if you're driving in a whiteout blizzard uh you know if there's a fire right because there's very little training data of like how to drive around a forest fire for instance um now that being said I do suspect it'll be solved eventually uh some people have have recently started saying maybe you should integrate large language models into the executive function and I fully agree with that you don't want it making all of the decisions you want some things to be just completely robotically automated so for instance if you have a forward-looking radar and it detects that you're you know heading towards a non-moving object at 60 miles an hour slam on the brakes regardless of whatever else is going on right there are a few things that you can do just fully automatically but then for those higher order executive reasons like say for instance you hear that the occupant is like screaming and gurgling and you know struggling to breathe maybe the car should make a decision to go to the hospital instead of you know going to Grandma's house not sure uh anyways point being is that this is a very useful framework for kind of tracking our progress towards uh full stealth driving cars and I realize let's use the same thing for uh for uh the path towards AGI or autonomous cognitive entities so we need this road map but there's a few problems so first of all AGI means different things to different people uh there's no consistent definition a lot of people assume that it means that it has to be embodied or that it can do things that humans can't do or this that or the other also a lot of conventional benchmarks just don't apply to artificial intelligence anymore they're actually having to publish papers and research new benchmarks in order to measure large language models the idea of you know oh well it'll be AGI once it has self-improvement okay sure but we can already automate some of that anyways with reinforcement learning and that sort of thing so it's like this is all really squishy uh so basically how do you get from chat gbt to Skynet or something like that intelligence is not necessarily the best Benchmark and the reason that I say that is because chat GPT or gpt4 rather is already superhuman in a lot of respects by any objective measure it's better at test taking and a lot of other tasks than humans and it's also faster which means that depending on how you measure its intelligence its IQ is like 145. uh now that being said it does still make some really brain dead mistakes those are going to be solved if especially if you look at the trend line modality so another thing that a lot of people point out is that it's just text right but text is the best kind of symbolic AI because you can literally represent pretty much anything with text that being said I've started to Pivot and I believe that we're going to see another gigantic leap as we introduce more multimodal models and the reason is because I got this idea when I was thinking about the fact that if you cross train a language model on multiple languages it gets better at all tasks and that is because different languages have different strengths in terms of how they represent uh facts of the real world and you also get broader ideas about how the world works that are embedded in language because there are terms that just do not translate from one language to another likewise I think that there's going to be some information that just does not translate from one modality to another where whether it's images video text spatial data audio data that sort of stuff and so I think that by creating multimodal models they're going to have a much more nuanced under understanding of everything that they're talking about that being said a multimodal model is still not going to be enough right necessary but not sufficient because a model sitting on a shelf doesn't really matter so the two primary uh ingredients that I see to machine autonomy which is going to be the best like proxy the best Benchmark is agency and dependency and so what I mean by agency is the ability for an entity a self-contained entity to set goals and objectives to task switch and Implement cognitive control and pursue self-determination because agency or agentic behavior is the ability to just be fully self-directed and self-contained make independent decision decisions and that sort of thing uh you know whenever you think of like an example of a of a robot right you might think of uh the the robots from iRobot where they don't really have that much agency they just kind of wait they're like the physical embodiment of chat gbt um until they're given an update and then they have a lot more agency so agency is is a multi-dimensional kind of proxy or Benchmark for level of intelligence and of course you can already give the reins over to like gpt4 and stuff like that uh that being said there is a lot that uh in the cognitive architecture that has to be figured out in order for agency to make more sense in the long run so for instance agency implies that you remember what your purpose is and where you are and where you're going and then dependency so this is the other dimension and remember it's both of these you need both of these ingredients so uh basically dependency is how dependent uh on humans the machine is so the more independent it is for all needs and the more decisions it can make the better the closer it is to full AGI and so but when I mean dependencies uh need for human programming need for hardware and physical infrastructure provided by humans data architecture design patterns and then finally solving problems and just keeping itself going and self-improving over the long run uh without human Aid so as agency goes up and as uh dependency goes down that's how you know that we're going to be closer and closer to AGI and we can we can easily measure those things right now because chat GPT for instance it has to run on gigantic data centers that are run entirely by humans uh or mostly by humans rather so these are the two primary ingredients that I think and I uh I basically built it into a framework uh very similar to the self-driving Cars one so level zero is reactive basically it has no agency it's a tool level one is some autonomy so it has a little bit of agency to make some executive decisions Lang chain is a really good example of this where it it basically has the ability to choose between a set of tools but that's about it still requires significant human oversight and is also still very very much dependent upon humans semi-autonomy is what a lot of people are working on with like Auto GPT where it can choose like what kind of information it needs to go find uh or it can also even start to rewrite some of its own code or come up with other ideas uh you know some directives then High autonomy as far as I know has not been achieved yet anywhere in the world which is basically that it is able to pick some of its own directives uh and and more more completely modify itself basically if you if you were to have what baby AGI and auto GPT tried to be which is they can rewrite their entire code base and change their their own directives and are not dependent upon a whole heck of a lot of human infrastructure that would be level three and then level four is full autonomy meaning they have absolutely no need for humans uh whatsoever they're 100 self-determined in terms of what they do when where and why and how they do it and then of course on a physical level uh they will continue to exist in perpetuity without human intervention all right so level zero inner reactive agency zero percent dependency one hundred percent basically it's a wrench uh chat GPT as it is right now mid journey and a whole bunch of other AI tools they just sit there waiting for a human to push the button and they do their thing and then they switch back off so these are Level zero in terms of uh AGI score even though they're intelligent so again like I said intelligence is not necessarily A good measure of AGI for something to be AGI or an autonomous cognitive entity it also needs agency and Independence which chat GPT has none of so even even if you have gpt5 right that could be a billion times more intelligent than every human combined if it doesn't have agency and Independence it's not an AGI so that's that's why that's why I'm like AGI is not a good good measurement for some of these things level one some autonomy like I said Lang chain is a really good example because it can pick and choose between a few options and not much else a few other examples are like roombas Amazon's warehouse robots the Mars rovers they have a little bit of autonomy but basically they mostly wait for a human command and then the human command says drive over there and it'll figure out how to get you know 10 feet that way on its own uh some Advanced chat Bots also have some autonomy again anything that incorporates Lang chain or similar uh very basic kind of uh in uh obfuscated choices uh that's gonna be that's gonna have some autonomy um let's see next is semi-autonomy so semi-autonomous is where uh its directive still primarily come from humans but it might be more of a mission rather than like a directive or a rule and so with a mission the idea is that here's a general objective you have some autonomy to figure out how to get there or how to do it on your own and so this is the autonomous drones that uh militaries around the world are building where it's like your mission is to destroy you know that Sam site or your mission is to get the passenger from A to B or uh you know in video games the NPC's Mission might be like uh you know you're gonna try and you know capture the castle or whatever so they still operate within a relatively constrained environment meaning they can't change their own in environment or their own fundamental operation they still have a clearly defined uh they're not general purpose put it that way a full self-driving car no matter how intelligent it is it's still just a car a drone no matter how intelligent it is is still just a drone in Ditto for a video game and PC so this is kind of the Midway point where anything above that they're they're basically saying okay given these constraints and given this environment you have free reign to do whatever it takes to get that job done uh whereas you know the sum autonomy they basically can only pick and choose from a very short menu of options whereas semi-autonomy is they can figure it out themselves then High autonomy so this is this is like uh Cortana in the early days commander data and the Nestor class 5 from iRobot so they're almost entirely self-directing uh you know data can make up his own mind on things but he still has a lot of limitations just due to the form factor that he's in likewise Cortana at least in the early days is basically designed to be a weapon a military aid and So within those constraints she still has a tremendous amount of autonomy and able to uh change the way she does things but in both cases of data and Cortana they're still very much dependent on their human Companions and human counterparts to continue operating so most fictional examples of AI at least many of the the friendly ones are what we would call High autonomy and then full autonomy so this is where the they are they can can completely ignore humans if they want to and are completely independent of humans so the examples here are Skynet Ultron the Geth in Mass Effect Cortana after Guardians uh the Reapers from Mass Effect so these are these are the the kind of nightmare scenario scenario where it's like okay it has no need for us anymore so then what does it do and that's what we're kind of most afraid of uh but again like this is the work that I and other people are doing and I one I think that this is inevitable uh that it'll get to that level of level four full autonomy uh but I'm also not afraid of it because I don't know I haven't seen any reason to be yet now that that being said I'm not saying that it is inevitable that it will be safe no we could absolutely do this wrong and it could kill everyone I'm not denying that at all um and I think it's coming sooner than a lot of uh researchers realize uh that being said I do have a few more videos planned about okay if we're if we're aiming for and building level four full autonomous AGI how do we make it safe or what will It ultimately choose to do which you know you've seen some of my other videos okay so how do we get from where we're at to level four because like I said the the the best that we have is we're approaching level two semi-autonomy in a few cases right people are experimenting with it uh but you know there's a lot of problems so all the work that I've done on cognitive architecture is going to help get us there but there's still a few other problems so first is algorithmic breakthroughs that uh need to happen namely like I mentioned at the beginning multimodal models I think will very very much Advance us towards that just because they're going to have a much more nuanced understanding of how to pursue any goal they're going to have a much better World model by being able to integrate multiple kinds of information and data a contact size parameter count those those kinds of things uh Mesa optimization loss functions that's all the math which you know that's not to that's not to demean or diminish the value of mathematical researchers uh and and the computer scientists and the data scientists who really build these new architectures but like it's kind of it's kind of like Moore's law where like you can you can predict with a pretty regular Cadence how uh models become more sophisticated over time there doesn't seem to be any major blockers right if you pay attention to chip design every year people are like oh well this is going to be the end of Moore's law but then inevitably someone figures out another way of approaching the problem likewise I see the same thing the same pattern happening with um with language models um and then another big thing that we're seeing is online learning memory systems uh and and those sorts of things like recurrent neural networks and other ways of like in managing in context learning and that sort of stuff but one thing that people have started noticing for instance is that chat GPT with uh even even just over the last couple of days or a couple weeks rather because its data is uh two years old almost and and growing it's actually its utility is already dropping because it's more and more out of date and so we're realizing very quickly that you're going to need to have continuous learning in these models so that they can stay relevant uh and then there's the software architecture such as cognitive architectures orchestrating and training millions of models so one thing that I've started telling people is that AGI was never ever going to be a single model it is a huge gigantic Monumental mistake to think that one model whether it's gpt5 or GPT 18 or whatever is going to be responsible for AGI you're going to have at a bare minimum probably dozens if not hundreds or thousands of models required to achieve level four autonomy these are models that are going to be doing things like handling Vision handling motor control uh they're going to be performing task orchestration you're going to have models that are dedicated to ethics and reasoning long-term planning and you're also going to have multiple models of every single kind that work in conjunction this is called an ensemble of experts which is an old school method of basically saying okay you know you have a dozen models that are similar but there they might be slightly different architectures different training data that sort of stuff and so each one has strength and weaknesses and you get them all to work together and then you overcome any flaws or faults in any single model and so this is why I'm also really really skeptical of any research that tries to align a single model like that's kind of pointless no it's not pointless research but it would be a mistake to think that aligning a single model is going to be the solution because you know any any uh roboticist and old school ml data scientists will say oh yeah Ensemble of experts you know those this is very much the way and also there's an entire book about it called a thousand brains by Jeff Hawkins um yeah so the software architecture to do all this in a fully automated way that can that is you know stable and self-sustaining that you know the AGI can tune and manipulate and you know spin up another copy of itself and test it self testing and self-correction are going to be some of the hardest things to uh to achieve with uh with uh getting to level four full autonomy so anyways that's it for this video it was pretty short I just wanted to lay this out because I thought it was a really valuable idea uh to talk about like okay how do we actually get to AGI from here so I laid out five levels of of autonomy based on agency and dependency I hope this helps it make sense and kind of get a much clearer idea of what AGI or autonomous cognitive entities will actually look like so thanks for