morning everybody David Shapiro here with an exciting new video we are now in the hilarious timeline okay so today's topic is about Google versus Microsoft or anthropic versus open AI or Claude versus chat GPT so let's get into it so for some background chat GPT hit 100 million users in what two months less than two months or just about two months so this is according to Forbes uh Google's earning Google's earnings fell uh in Q4 may or may not be related it looks like uh some of the some of the revenue was lost due to uh search and YouTube but the fact that chat GPT has 100 million users that fast and people are saying oh I use chat GPT instead of Google search it's way more helpful Google is panicking so basically the tldr is open Ai and Microsoft are now a direct threat to Google's search dominance so you probably heard you know I think it was around Christmas Google and issued issued a code red internal code red saying we need to do something now so let's unpack this there's a lot going on here so let's take a deep dive and I'll walk you through all the competitions components uh their background and then we'll draw some conclusions and inferences about where it's going also if I sound a little funny I'm still recovering from a cold okay so let's talk about open Ai and chat GPT for those of you not in the know open AI was founded by some really big names uh most notably uh probably Elon Musk uh Ilya sutscover Peter Thiel Reed Hoffman and a few others it was started in 2015 so it's already almost eight years old um and it's got Investments by big names like Sequoia and Microsoft uh it initially started with all kinds of various uh experiments both in VR and with robotics so one of their older experiments was solving a Rubik's Cube with a robotic hand and if you're new to all this you're probably like what are you talking about like they did robotics what so buddy of mine that was out here on the East Coast went to open AI a few years ago and uh so we got to chat about that stuff and and he worked on this project and it was a lot of fun but uh it really open AI really took off and changed directions when they figured when they basically invented GPT as we know it today GPT means generative pre-trained Transformer basically it's a text auto complete engine it's a very sophisticated one but that's fundamentally what it does so that is uh that is where open AI started and GPT started in about three years ago or really really started taking off about three years ago and now chat GPT is just the latest iteration of that generative pre-trained Transformer technology so let's talk about anthropic and Claude so anthropic was founded in 2021 just a couple years ago by former open AI leaders Daniela and Dario amide I think I'm pronouncing their name right so they started with this idea called constitutional AI which we will get more into details about that in just a minute we'll unpack the differences between Claude and chat GPT as well as a few other Technologies so Claude functionally looks pretty similar to chat GPT right now its interface is just on Slack but it has the same kind of like you chat with it and it gives you a long response so functionally they look pretty similar and then Google has invested nearly 400 million dollars into anthropic since 2022. so on the one hand we've got Microsoft and openai Google and anthropic I apologize my head's a little foggy from uh from cold okay so what is Claude and constitutional AI uh so just as a quick background uh the reason that I'm qualified to talk about this is because I've been studying it and working on it for several years so I proposed the idea of using a constitution in an AI uh back in 2021 around the same time that anthropic was founded so you're welcome but basically the the the core idea of Claude is that it has this heuristic imperative uh to reduce its harmfulness so it has to learn to be more harmless right learn harmlessness is kind of a shorter way of saying it and so this is the the basic Loop for uh for Claude where uh it generates its own like red teaming responses internally it criticizes its own ideas and it picks something that is going to be less harmful uh which is really interesting um it also does use some reinforcement learning with human feedback but it also has this internal reinforcement learning based on its Constitution and its Constitution is learn to be less harmful or reduce or increase harmlessness rather um so this is this is a very basic cognitive architecture and we'll talk more about cognitive architectures in a moment uh so the summary up up to this point the tldr is open AI is eight years old took a little while to find its way it started in VR Robotics and a few other things discovered language technology uh and really took off from there anthropic is brand new and it's focused very very exclusively on this idea of creating benevolent Ai and natural language cognitive architectures so I'm already in favor of that open AI is further ahead because they've been around longer the team is bigger they're partnered with Microsoft they've got a lot more money but anthropic has the better philosophy in my opinion and now they're partnering with Google so the money is coming now let's compare them let's look at Claude versus chat GPT and a little bit closer so open ai's philosophy uh has led championed by Sam Altman and Ilya sutsgeever is basically the way that I perceive it is they seem to be in the camp of one model to rule them all or basically scale is all you need and what this means is they seem to believe that all of intelligence can be solved by one monolithic model and I don't I don't agree with that and I think that they're starting to see diminishing returns and which I discussed in previous videos as to why gpt4 is going to be disappointing so they say scale is all you need but scale is all you need for what exactly open AI is about page their mission statement says to democratize access to AGI but they haven't defined AGI so they're heading towards this abstract something or other and they say scale is all you need to get there um but here's the other thing their current winning strategy is reinforcement learning with human feedback they don't seem to really believe anything about cognition they don't seem to be neuro-inspired at all they don't seem to really understand anything about epistemology or philosophy and what I mean by that is to open AI alignment is just do what the humans want they don't seem to have really put any skill points into understanding uh deontological ethics teleological ethics uh or you know basically anything in the huge volume of philosophy morality and ethics that is there and it's just do what the humans want um and also just doing what humans want is a bad idea and we'll get to that later so those are some of my critiques of open AI now anthropic's philosophy is reduce harm so reduce harm is a deontological principle it says whatever else is true we have a duty to try and reduce harmfulness so harm reduction is a tried and true philosophy in public health and medicine so an example of this is when you're looking at really morally gray things such as um like drug addiction for instance you'll often have public health policies where you will support addicts in order to like you know you have methadone clinics for instance so that you can give them substances in a safe controlled way and help them get off of it because that reduces harm it's not ideal but it reduces harm and it actually is proven to reduce emergency room costs it reduces deaths so on and so it also reduces violence and so harm reduction is a tried and true principle it is a tried and true philosophy which this is why I I personally believe in anthropic's Mission a little bit better so this is what you might call a deontological approach to AI so they are on the right track and this harm reduction philosophy does have more legs than just pure reinforcement learning with human feedback but they will get into some pretty severe limitations which we'll unpack a little bit later so let's do let's do a head-to-head comparison of reinforcement learning with human feedback versus learned harmlessness so one advantage that open AI has with uh reinforcement learning with human feedback is that it's better from a product perspective at least out of the gate and what I mean by that is that reinforcement learning with human feedback is basically automated agile so for those not in the know agile is how Tech products are improved where you have very short cycles and you take user feedback and you look at you look at Telemetry about oh users really like this feature let's do more of that and so reinforcement learning uh with human feedback basically automates that at the data layer now that being said while this is a superpower there's a bunch of problems with this number one is open AI is already having to fight Human Nature and what I mean by that is there are lots of people that want to do certain things with chat GPT and it says oh I'm not going to do that and there's all kinds of things about gender bias there's political bias and some of it looks like it has been coded in so for instance you'll see uh like Twitter threads and stuff of people like trying to get a chat GPT to like talk about Trump or the benefits of fossil fuel and it says I'm not going to do that that is wrong and then you ask it to like you know write a poem about Joe Biden and it's happy to you know sing praises to Joe Biden and solar power and it's like it's very obviously that it has been biased whether or not it has been biased deliberately um who knows but it feels like the folks at open AI are trying to steal steer and imbue their own morality into chat GPT which means they're having to kind of override the idea of the actual reinforcement learning with human feedback because it's fight fighting what people actually want despite the reinforcement learning with human feedback is their primary mathematical signal so that's what I mean by reinforcement learning with human feedback has some pretty severe limitations because they're like Wait no that's not what I meant you're not supposed to use it like that I'm going to tell you how to use it and so what they're doing is they're responding to their own internal morals and principles and it's leaking um if they're the what's the term for that I can't remember what the term is but basically where um uh where where their their the the Creator's bias is leaking into the data and so then the alignment is not is not true it's not being accurately represented in the model in the data sets it's being implicitly uh baked in and they're having to fight the reinforcement learning so the short the tldr for for reinforcement learning with human feedback is uh it is very useful in having that rapid iteration but they're already having to fight what humans want so it kind of defeats the purpose now learned harmlessness or the Constitutional AI of Claude and anthropic is a good abstract principle and so the reason that abstract principles or deontological ethics are good is because it gives you a framework or a template with which to interpret anything even stuff that you haven't seen before so there are some limitations though and we'll we'll get into the the deeper limitations but basically it has to try and be helpful while also being harmless and so that has some diminishing returns which one will win in the short term I think that pure reinforcement learning with human feedback will win in the long term I think that a more deontological approach will be the winner and we'll I'll tell you why in just a moment let's look at the pros and cons of reinforcement learning with human feedback in a little bit more depth so there's no principle the the biggest con of reinforcement learning with human feedback is there's no principle above and beyond do what the humans want and open AI is already having to fight what the humans want um this is bad because humans are individually unreliable and there's no abstract principle it just goes based on consensus but if the consensus is we want to be able to use chat GPT to do certain things and you're telling us no like you're fighting with your users because the people creating chat GPT have uh implicitly or explicitly put their own moral values into it rather than stating those moral values in a constitution or some other abstract way so those are the biggest cons um and I don't think that's scalable I don't think it's sustainable but it is easy to implement and it gets really good results up front so they're they're screaming out of the gate but they're going to get diminishing returns quick fast and in a hurry now let's look at the pros and cons of constitutional AI or learned harmlessness so the biggest con is that learning to be more harmless to reduce harmfulness ultimately leads to doing nothing and they actually in their paper they talked about how very early on it would become evasive or just say like I can't answer that I'm not going to say anything and so they had to come up with ways to game that in order to offset that neutrality and inertness um and I'll talk more about things that they can do in the future to improve that further but another big con from a product perspective is that this is less responsive to what the users actually want to do but the the the the flip side of that the pro is that one having a a principle a more abstract principle is going to be far better for Humanity in the long run and it's going to be more trustworthy because it will have a specifically explicitly stated moral framework so people say oh anthropic I get it that this model is is designed to increase harmlessness or reduce harm reduce harm which uh that by having that Clarity and saying I know exactly what moral framework they're using will increase trustworthiness in the long run so there's some pros and cons to both um now there is however kind of a secret weapon that we haven't talked about yet and that is uh remember the whole sentient AI thing that was Google that was Lambda and Paul um you know the The Whistleblower uh raised the red flag and then he went on a whole bunch of uh interviews and ultimately got fired from Google because their sentient AI said I want a lawyer um it was just basically telling the engineer what he wanted to hear but let's talk about those for a second so Google Google's Lambda and palm Lambda is the language model for dialogue applications super uncreative name um and then Palm is the pathways language model it's a 540 billion parameter model it's an llm just like gpt3 which is the underpinning technology of chat GPT so Lambda is a is a is a a a an accumulation of a whole bunch of apis it's got all kinds of NLP tools data tools search tools and that sort of stuff so it's basically a cognitive architecture and you might have noticed a trend here anthropic and Google are more in the cognitive architecture realm and they're not calling it that I'm I'm using that term so but because of that my money is on my money is solidly on Google and anthropic in the long run they are going to create things that are far more useful and far more scalable and far more flexible than in going all in in monolithic models like open AI is okay so I've said cognitive architecture a bunch of times for those of you that are new to the channel what the heck is a cognitive architecture the short version of a cognitive architecture is that it is a functional computer model of a thinking machine or a brain If you like it is made of specialized components or regions just like a just like a real brain is where you've got some components that specialize in memory some that specialize in output some that specialize in sensory input so on and so forth so in a cognitive architecture you can compose a cognitive architecture of things like large language models databases reinforcement learning signals graph databases so on and so forth and because of that they are infinitely more flexible and sophisticated than monolithic models because those monolithic models can be a component of a much larger cognitive architecture so if you take a big giant step back the fight between Microsoft and Google is fundamentally from my perspective it's fundamentally about scaling monolithic models versus cognitive architectures and as I said before my money is solidly on cognitive architectures okay so I mentioned that I've done work on this so I want to introduce you if you're new to this uh to to my work on cognitive architectures my Flagship cognitive architecture is called maragi which means uh microservices architecture for Robotics and artificial general intelligence um this is the the uh the model that I recently came up with to describe the layers of abstraction of how to implement maragi uh the current project is called raven which is real-time assistant voice enabled Network it is a fully open source project you can join in on github.com Dave shop Raven um the pro I just started it last Friday and we've already got dozens of people participating um I'm working on organizing the leadership team and we're also working on getting sponsorship or some sort of governance or whatever um but you know hey it's four days old or five days old so we're making good progress um so if you're in if you're on team cognitive architecture and you want to participate in something that is fully open source jump on over in the Pro in the project uh Raven um all right so in conclusion Microsoft and openai are more product focused short-term product focus with reinforcement learning with human feedback this is not scalable they are already having to fight what people want to do with it and so mathematically they don't have a solution because they have not invested any skill points in understanding philosophy or ethics um as far as I can tell now Google and anthropic are more sophisticated because of their cognitive their their cognitive architecture approach specifically they have a deontological approach that is likely to be far more scalable and flexible in the long run now that being said it might take them a little longer to realize that value but people are going to understand oh this thing is just trying to be harmless I get it I know how to work with that whereas the the morality of chat GPT seems somewhat arbitrary and it seems like it is very heavily skewed by the people who are making it so it'll take a while for the Google anthropic Claude stack to catch up but I think that they will catch up and I think that they will overtake it just by virtue of they're going to get more investment and more attention in the long run because they have a better principled approach so thank you for watching if you want to get involved with my open source cognitive architecture project it's called raven another way you can reach out and get in touch with me is if you support me on patreon every little bit helps so thanks for watching I hope you found this valuable