what does AI actually understand one of the most common things that we see and hear is that AI is no more than a stochastic parrot and of course I'm one of the first people to say that humans are no more than a stochastic parrot because why when you say that AI is a stochastic parrot you are just parting something that you heard based on your training data now rather than getting lost in a petty internet squabble over what is true intelligence and what is true understanding there was this bombshell paper that came out just a few days ago October 3rd by Max tegmark and west gurny of Massachusetts Institute of Technology MIT Max techark is as you may or may not know uh the guy who pinned the famous pause paper calling for a six-month moratorium on AI I watched some interviews with him and he knew that it wouldn't work that was not the point the point was just to raise awareness which it seems like that succeeded um but yeah so max Tech Mark also wrote the book life 3.0 where he talks about the possibility of substrate independence meaning that there's no reason that uh machines could not eventually be conscious in their own way so this paper it is titled quite simply language models represent space and time the long story short is that just by training on corpuses of text corpora of text large language models in this case they tested llama 2 so not even a Frontier Model is able to uh embed the concept of geospatial data and chronologically linear time and this is something that one is not surprising to me and I'll explain to you why in just a few minutes but it's really important because not only does it encode the ability to uh understand you know relative spatial data and chronological linear time they were actually able to identify the specific parameters or the the neurons that allow it to encode this and so what this implies and why it's not surprising to me is that by reading gigantic mountains of training data uh just through text just through semantic similarity it has to learn to generalize into more and more abstract Concepts such as space such as time so we saw this with other recent papers where just training language models on two-dimensional data they are able to generate a three-dimensional understanding of the world and there's uh another video or another paper that came out recently talking about how just training uh models on linear sequences of moves for such as like chess and other games even though their input data was onedimensional they were able to understand and infer and in generalize a two-dimensional play Space so the pattern that is emerging and again this is not surprising to me the pattern that is emerging is that when you have enough training data and when you have a sufficiently deep neural network it is able to generate increasingly uh abstract representations of the problem space in which it is operating so when you talk about the ability to generalize knowledge the ability to generalize intelligence it is no longer just a stochastic parot it has an internal model of the space that it's working in image uh image generators they have a three-dimensional model of the world so that they can understand threedimensional relationships between uh between objects in order to better generate uh output images so this is not surprising to me because of a book that is very popular now I was not a big fan of this book Thinking Fast and Slow by Daniel conoman you can see I bought this back in 2015 the reason that this book was not impressive to to me is because anyone with a modicum of like metacognitive skills or meditation or anything is like well duh so if you're not familiar with this book basically he argues that there are two quote systems of the brain and he he goes through Great Lengths to say that these are not neurological systems that these are basically just two modes of using your brain so there's system one thinking which is fast thinking which is uh intuitive it's instantaneous it's knee-jerk it's off the cuff and then there's slow thinking which is deliberately slowing down to think through things and piece through it and use your brain to kind of steer the thought process and iterate upon thoughts so if you're familiar with my work in cognitive architectures this is basically how I approach cognitive architecture where a large language model with one instance is system one thinking it just gives you an off-the-cuff intuitive response with no double-checking the point of a cognitive architecture is explicitly to give it system to thinking uh so that's that but having watched the space for a while starting with data skeptic a podcast uh that I started listening to many years ago and uh brain inspired a neuroscience podcast about AI uh it has been obvious to me for a while that deep neural networks depending on what the problem space is they generally learn to abstract uh kind of the the space that they're working in in order to come up with these some of these more abstract Concepts in Vision models we see that uh that that uh what they do is they discover boundaries they discover gradients they discover all the same kinds of things that the human optic nerve and occipital discover about how to process images uh likewise as uh deep neural networks learn as elucidated in this paper by Max tegmark and some of these other papers talking about theory of Mind basically in order so even though the objective function is just to predict the next word in order to accurately predict the next word it needs an internal model so that it can accurately predict the next word and this is very close to how human brains work because you know it's not as popular now but 5 10 years ago people were saying oh yeah the human brain is mostly just a prediction engine that's what we do like we predict the next word in in a comedy routine and when and when uh you you you know your expectations are subverted that's when it's funny that's what comedians do is they they play on those expectations this is how you drive a car you can anticipate what cars going to be where based on your knowledge of how cars move and how people move and the traffic rules and you're able to anticipate where you need to be because you remember like driving to work yesterday and so then you pay attention to the same cues that kind of get you to work on autopilot the following day and so a lot of these things are all system one thinking uh basically where it's like okay based on your training data based on your experience in recall you just kind of can get through life mostly on autopilot and in fact there are all kinds of like neotropic uh brain training uh schemes out there that basically cause you to bank on system two thinking which is you deliberately put yourself in situations that force you to think through things so that you're engaging your brain rather than um working on autopilot uh and so one way to think of it of of of system one versus system two is autopilot versus like manual flying or whatever cruise control versus manual driving and anyways so this is all very profound uh and the reason that I wanted to make this video is because there's this pattern emerging this trend emerging and there's this growing awareness of like what is understanding what does it mean to truly understand something um and there's a few ways to unpack this you can look at understanding from a functional perspective does it produce the right answer how does it produce the right answer um why does it produce the wrong answer right and so just looking at things in terms of benchmarks and Al and and and those uh Avenues there's a lot of value in that but we're getting at a more fundamental question which is what is the similarity between humans and machines because the substrate of human brains versus large language models is very different the training methods are very different uh yet despite that we're seeing more and more convergence and what I mean by converg convergence is not that like we're going to merge into Borg or anything like that but what I mean is that despite the fact that the training data is very different despite the fact that the substrate is very different the thing that we have in common is the problem space we're training uh IM we're training image generators to generate images that that you know we find appealing and they mathematically derive how to do that but then they also in the process of doing that they derive three-dimensional models of the world and other understandings of things uh in order to better generate images that we like likewise language models we train them on texts that by and large humans wrote um and that is comprehensible to humans and so it's no surprise that a mathematical model is able to eventually generate text that we find you know useful meaningful uh and so on it well it's forcing us to to question like what does it mean for us to understand cuz like I often have a tongue-and-cheek joke like humans don't understand anything we only think that we do and likewise language models don't understand anything they only think that they do uh but when you have like tree of thought and and multi-step reasoning that's basically using large language models for system 2 thinking and it's like hey you just generated this offthe cuff thing now check your work and it's like oh well okay we could do better or let's brainstorm this and have a more structured approach um and so this is basically what a cognitive architecture is is how do we Implement systems to thinking um with language models so that that way it's not just an intuitive offthe cuff you know immediate uh just guess um anyways that's kind of that's kind of the long story short of this is uh this is how I think of large language models and AI models in in in general so the rule of thumb moving forward long story short here's your takeaway is if a if a if an AI model if a neural network is a single pass forward a single Fe feed forward inference then that is a system one thinking machine if it has loops if it has feedback mechanisms if it can iterate on the material that it's thinking about it is a system 2 device so this is what we're doing with the ace framework the autonomous cognitive Entity framework where it's actually many many many uh Loops um that are that are basically able to form dynamically as needed that's over complicated it's much simpler than that if you go look at the diagrams anyways thanks for watching I hope you got a lot out of this it just felt really important to say this especially in light of Max Tech marks uh paper which is amazing and you should read it even if you don't understand it just skim it and look at the pretty colors um like it will the things that they're talking about will drastically change the way that you understand how language models work and again this is llama 2 this is not even multimodal models this is not even uh Frontier models that are going to come out next year so anyways thanks for watching like subscribe etc etc have a good one