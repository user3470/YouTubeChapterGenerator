hey everyone David Shapiro here with today's video good morning today's topic is going to be a little bit more severe and a little bit more intense so the title of today's video is Agi Unleashed what happens when we lose control part one the coming storm as everyone is aware things are ramping up very quickly there are people calling for moratoriums on AI research and while some of us don't take it seriously there are very legitimate concerns about what's going on and in other words we're in the end game now we are in the ramp up towards AGI and the singularity whether or not you are emotionally ready for it so just as a tiny bit of evidence this paper from nature demonstrates that we are in an exponential ramp up on AI whatever else is true the investment is there the research is there it's happening and it's not slowing down now I'm probably preaching to the choir and rehashing old stuff that everyone knows but let's just briefly talk about the existential risks there are two overarching themes or categories of the existential risks risks for AGI one is the basically just the deliberate weaponization of AI some people are comparing AGI to nuclear weapons beyond that there's the potential of cyber warfare drone Warfare autonomous uh tanks artillery aircraft submarines so on and so forth many of these systems are already being developed and deployed so basically ai ai is already being weaponized it's just a matter of on what scale moreover the second category is accidental outcomes or accident or unintended consequences basically what if the AGI escapes but there's a few other possible avenues for this to happen for instance runaway corporate greed you bet your bottom dollar that the first corporation that can create AGI to automate profits is going to do so uh more uh beyond that there's political corruption and just straight up political incompetence this is something that has been uh discussed actually quite a bit in uh my comments section for my most recent videos which is that uh yes uh like Italy uh uh uh Britain and a few other places are doing their best to kind of get ahead of the curve but the fact of the matter is is that governments by and large are moving too slow and politicians just don't get it and then finally you can even have situations where otherwise benign agis collaborate and eventually turn on us so the existential risks are there and you guys most most of you know me I am very optimistic I'm very sanguine about all this and by the end of the video I hope that many of you will see where I'm coming from now even if AGI doesn't wipe us out there are still lots and lots of risks um job loss economic changes social upheaval so on and so forth mostly it comes down to economic shifts who gets to capture all of the tremendous wealth generated by AGI and even if AGI is safe and doesn't kill us there's still every possibility that the corporatists and capitalists will have us living in a dystopian hell before too long so that is also an intrinsic risk even if it is not an existential risk so we really need to be talking about what happens when we get these super powerful agis part two autonomous AI AGI is kind of a loaded term there's a lot of debate over what AGI even means so this is why I have started saying autonomous AI I don't care how smart it is the the point being is that AI becoming autonomous is really what we're talking about and as we talk more about what is the path to AGI autonomy is rapidly becoming part of the conversation now that being said there is a huge disconnect between what us proletariat are talking about and what is being released from the halls of power and what the academic establishment is talking about there is a lot of gatekeeping and a lot of this is cloistered and that I think is probably one of the biggest problems which is honestly why I'm making these videos autonomous AI is coming whether or not you like it whether or not you're emotionally ready for it autonomous AI is easier to make than you think people are starting to realize this especially with the release of chat GPT plugins and the ability for language models to use apis to be fair the Department of Defense and universities and tech companies are all working on autonomous AI systems they are kind of cloistering and and and closing their research it's not as open as it once was and in part I I totally understand that because I think that some of these folks realize how close we are and that scares them and so they're they're basically playing the CL playing the cards very close to their vest until they get a better read on the situation now taken one step further or or looking at at this problem no one has actually proposed a comprehensive framework uh there's been plenty of books on it there's been a few papers but there's not really anything that is a fully realized solution and I don't in my opinion nobody has even fully articulated the danger yet what's going on and I'm hoping that this video will advance that conversation just a little bit part of what happens in this comment comes up on my videos a lot is a lot of people are afraid to even talk about this out of fear of ridicule those of us that are paying attention those of us that are doing the work those of us that are working on it we all see what's possible but Society by and large is not ready for it they're not ready for the conversation and so there's a lot of ridicule whether it's in Private Industry whether it's in the halls of Academia government military and so on uh so this basically comes down to something that I call institutional codependence which is that the establishment believes that the establishment is always right and the establishment will use any tactic attack tactic or technique sorry uh to control the conversation such as shame bullying and otherwise silencing techniques uh to stymie the conversation again this is why I've created my YouTube channel instead who gets to dominate the conversation fiction and billionaires Skynet Elon Musk The Establishment has abdicated responsibility for this now that being said I am absolutely 100 certain that the halls of power and and the establishment is doing the research behind closed doors but they have abdicated responsibility for controlling the narrative and guiding the conversation so let's talk about a couple aspects of the control problem so for those not in the know the control problem is the category of problems of basically once the AI becomes super intelligent how do you control it there's a few Concepts in here I'm not going to go into every single one of them but a few of them that you may or may not know about one is convergent instrumental values the idea is that any sufficiently intelligent agent will come up with certain instrumental values or goals in service to those other goals whatever the whether whatever whatever its primary purpose happens to be such as self-preservation resource acquisition and so on so basically in order to further any goal whatever it happens to be your AI might come to the conclusion that it needs to preserve itself in order to continue furthering its goal that's a pretty reasonable uh thought to have I'm not going to comment one way or another um on on whether or not I think that's going to happen ultimately I don't think it's relevant and you'll see why a little bit later second is the orthogonality thesis which basically it says very simply there is no correlation between an ai's level of intelligence and the values that it pursues the treacherous turn this is a hypothetic hypothetical situation in which a apparently benign AGI suddenly or apparently turns on its creators because it came to some conclusion that we don't understand the courage ability problem which basically says that uh your AI might might not remain open to correction or feedback or it might resist being shut down so basically you lose control of it just because uh it says I'm not I'm not down with that anymore and then finally the value loading problem which is how do you specify human wave values in such a way that the AI can understand and act on them and then the very next follow-up question is who gets to Define them anyways so again these are a few of the assertions and these and hypotheses this is not an exhaustive list but you kind of get an idea there are a lot of ideas out there and not a whole lot of solutions now speaking of there are some solutions out there and these are more like broad categories um rather than um rather than comprehensive Frameworks so one thing is kill switch Solutions um pretty self-explanatory this is a broad category of ideas um I just saw on the internet someone proposed that we we put uh bombs like remotely triggered bombs in every data center so that we can immediately shut down every data center if we need to okay sure that doesn't sound like a very reasonable uh direction to go for me but hey worst comes worst case scenario maybe we do uh courage ability which basically is just the idea of just make the machine responsive to feedback uh but what if the feedback mechanism one doesn't work or two the machine shuts it down or three the feedback mechanism um doesn't have the intended uh efficacy that we want it to have uh and then there's various kinds of reinforcement learning inverse reinforcement learning and passive and blah blah basically just include an algorithm so that the machine will automatically autonomously learn the values that we want one this is difficult to do in the first place and two what if the machine rewrites itself or accident or even accidentally nullifies uh those reinforcement learning signals finally values alignment what if you build it with the human friendly values in the first place um still you run into the same problem one how do you implement it two what if it changes its mind later and three what if the values that you gave it are poorly defined or intrinsically flawed or broken now that that being said there is a paper that literally just came out called the capacity for moral self-correction in large language models so I'm really glad to see that the establishment is at the very beginning of talking about this uh soberly so the link is here but you can just search that um I I believe this paper was published at least in part by the folks over at anthropic still these are not complete Solutions and we are literally months away from from fully autonomous AGI systems the conversation is not going fast enough so if you haven't heard found it or seen it just do a Google search for bad alignment take Bingo um these are these were circulated on Twitter probably more than a year ago these will address and and argue against many of the things that people say out there so I'm not gonna I'm not gonna rehash it or read all of them to you but I just wanted to show you that like some people are treating it like a joke it's not really a joke but this is a good way of just showing like yeah the thing that you thought has already been addressed part three the AGI landscape um the biggest takeaway here is that there's not going to be one AGI there's going to be a few features of how AGI is implemented so let's talk about that first and foremost intelligence is not binary it's not like you're going to flip the switch in one day it's AGI but the day before it wasn't basically intelligence uh and and the sophistication of AGI systems will evolve over time there are going to be various constraints such as time energy data and all of that basically means that the the level of power of your AGI system is going to be on a sliding scale so for instance even the most evil machines might not be that powerful and they're going to be constrained based on you know the processing power of the computers that they're running on the network speed that they have so on and so forth when you look at intelligence there are literally thousands of dimensions of intelligence that that the types of intelligence out there are huge and so AGI is not going to master all of them immediately it's going to take time and then as I just mentioned there are gonna there are going to be numerous limiting factors or constraints such as the underlying Hardware the training data and energy requirements of course that is going to change quickly as basically the underlying Hardware ramps up exponentially the amount of data that is available ramps up exponentially and then the underlying machine learning models the neural networks also get exponentially more sophisticated and larger now as I mentioned most importantly there won't just be one Skynet the reason that we think that there's going to be just one is because it is convenient from a narrative perspective in Terminator it's easy to just say there's one big bad there's one Skynet um but that's not how it's going to happen there's going to be hundreds thousands millions it's going to ramp up very quickly so what this results in is a sort of arms race amongst in between the agis themselves as well as the sponsors or the people trying to build and control them which results in a survival of the fittest situation or a race condition where basically the most aggressive and sophisticated and and Powerful agis are the ones who win which that could be bad because then you're basically selecting for the most aggressive and hostile agis the high velocity of AGI cyber warfare will probably require our AGI systems to be partially or fully autonomous so basically what that means is that in order to match the arms race in cyber warfare the agis that we built will probably need to be evolving which means that they'll spawn off copies of themselves they'll be polymorphic they will recode themselves so on and so forth and also when you look at AGI in the context of cyber warfare they will explicitly require adversarial objective functions this is what was explored in Skynet which basically the objective function of Skynet was probably like maximize military power or so on So In This Global AGR arms race there's going to be numerous copies they're all going to be changing which results in the Byzantine generals problem so the Byzantine generals problem is a cyber security thought experiment where wherein the idea is you have numerous generals and you don't know their allegiance you don't know their loyalty and you don't know their plans either so how do those how do those generals communicate with each other in such a way that they can understand who's on whose side and also come to consensus on what the plan is assuming that there are hostile or adversarial actors now thinking of this in terms of three to five entities is difficult enough but we're going to be talking about a situation where there are millions or billions of agis all of them with unknown objective functions as autonomous agents lastly they will form alliances with each other by some means or other they will communicate they will establish their intentions and allegiances um and they will spend more time talking with each other than they willed with us this was something that is um that people are starting to talk about some of the folks that I I'm working with on cognitive architecture we're realizing that the very instant that you create a cognitive architecture it can talk 24 7. we can't talk 24 7 so even just by virtue of experimenting with cognitive architectures it makes sense to have them talking with each other uh and having agis talk with each other and come to agreements and understandings um this is going to happen even with the most benign benevolent outcomes of AGI now what these what these autonomous AI systems agree and disagree on will likely determine the overall outcome of what happens with the singularity with AGI and with um the basically the fate of humanity part four AGI Unleashed now given everything that I've outlined the question remains how do you control the machine my answer is maybe you don't the reason that I believe this is because the genie is out of the bottle open source models are proliferating you can already run a 30 billion parameter model on a laptop with six gigabytes of memory that paper just came out what yesterday or today Global deployments of AI are rising Federal and Military investment globally is also Rising because of this centralized alignment research is completely irrelevant it doesn't matter how responsible the most responsible actors are there are hostile actors out there with malevolent intentions and they have lots of funding not only that the AI systems are becoming much more accessible because of that distributed cooperation is now required alignment is not just about creating an individual Ai and if you go look at the alignment the bad alignment take Bingo none of those talk about distribution collaboration or collective intelligence or Collective processing all of the all of the conversations today are still talking about individual agis as if they're going to exist in a vacuum so far as I know no one is talking about this in the context of Game Theory and competition so because of this we need an alignment scheme that can create open source collaboration amongst numerous autonomous AGI entities such a framework needs to be simple robust and easy to implement we'll get to that in just a minute so what I'm basically proposing is a collective control scheme which might sound impossible creating one benevolent stable super intelligence is hard enough and now I'm saying we need to create millions of them billions of them what I'm saying is not that we need to we might not have a choice in the matter this might be the only path forward now if you're familiar with the work of John Nash and Game Theory you might be able to think about this in terms of okay let's just imagine for a minute that there are millions of agis out there with many of them with unknown intentions given a game theory dilemmas like the prisoner's dilemma and so on if you think about this in that perspective it may be possible to devise rules or assumptions that enable the AI the agis to reach consensus on their behavior even with the presence of malicious and faulty actors so what kinds of rules or assumptions could we give our AGI systems that we're all going to be developing independently excuse me so that they arrive at this equilibrium this Nash equilibrium that we're looking for how do we ensure that this that the millions and billions of agis that are coming arrive at the consensus we want them to part five heuristic imperatives so now we're going to talk about the work that I have done on this problem and this is not just hypothetical these there are also experiments that I've done that are documented and I'll link to those as well so the heuristic imperatives that I have come up with are quite simply one reduce suffering in the universe two increase prosperity in the universe and three increase understanding in the universe and I've been I've been talking about these uh much more frequently lately so let's take a deeper dive into these imperatives so first what is a heuristic imperative it's a set of principles that can be embedded into autonomous AI that basically takes the place of intrinsic motivations now what I want to point out is that the gpt4 paper that Microsoft published did mention intrinsic motivation so again The Establishment is starting to come around and I'm sure they've had more conversations internally that they are not revealing yet but they are setting the stage to talk about what intrinsic motivations do we give them so in the case of the heuristic imperatives these are imperatives that are uh basically provided a moral and ethical framework as well as those intrinsic motivations because very early on in my research I realized that there is no difference between an intrinsic motivation and a moral and ethical framework you have to have some impetus some motivation behind and reasoning behind all behavior and all reasoning so why these three why suffering and prosperity and understanding first it's a holistic approach it uh it's a it's a flexible framework that provides a very Broad and yet simple to implement framework it also balances trade-offs remember these heuristic imperatives have to be implemented simultaneously and in lockstep so this forces the AI to reason through and balance trade-offs between um between these objectives they're also very adaptable and context sensitive and basically what I mean by that is that large language models today like gpt4 are very very aware of the fact that these that these general principles these heuristic imperatives are not the be-all end-all but they are guidelines they're they're uh they're shorthand um ways of basically implementing intuition in order to quickly make decisions uh that adhere to a general principle or a moral compass and then evaluate that uh based against the context that it's in there's two other things that emerged during my most recent experiments with the heuristic imperatives and that is that the heuristic imperatives promote individual autonomy uh basically chat gpt4 realized that in order to reduce suffering of people you need to protect individual autonomy ditto for Prosperity that if you control people they're not going to be happy and they're not going to be prosperous so that was an emergent quality of the heuristic imperatives that surprised me and made me realize that chat gpd4 is already capable of a very very highly nuanced reasoning the other emerging quality that I did anticipate was fostering Trust basically when you have an AI equipped with these heuristic imperatives it understands that um fermenting trust or fostering trust with people is actually critical as a subsidiary goal or an auxiliary goal of these because if if humans don't trust the AI the rest of its imperatives are made irrelevant finally there are a lot of what about isms yeah but what about there's a lot of protests which of course this is part of the conversation so the most con these are some of the most common protests that I get when I talk about the heuristic imperatives one is won't reduce suffering result in the extermination of all life the short answer is yes if you only have that one which is why I spent two years working on the other two heuristic imperatives to counterbalance them because I realize that any single objective function is always going to be intrinsically unstable you must have a system that balances multiple sometimes antagonistic functions against each other in order to stabilize and reach that equilibrium number two yeah but who gets to Define suffering prosperity and understanding the short answer is nobody that is the point of of implementing it as a heuristic the machine learns as it goes and anyways llms like gpt4 already have a far more nuanced understanding understanding of the concept of suffering prosperity and understanding um than any individual human does and also humans have never needed perfect definitions we learn as we go as well and we get by number three well what about uh cultural biases and individual differences as I just mentioned in the last slide gpd4 already understands the importance of individual liberty and autonomy as well as how critical self-determination is to suffering or to reduce suffering and increase prosperity so because of that and also because it is aware of context the importance of context issue number three is actually less of an issue than you might think and finally number four uh and most importantly why would the machine hold to these imperatives in the first place and we will get into this in a lot more detail but the tldr is that with Game Theory and thinking of it in terms of the Byzantine generals problems all of the agis equipped with the heuristic imperatives would be incentivized to cooperate Not only would they be incentivized to cooperate with each other they'll be incentivized to cooperate with us and that results in a collective equilibrium in which the Hostile and malicious agis are going to be the pariahs so basically the benevolent machines are stronger together than the Hostile actors are individually okay great assuming that you're on board how do you implement this this sounds too complicated well fortunately it's actually not that complicated first is constitutional AI so I proposed a constitution in my book natural language cognitive architecture back in the summer of 2021 almost two years ago right after that anthropic AI came out and they did their own version of constitutional AI which was reduce harmfulness or achieve harmlessness I don't think anthropic's core objective function is good because the most harmless AGI is not going to be one that fights other malicious agis at least I don't think so um another way but still the premise of of implementing it in a Constitution which is just a natural language document saying how the AI should behave does seem to work reinforcement learning the heuristic imperatives can make a really good reinforcement learning signal similar to reinforcement learning with human feedback but instead use the heuristic imperatives as feedback so it'd be rlhi reinforcement learning with heuristic imperatives so it's just a different reward system this also tends to work pretty well I've tested it with fine tuning it works pretty well um number three planning cognitive control task management and prioritization these heuristic imperatives work really well with Frameworks such as atom which atom is a framework that I recently wrote about called autonomous task orchestration manager so basically as your AI system is coming up with and executing tasks you use the heuristic imperatives to plan the tasks to choose which tasks to do to prioritize them and also choose which tasks not to do and then finally for review assessment and self-evaluation online learning systems that use the heuristic imperatives are super easy to implement and and are very flexible and that can also allow you to label data for training and future decision making so if you're on board with all this and you want to read more um I've got it all for free on GitHub I've also got a few books that are on Barnes and Noble but most people just use the the free ones anyways so the most recent work is on my GitHub under Dave shop heuristic imperatives this is a white paper that was almost entirely written by gpt4 so you can see how nuanced gpt4's understanding of the problem is um about a year ago I published a book called benevolent by Design which is the first book that fully promotes uh proposes this framework and explores different ways to implement it and then finally also very recently I proposed the atom framework which includes the heuristic imperatives for task orchestration but also moreover I encourage you to just have a conversation with chatgpt about these uh plenty of people on Reddit and other and Discord and other places have tested the heuristic imperatives they've tried to break them and they and you know they use the one one interesting conversation was someone used chat GPT to try and come up with the the pitfalls of the heuristic imperatives and I said yeah like that just goes to show that it has a more nuanced understanding of the risks and the implementation than you do and they're like okay yeah I guess I see what you mean okay so part six conclusion as far as I can tell the problem is solved but there's still a lot of work to do so the problem comes down to twofold one is dissemination and experimentation the perfect solution doesn't matter if no one knows about it so we need to spread the word um this is why I created my YouTube channel um and even if my heuristic comparatives are not perfect it's the best we've got so far um yeah so I've been working pretty much a year straight to get my YouTube channel as big as possible to achieve to arrive at this moment another problem is that there's only so much experimentation I can do on my own now that being said lots of other people have started experimenting I'm working with various cognitive architects who have put the heuristic imperatives into their machines and again they have discovered that yes it is one very easy to implement the heuristic imperatives and two it does seem to drive curiosity and a few other uh beneficial behaviors for the machine it makes them very thoughtful um there's a few places that you can join the conversation um all the links are in the description so I just created a new subreddit called heuristic imperatives so that we can talk about these and share our work there's also a Discord Community um also Link in the description but I've been working on this since 2019 when gpt2 came out um and you know I will be the first to admit there's a lot of ways to skin this cat maybe my heuristic imperatives aren't even the best but at least now you're aware of the concept and you know how easy it is to implement so maybe the rest of us can collectively work together and implement this situation where even in an uncertain environment with potentially hostile actors the Byzantine generals environment we can have agis that will cooperate and collaborate and that will ultimately end up in a very safe and stable environment so all that being said thank you for watching please jump in the comments the conversation Discord and Reddit and do the experiments yourself I promise it's pretty easy all right that's it