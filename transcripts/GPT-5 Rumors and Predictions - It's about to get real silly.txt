good morning everybody David Shapiro here with another spicy video things are getting real interesting real fast so uh my gbt4 predictions video um was pretty popular so let's do the same thing but first let's do a quick recap for GPT for uh before jumping into GPT 5. so perhaps the spiciest thing that happened after the release of chat gpt4 which is not the foundation model of of current gpt4 but Microsoft research this is Microsoft this is not some Podunk shop this is Microsoft says uh the gpt4 represents the first Sparks of AGI and that it performs uh strikingly close to human level performance on many many tasks uh so given the breadth and depth of its capabilities it could be reasonably viewed as an early yet incomplete AGI so that was the the kind of the thing that the shot heard around the world so to speak um there's been a little bit other news or numbers and features sorry about gpt4 so uh the the the base model of chat gpt4 has an 8 000 token window which that alone has been a game changer doubling from four thousand to eight thousand tokens unlocks a lot of capabilities they already have an officially announced 32 000 token window so that is eight times larger than GPT 3. uh which a 32 000 token window is gonna be a even larger Game Changer there's going to be so many things that you can unlock with that now in terms of parameter count we don't really know but if I had to give you a best guess looking at the scale of speed because you look at Curie versus DaVinci which are you know gpt3 models um the dis the difference was about a 10x uh difference right and so then you look at the relative speed of chat gpt4 versus chat gpt3 and it's like okay maybe it's about 10 times again so if I had to guess maybe chat gpt4 is about you know in the 1 trillion parameter uh range who knows but it is definitely slower and the fact that it is slower indicates that it's doing more processing which means more parameters or more layers a deeper larger model now one other thing about gpt4 that most of us haven't seen yet but they did demonstrate it is that it is multimodal it's not just text anymore it supports images another thing about chatgvt4 is it passed the bar exam and the 90th percentile it has passed some other tests in the 99th percentile so it's pretty smart um on some benchmarks it outperforms most humans already and then from using chat gpt4 it is qualitatively better at pretty much everything it is a step Improvement above everything that chat GPT or the gpt3 and chat gpt3 can do and then of course MIT released a study showing that even just chat GPT 3.5 increased White Collar productivity by 40 percent and gpt4 is going to do the same thing again so these models are coming they're already having a huge impact and people are just beginning to learn how to use them so that's that's all three GPT 3.5 and 4 just as a recap before we jump into GPT 5. now I believe this is the last slide of kind of recapping the way that things are right now so as many of you have heard there has been an open letter signed by a whole bunch of people calling that's being dubbed the great pause people are are calling for the great pause which is a six-month moratorium on building anything more powerful than gpt4 uh the reasons are safety ethics regulations so on and so forth there's also been a call for a a public uh version um basically the the CERN of AI which when you look at how much uh money goes into CERN it's billions and billions of dollars a year um funding AI research at just one percent of that is a drop in the bucket and could probably produce public versions you know uh uh common uh commonly owned or fully open source versions um just kind of like how the internet was was developed um you know actually at CERN or at least the the World Wide Web um HTML and so on um there have been no major regulatory movements yet which is really interesting so no governments as far as they know even in the even in Europe have gone so far as to say hey let's let's put the kibosh on this for a little while which usually uh the European Union and European nations are a little bit more um kind of ahead of the curve because America is very reactionary I can't remember the name of this uh Paradigm but American politics and legislation is is very deliberately only going to react to things once they happen rather than preemptively legislate whereas Germany and the EU and France and other places are much more likely to proactively legislate things just on the anticipation of a problem but even Europe as far as I know has not put any restrictions on language models and deep learning so that's very interesting according to uh some rumors this ricocheted around read it a while ago gpt5 is already being trained on 25 000 Nvidia gpus um the the estimate was over 200 million dollars worth of Nvidia Hardware is being used to train gpt5 again that's a rumor um another big piece of news was Sam Altman was recently on the Lex Friedman podcast and what he said and this this to me was from a technical perspective the most interesting thing he said that gpd4 did not come about from any Paradigm shifts it was not a new architecture or anything but that it came about from hundreds and hundreds of small incremental improvements that had a multiplicative effect across the whole thing which resulted in you know new new ways of processing and preparing data better algorithms so on and so forth and so if gpt4 came about from incremental improvements and nothing major maybe we can expect more of the same for gpt5 that it's going to be uh ongoing improvements of data pre-processing um training patterns so on and so forth so that's in the news so now let's skip ahead to GPT 5 predictions and some rumors uh all right so first top of mind when is it going to come out uh obviously the internet is Rife with rumors some of it has more validity than others um according to one website and I found some of this with uh the help of Bing actually ironically enough um one website said that they expect GPT 4.5 to come out this September so that would be a little bit quicker of a turnaround um another uh blog said that we should expect gpt5 by the end of 2024 or early 2025 just given the the historical pattern that seems pretty reasonable when you consider that the testing cycle for gpt4 was six to nine months so they had it like rumor has it that um that they had gpt4 like last summer or last fall so maybe our predictions about when gpt4 was completed was correct but the but they they delayed it the release due to testing who knows um uh one Twitter user said that I I don't know if this was in response to the leaked Morgan Stanley document um but basically you know uh and of course it's on Twitter so take it with a grant salt but basically that uh he said that gpt5 is scheduled to be finished with its training this December so that kind of lines up with late 2023 early 2024 and then you add the training cycle of six to nine months that puts it at Mid 2024. um so another thing that was interesting is in the documentation openai has a few snapshots of of the current models that are set to expire in June which is really interesting because they've never done that before so my interpretation is that they're going to say okay we're going to expire these models um but you can use them because they're probably testing new ideas um and then they're gonna you know recycle those uh models or replace them or upgrade them or some something so either way all of this all of these rumors and some of the facts that we're gleaning really kind of point to a shorter testing and release cycle which considering open ai's close partnership with Microsoft Microsoft is very familiar with a regular Cadence right you've got Patch Tuesday with Microsoft server and Microsoft desktop they regularly release new versions uh major and minor versions of Windows and other software so they're probably being pushed to be more like a conventional software vendor and of course that's the direction it's all going right now large language models and AI are new and shiny but before long it's going to be a commodity just like anything else just like your smartphone just like your laptop whatever so I think that I think that the we we probably can't expect some more traction by the end of this year even if it's an incremental update but certainly gpt5 I think that probably mid 2024 at the earliest if I had to guess but I think that the end of 2024 that's seems to be where the consensus is right now I wouldn't put money on it you never know but that seems to be the consensus window size so one of the biggest functional changes of the jump from GPT 3 to 4 was going from a 4000 token window up to an 8 000 token window with being teased with a 32 000 token window the amount of problems that I have been able to solve and address just by doubling the token window incredible so if that pattern continues where it either you know it goes up 2X or 8X or whatever if you extrapolate that pattern out then gpt5 could have anywhere from 64 000 tokens to 256 000 tokens so that is roughly 42 000 words up to 170 000 words to put that into perspective I think that Dune the original Dune was a hundred and eighty thousand words so it could read all of Dune in one go um Couldn't Write it but when you consider that most novels are 50 to 70 000 words that is more than enough uh token window to read an entire novel and write an another draft of it so just digest that for a minute and think about how much information that is the number of scientific papers that that could be so on and so forth now when we talk about window size if we assume that they overcome any diminishing returns on memory performance and compute because it's going to be a trade-off right the the larger those internal vectors are the more memory it's going to take and that one thing that I didn't include in this because it looked a little too dry but people are basically predicting that gpt4 takes 10 to 40 times as much compute as gpt3 and then if you extrapolate that out again gpt5 will take another 10 to 40 times as much compute so the amount of compute is ramping up exponentially possibly we don't know but what if there's going to be diminishing returns on an algorithmic level so for instance maybe um when you get the vectors that large you might get a dilution which uh for for rnns and other things basically dilution I'm probably using the wrong word but it kind of forgets what it was talking about at the end of it so do we need new attention mechanisms are we going to need a new architecture or just hundreds of more kinds of algorithmic and incremental optimizations we don't know one other thing that we need to be asking ourselves is how many tokens do we actually need right because chat GPT with 8 000 tokens is able to serve ninety percent of our needs right now only with very long conversations does it forget the original like at the beginning and also I think there's some evidence that they have other memory stuff going on because I've had some pretty long conversations with chat GPT now and I and I ask it like okay what was the first thing that we talked about and it remembers so I don't know if they've got some search and retrieval going on or some good summarization not sure but the point is there's probably a diminishing returns in terms of utility value in terms of functional value to us the end user and that includes um you know ordinary citizens and civilians like us as well as corporations in business and Enterprise use cases more is not always better so there might be a trade-off in terms of speed cost and intelligence right because what if what if they find out that like okay 8 000 tokens actually satisfies 95 of all use cases so let's just make that 8 000 token um model make it faster cheaper and smarter and then you know maybe we have uh models that are optimized for much larger windows for specific kinds of tasks like summarizing you know half a million uh scientific papers not really sure but it is interesting because honestly if they came out with a 256 000 token model tomorrow I think that 99 of people are never going to use that many tokens could be wrong you know I probably sound like some of the people who said like oh nobody's ever going to use a desktop computer so maybe I'm completely wrong you know I I'm the first to admit I frequently am wrong when I make some of these predictions and sometimes I'm hilariously wrong um okay so moving on modality for me the biggest shock of gpt4 was that it was multimodal I didn't think they were going to go there yet but gpt4 they demonstrated it it can you can give it pictures it can spit out pictures most people don't have access to that yet it probably requires some work on the API because if you're just sending text over a Json you know a rest API that's one thing sending images it's a little bit different so I suspect that they're probably working on the Integrations with that um which that's a lot to figure out I don't envy them that problem it sounds very tedious but when you look at the fact that that open AI has Dolly they have whisper gpt4 has images you do the math I suspect oh and then you look at um at how uh how much like text to video and video to text is coming out I suspect that gpt5 will be audio video images and text if not more uh but even still that would be a great start so I was talking with some people about this and what does that mean for for vectors because if you can represent an image or audio or video or text in vectors those vectors are going to have a lot more Nuance to them and so the vector is the embedding right that is the mathematical representation of the input which is then used to generate the output of these models so if you have these multimodal vectors it's entirely possible that these vectors are going to be more abstract and human-like thoughts inside the model which that has all kinds of potential implications and I'm not saying that it's going to magically become sentient or or self-aware or anything like that just that if you have a more nuanced way of of representing information about you know reality it's entirely possible that that will unlock entirely new capabilities Within gpt5 so one other big question is where are they getting the data uh one of the rumors was that they actually ran out of high quality internet Text data that they actually downloaded the entire internet and after they filtered out the garbage um they realized there's not any more text Data out there we need other modalities and that's why they worked on whisper that's why they worked on on Dolly and so if that's the case then maybe they're working on downloading all of YouTube all the podcasts all of every you know uh was it Dailymotion or whatever you know like basically every content provider out there that they can get their hands on and um legally and ethically get that data if it's under Creative Commons or other um open open licensing um so anyways this is it's really difficult to anticipate but just the fact that gpt3 was was single modal and gpt4 is multimodal I think we should at least assume that that trend is going to continue again there might be diminishing returns they might find that most people don't need multimodal models and so then we might end up with a branching um kind of schema Nvidia does this by the way Nvidia publishes hundreds and hundreds of different models that have different specializations um and Nvidia is really good at cranking out very specific models for specific tasks whereas at least right now open at open AI seems to be focusing on one Flagship model that that uh that business model might change over time not sure um okay so intelligence and capabilities this is where I kind of really dive off into sci-fi land so if we look at the the relative performance of gpt3 versus gpt3 gpt4 sorry three versus four it was a huge jump in intelligence where it went from you know I think GPT 3.5 was able to pass the bar in the 10th percentile and then four was able to pass in a 90th percentile so that's a that's a Quantum Leap Forward so if we extrapolate that out then we could probably assume that gpt5 is going to pass all tests and all benchmarks in the 99th percentile or or greater um if it if it's that smart then with the correct Integrations which are already working on Integrations chat GPT uh plugins right with the correct Integrations gpt5 could then outperform humans at 99 of all other tasks that includes stem jobs science technology engineering and math and so the the idea that I had was basically given the right Integrations and enough time you could ask gpt5 to design a spaceship and it will do it and then if you give it the right robotics it could build the thing too um so like I when I when I wrote that down I was like this is absurd then I'm like you know if we take out the the Quantum Leap from three to four and do that again this is actually within the realm of possibility I think and then another probably even more controversial prediction is that um it will be able to surpass humans in most Artistic Endeavors as well such as writing Symphonies uh composing stories um and even acting on stage given uh the correct rigging and framework so like maybe it can control a virtual actor like in the Unreal Engine or a robotic actor because you look at Disney Disney is making very very very life-like animatronics um so I suspect that one way or another human actors are going the way of the dinosaurs just full stop um why because human actors are expensive um and most actors have signed away writes their likenesses by now anyways uh many of them unwittingly this this came up in conversation where uh voice actors and even some actors that are getting older have very deliberately signed away their likeness so that they can be immortalized in AI um so if if any of this is remotely what happens with gpt5 I can understand why people are calling for a moratorium um but it's going to happen because competition is there right if open AI doesn't do it someone else is going to and if America doesn't do it some other country is going to do it and nobody wants to fall behind so I really don't think a moratorium is going to happen but that begs the question what does happen is this AGI is this Singularity is this you know are we going to get regulation is it are we going to get competition and of course if you're familiar with my channel you saw that I predicted AGI within 18 months if GPT 5 qualifies we could have gpt5 mid 2024 so the timing is is there so all that being said buckle up as a commenter said in a previous video it's about to get silly yes that's pretty much all we can really guarantee right now is that it's about to get real silly real fast thanks for watching