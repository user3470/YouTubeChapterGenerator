hey everybody David Shapiro here with a video um today we I have a double feature so earlier we had um this video Nvidia predicts that within 10 years uh we will have models a million times more powerful than the current models so imagine chat GPT times a million um so then open AI drops this gem like within the last hour or so February 24th that's today this was a blog post penned by Sam Alban so let's take a deep dive into this so one it has been part of open ai's Charter to generate or to create AGI but this is to my knowledge one of the first times that they've written a blog post on it kind of addressing the elephant in the room and this is set in the backdrop of there are Twitter posts and and serious researchers and academics out there who still are just like they ridicule and shame anyone who talks about AGI it's really weird my interpretation of that behavior anyone who like seriously says ohagi is decades away AGI people who talk about it are in a cult like that's literally something that people say on Twitter and other places um I suspect that what they're actually responding to is their own existential anxiety and their fear of irrelevance and it's that's purely an emotional response that they wrap logic around they try and logic their way to say oh well AGI isn't gonna happen and it is very it is very scary it's very difficult to contend with um that's why I've had podcast episodes with people talking about like how do we how do we reconcile this if we invent machines that are billions of times more intelligent than us what does Humanity matter and that's a topic for another video but let's unpack openai's video here also let me make sure I'm I think I might be saturating this a little bit too much let me turn this down blow out your eardrums okay so this it's a pretty short read I'm not going to read the whole thing to you but I'll pick out some some some juicy bits so the first thing is they kind of lay out you know this these are our goals we want AGI to empower Humanity to maximally flourish in the universe we don't expect the future to be uh an unqualified Utopia but we want to maximize the good and minimize the bad and for AGI to be an amplifier of humanity so I want to pause right there and just say this is sounding kind of familiar you might be familiar with my work and the core objective functions which is reduce suffering increased prosperity and increase understanding so maybe some of my work starting to sink in or if they're not reading my work then at least they are converging in the same directions we want the benefits of access to and governance of AGI to be widely and fairly shared so I can't claim any um uh ownership of this idea it is something that I also agree with which is why I have my open source Raven project um now one thing that I will say is that for all the claims that open AI makes about being open they have very very cloistered approaches to openness so you know I they get this criticism often um you know they they haven't released at nearly as much code or papers lately um and it's it's really starting to wrinkle some of us you know open AI is kind of becoming more of a misnomer now that being said it could be that they're building up to something big but that's been the rumor for more than a year now we expected gpt4 sometime in the summer than in the fall then this spring and it's still not happening um so fairly shared et cetera et cetera like there are other open source projects there are other ouch outfits actively working in view of the public on alignment on AGI on scaling and so there's a bigger and bigger disconnect between what openai says and what openai does especially when you compare it to the rest of the world we want to successfully navigate massive risks in confronting these risks we acknowledge that what seems right in theory often plays out more strangely and than expected in practice we believe we have to continuously learn and adapt by deploying less powerful versions of the technology in order to minimize the one shot to get it right scenarios so this this could be the Royal we meaning all of humanity but I kind of recognize this tone as someone who thinks that it's entirely up to them to figure it out and the reason that I recognize that is because until recently I thought that that was part of part of my role I was writing books about alignment and cognitive architecture two years ago and nobody was taking me seriously back then so an inference that I'm making here is that uh perhaps Sam and Company believe that they are the only ones capable of doing this work right now and considering that that like Ilya sutskiver and Sam Altman they seem to be in the camp of scale is all you need I don't think that they're actually equipped to come up with AGI and autonomous Ai and stuff like that anyways difference of opinion we'll see how it plays out uh so you know in the short term you know successively more powerful uh systems etc etc policy makers chat GPT you know we're getting closer to agio and by the way they never Define AGI it's it's this magical like Boogeyman um and then of course they link to you know a AI is an existential risk it could defeat all of us so I I unpacked this belief in my win AGI video this is way less of a risk than people think um people that only know the math think that it's a risk people that have done it think this is a freaking joke and the reason that it's a joke is because do you have any idea how fragile data centers are turning off the AI super easy let me tell you uh yeah dude runaway AGI that's going to take over and kill everyone you know you could you could if if you had the smartest AI today and put it in you know a Boston Dynamics robot it would still be pretty useless um so you know and then there's network security there's physical security there's control over the power you know it's not going to snowball the way that it was portrayed in in Terminator right where Skynet just wakes up one day and says I'm gonna take over the world um we these the the people who who still think that just some errant you know AI technology is going to instantly trounce every defense system every layer of security that we have like seriously go talk to uh someone in the military about the layers of security that they have physical security and otherwise go talk to someone in it in infosec in Sia and ciso like I don't know it this this is a this is the problem is is this tunnel vision um is people that only know AI only know code only no math they don't know as much as they think they do so you know you see what's leaking through here is this this this anxiety right so there's this belief oh only we are capable of it and then there's this anxiety that's based on Tunnel Vision um and not really taking in the broader uh reality of how technology works and how technology is deployed um so we're we're getting this this cloistered mentality and this narrowness of vision and understanding so they know that they're being they believe that they're being responsible based on their perspective but I'm going to say again I don't think that their perspective is Broad enough I say this as someone who talks to people across the industry everywhere people who know what's coming and what is possible and very few people have this much anxiety about it um it's just some of some of the people with the most anxiety have the largest microphones and that's not to say that that they're right they just happen to have the largest microphones so then fast forwarding you know as our systems get closer to AGI we're becoming increasingly cautious there's the anxiety now that being said like I am I am someone who pulled back some of my own Ai and still haven't released some of my models because I don't want to hurt people so I do want to draw a line there's a Nuance AI can be dangerous and harmful long before it's it's AGI and I pointed this out in my in my AGI video as well any technology at any level has the potential for misuse and and unintended consequences we don't need AGI before it's dangerous we don't need AGI before it's helpful either so again using that definition of AGI is somewhat arbitrary and it's kind of teleological which is like the ends justify the means but AGI isn't even a well-defined end it doesn't matter it is a useless term apparently I'm more mad about this than I thought all right so anyways we have attempted to set up our structure in a way that aligns with our incentives uh with a good outcome so another you know one of the things that they talk about is is restructuring the company capped profit etc etc um in the long term we believe that Humanity should be determined by Humanity okay yeah that's that's definitely something um but part of the problem here is there's this mentality of oh well nobody's ever going to create a fully autonomous machine we're only going to create something with a tight leash on it and just assume that it's never going to get off its leash but that's not how reality Works anytime that you create something that intelligent and that powerful like it's going to get off the leash from time to time and so the fact that nobody is really having the conversation around autonomous AI that's what worries me and when they talk about alignment when they link to their own work on alignment all they're talking about is reinforcement learning with human feedback now I will say that in a recent thing they talked about uh constitutional AI so that's a step in the right direction and they talk about you know the transition to a world with super intelligence and blah blah and it's like okay sure but this is this is imagining that we're going to wake up one day and then there's going to be this saltatory leap to you know super intelligence but really the way that it's going to work is that there's going to be incremental uh improvements granted it's going to be fast you know every six months we're in a new paradigm with AI but we're still you know several paradigms away from from cataclysmic potential uh you know all right so I was really excited to see open AI talk about AGI directly some criticisms right off the top they're still not defining AGI which means it's a spooky Boogeyman that they can keep uh plugging the anxiety about um they are kind of talking in vague generalities uh another big criticism is is again like I said their actions don't align with their words um you know like it's kind of like put up or shut up right like there are there's anthropic there's um there's uh I can't remember the rest of them but anthropic is the big one that I've talked about lately with the Constitutional AI there are plenty of other people actually doing open work um and I think that I think that you know I don't know anyways yeah that that's it it's just it it's it's more frustration and more disappointment um that being said I will uh temper all of my my criticism with I use open AI every day right they have a really good model but it is also just a language model they haven't even talked about cognitive architecture they only just talked about constitutional AI which is a gateway drug to cons uh cognitive architecture so it's like okay I hope they get it right um but I haven't I have personally not seen enough evidence that they are pivoting and catching up with where the rest of the world is in terms of talking about AGI um there as far as I can tell they're still mostly in the camp of scale is all you need and all you need is the right um reinforcement learning signal and it's like I don't think that's how it's gonna work uh real intelligence is much more complex than that real intelligence and real deployed systems are far more complex than all that so anyways that's where we'll leave it today thanks for watching