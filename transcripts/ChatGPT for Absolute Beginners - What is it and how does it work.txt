hey everybody David Shapiro here with another video uh it occurred to me that there's a lot of people that are new to GPT and chat GPT and uh so you might have questions about how it works uh before we get started in the video I wanted to direct your attention to my patreon page now I put a lot of content out entirely for free I want to help make the world a better place uh by um by sharing my knowledge um and in exchange I'm hoping that I can get a little bit more support for uh for the work that I do so if you find my content valuable please jump over to patreon and consider supporting me one advantage that you get or there's two advantages that you get for supporting me on patreon one you get access to my exclusive blog and two I've started uploading uh patreon exclusive videos so with all that being said let's go ahead and jump into today's presentation what is chat GPT and how does it work so Chad gbt was built by a company called openai openai was established a few years back as an open source Consortium or not really a Consortium just a company an open source company with the goal of creating safe AGI that is the primary was the primary original purpose of open AI now it has since changed it is now closed source and it is also for profit so um obviously it has gotten some criticism for this uh because it's kind of runs contrary to its uh founding purpose um but they do still release open source um uh code every now and then okay well first before we get into chat GPT we have to answer the question what is GPT GPT means generative pre-trained Transformer it is a technology that basically just reads and generates text that is the the long and short of what GPT does it was trained it's a deep neural network that is trained to predict the next token now a token is just a fancy way of saying a few characters um and of course you you put characters together to make words you uh every everything that you read on a page is a series of characters new line periods spaces letters and so on so it was it it's a little bit deceptive that it was only trained to predict the next character now you might say okay well how is it so powerful if that's all that it does so the reason that it's so powerful is that because enable in order to predict the next character accurately you need to have a lot of knowledge and other capabilities and that is what it has learned to embed in its neural network over time and so you might hear that word embedded you might also hear latent space we'll talk about that in a couple slides so for instance it knows how to write a checklist by virtue of having read millions of checklists it also knows how to write code because it's read lots of code and so just by virtue of predicting what comes next it learns to figure out what comes next it was trained on billions and billions of tokens now the easiest way to think about it is that it is an autocomplete engine like what you might have on your phone but it's on steroids it doesn't just predict the next word kind of you know stochastically some people do say that it's a stochastic engine not really it's it's a little bit more complicated than that um so but it is an autocomplete engine on steroids now you might say that a human brain is an autocomplete engine too because we have the ability to predict and generate patterns but that's a that's a topic for another video the next thing that you need to know is that GPT comes in flavors so there's two kinds of flavors that it comes in one is that it comes in different sizes so there's uh there's larger ones and they're smaller ones and it's all measured by the parameter count so the parameter count is basically the number of connections inside of that neural network and there was a paper that came out a year or two ago that said that for these neural networks it's roughly a thousand parameters in a deep neural network are equivalent to the processing power of one human neuron so with GPT the largest model that we know of being 176 billion parameters this is roughly equivalent to 176 million neurons in a human brain so it's still much smaller than a human brain in terms of raw processing power and that of course is if that paper holds up we uh we typically adjust how we think of human brain power over time now GPT you know okay 176 billion parameters how much uh compute power does it take to run it takes roughly 700 gigabytes of vram I think it's 768 or something like that uh so that's roughly 90 Xboxes or maybe up to 100 Xbox Ones in order to run chat GPT or GPT now that's not necessarily what they're using um but that's just a rough approximation again you know take it with a grain of salt it could be could be more could be less oops so there's flavors in terms of size but there's also flavors in terms of what they are trained to do or fine-tuned to do so what I mean by that is that the original model that was pre that was trained just to predict the next token is what we call the vanilla model or the foundation model now Foundation models are really powerful but they also tend to go off the rails um and that is because they just predict the next token and it's very haphazard they're not they're not uh they're not trained to do any one thing they just predict the next tokens and it allows them to uh confabulate very deeply or kind of invent their own tasks and so what we've done is we've had uh fine-tuned data sets that have given us codex instruct and now chat GPT and we'll talk about how these fine-tuned data sets are created in just a moment but the key thing to know about fine tuning is that it's like it's it's based on a technology called transfer learning where you have the pre-trained model and you take it apart and you slap a new layer or two on the end um and then you train it on that one new task with a new data set but here's the thing rather than training it on billions of tokens it only takes a few thousand to train it on one specific new task thousands or Millions give me just a moment sorry I've been sick and I'm recovering so I'm drinking Pedialyte okay so it comes in flavors now how is the chat GPT flavor created the chat GPT flavor uses rlhf or reinforcement learning with human feedback so the way that this is trained is that you have a reinforcement learning model that uses a signal from humans that basically say like it gives it it tries uh it tries a generation it generates some text and asks you did you like this yes or no and people say yes or no good or bad and the the rlhf model then learns to predict what people will want and so then once you have a reinforcement learning model that can accurately predict what people will want it'll it basically just says will you like this yes or no that allows you to then label lots and lots and lots of data automatically very quickly and so what they did was they used that this method rlhf to create a new data set so above and beyond instruct and codecs which are what most people are familiar with now there's a data set for chat GPT and so basically I've got it right here at the end people preferred long thorough responses and so that's how chat GPT learn to communicate just by virtue of it gave a response and people gave it a thumbs up or a thumbs down and that is the direction that it went thank you now how does its memory work because one of the most remarkable things about chat GPT is that you can have pretty long conversations with it and it seems like it has a pretty long memory this is one of the things that makes it very powerful so one thing to know about GPT Technologies is it has a window size and so the window size is the total amount of text that it can read and generate so for instance text DaVinci 03 one of the larger models right now has a window size of 4000 tokens chat GPT is rumored to have a tokens window size of 8 000 tokens So when you say that it's when you when you think that it's uh three to four tokens per word on average um that includes spaces and hyphens and and and white space um that equates to about 10 pages of text give or take could be 15 depends on what's on the page so the most obvious way is that chat GPT just has what's called a rolling window where it reads the last 10 pages of your chat which if you have a short chat that means it can read the entire chat log and continue the conversation just as the same with um with the same Paradigm of of Auto autocomplete of just predicting the next text uh because it has read lots and lots of chat logs and it has a particular pattern that it follows where it gives very very verbose responses now there's a couple other possibilities of how its memory works again open AI is no longer open this is closed source and so it's proprietary technology it's also for-profit technology so this last part is is pure speculation on the part of the AI community it might use search or a scratch pad so what we mean by search is that every chat log that you give it is searchable so and once you have very long chat conversations it's not going to fit in the window anymore but based on what's going on currently in the conversation it could use that to queue up and look back in older messages in the conversation to figure out what's going on it could also use what's called a scratch Pad which is basically a running summary that holds out on the side uh or an ongoing summarization that it holds on the side that it can use to keep track of information regardless of how far back it goes again this is entirely hypothetical um we're not sure if that's what it does um there's another possibility that I didn't put on here that um I want to try and Implement which is that it can build a knowledge graph of the conversation as you go which means that it's constantly updating and uh and keeping track of new topics and stuff and then it can Traverse that Knowledge Graph and extract information from it as it goes I don't think that it does that but a future version absolutely could now why is this so powerful why is chat GPT so incredibly powerful that it has taken the Internet by storm well one of the things to keep in mind is that it has a lot of latent Space by virtue of the fact that it has read a significant chunk of the internet means that we don't even know what it knows we have a good idea of what it knows but we don't even have good benchmarks about how to measure the power of these models in fact there are new benchmarks coming out all the time because old NLP benchmarks don't really matter it doesn't it doesn't measure intelligence the right way um because what we have now those are those those old benchmarks were for NLP natural language processing what we're doing now is called natural language understanding and natural language generation so it's an entirely different Paradigm uh category of Technology so this latent space are these embeddings is what I mentioned earlier in that by virtue of figuring out what it takes to predict the next token it has also embedded a lot of knowledge or has a lot a lot of latent capabilities so that is one aspect of why it's so powerful another aspect of why it's so powerful is because your brain is interacting with a machine so it's kind of like a utility Droid which is why I picked a picture of R2D2 R2D2 on his own doesn't really do that much but he has capabilities that you don't and similarly chat GPT has capabilities that you don't and so you complement each other and so your brain has better faster memory than chat GPT so you can remember what's going on in the conversation you also have the ability to spontaneously come up with directives chat GPT does not so chat GPT does a different kind of work than you do but it does it faster and so by doing that different kind of work and doing it faster it takes a lot of mental work off your shoulders which is why it is so powerful same idea of behind having a utility Droid is it can do something that you can't like R2D2 can hack into computers it does it much faster than any human can and then R2D2 will follow you around so is chat GPT going to evolve into R2D2 maybe that'd be kind of cool now let's talk about what is chat GPT changed for us the biggest thing is that chat gbt is the first AI technology to take the World by storm it is the biggest proof that AI is ready so the first thing that's going to happen is a lot of investment so what I mean by this is that once a technology is commercially ready once it's commercially viable then you get a lot of money being put into it we saw the same thing with electric vehicles and solar power because for the longest time things like EVs and solar were not cost effective but now they are and of course there's a little bit of debate over whether or not EVS are actually cost effective but solar absolutely is which is why the rate of of investment in solar is accelerating and so now that the world knows that AI is real and it works because chat GPT is easy to use and the value is obvious the money is coming okay so that's that's the first thing that it changes um it will take a little while to prove it out and it will take a little while to implement and deploy it because chat GPT is just a prototype it's not ready for commercial purposes yet it's very useful as it is I use it all the time um so and this is just version one imagine version two or version 10. it's going to get exponentially more powerful now there's a lot of other problems to solve though mostly safety how do we use it correctly how do we how do we use it without doing any harm how do we make sure that it is not going to do more damage than good so there's a lot of improvements that need to be made that this is the big thing that changes is so there's the window size the length of memory some of the some of those topics I already mentioned um it needs to be able to follow instructions a little bit better because sometimes if you use chat GPT you might notice that it kind of gets stuck in a rut still where you can you can correct it and say no this isn't the way to do it sometimes it'll listen sometimes it won't um and one of the biggest things is going to be Integrations with external sources of information or other apis because right now it's self-contained in a tiny little bottle um but one of the biggest things that Technologies like chat GPT could could change is that everything might go faster all science all education all creativity all business everything could go faster because of the cognitive offload that this technology offers all right so what are the limitations and downsides of chat GPT first and foremost it's expensive to run um as we mentioned earlier it would require about 90 Xbox Ones to run it obviously that's not what they're using the computers to run these are very expensive though and open AI is not open about it that's another big downside but because this technology is so valuable there are many many up and coming competitors um so that is that is we're going to see a huge investment in 2023 in people trying to make clones of chat GPT um I have a video series where I started this and there's dozens hundreds of other people already working on chat GPT clones there is a huge potential for disruption such as lost jobs new jobs coming out too and even new ways of living and the biggest downside is probably going to be safety and privacy such as data security the conversations that you have with chat GPT if they get leaked it could be used against you or if at the very least it could be very embarrassing all right last slide what's next in 2023 well We're Off to the Races um 2023 is going to be the first year of the singularity mark my words we are going to remember 2023 is the year that the singularity began um another more boring term for that is the fourth Industrial Revolution um there's going to be lots of money being invested in these Technologies lots of new products and services we're going to see a lot of change very quickly because we're at a Tipping Point right so if you look back through time from the introduction of mass-produced cars it took I think it was 14 years from from the introduction of mass-produced cars to where like basically horses were not used anymore we're going to see change even faster than that um because we're at a Tipping Point and because these Technologies are very quick and easy to deploy relatively speaking um you know building building a million cars takes a long time getting a million users on chat GPT took three days so uh the the rate of change is going to be very fast and it's very difficult to predict where we're going to be a year from now at in the first week of January in 2024. all right well that's it thanks for watching um again please uh consider supporting me on patreon um it's patreon.com my goal is to be able to do this full time so that I can continue putting out content for free thanks for watching and have a good one