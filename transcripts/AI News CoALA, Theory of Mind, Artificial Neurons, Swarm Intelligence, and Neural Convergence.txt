there are three particularly interesting papers that are out on archive right now and archive is a pre-print server so these have not been formally accepted however they are very interesting to me and I noticed that they weren't getting any traction so I am here to remedy that situation so today I'm going to cover three papers that uh appear on archive the first one is collective or sorry conversational swarm intelligence CSI the second one is about theory of mind and their emergence in llms we've actually finally got some empirical evidence about how and why theory of mind is modeled in large language models and then finally a paper on proposing a new cognitive architecture for autonomous agents so first CSI conversational swarm intelligence uh this is a paper it was a relatively small experiment they had 25 people broken out into five chat rooms and they used chat GPT 3.5 to basically summarize and transmit the ideas from one chat room to another uh once a minute now this basically formed a little bridge between these five different chat rooms but the the result was actually relatively impactful for a for a somewhat low bandwidth kind of summary and transmission of ideas to propagate across this uh this very relatively small Information Network there was 30 percent more contributions and then seven percent less variance so why is this significant so if contributions go up by a third that means that people are more engaged while they're problem solving and so taking a big step back what like so what why well imagine you're on Discord and you've got a whole bunch of different channels and people are all kind of scattered all over the place and there's conversations happening in different channels and different Discord servers and you need to coordinate this is the kind of thing that a lot of us who are users of slack Discord and Microsoft teams and pretty much any other chat platform wish that we could have because what this does is it is in real time it surfaces the primary uh like insights and decisions that are being created as people Converse and then propagates that across a network and so this uh some of the advantages of this method is that it's really really simple they use chat GPT 3.5 which is cheap and fast in order to propagate these signals these information across networks so how is this going to play out why do I think that this is significant the biggest reason that I think that this is significant is because as the information landscape out there accelerates and we have more internet saturation more internet penetration and more conversations we need ways of coordinating massive uh like efforts whether it's uh coordinating open source planning with inside companies uh or even just on social media discussing social issues discussing policy issues that sort of thing I think that this technology will fundamentally offer us new ways to disseminate information and to get up to speed and share ideas and also magnify good ideas because imagine it this way imagine that there is a Discord server or subreddit or whatever where people are you know workshopping ideas about like how do we Implement Ubi or how do we solve climate change or whatever the good ideas that originate in one server are going to be automatically propagated to other servers or other chat rooms where they can be discussed and added and so it basically destroys the echo chamber entirely but it's it it destroys the echo chamber by mediating that conversation by simply just saying hey this other group you know it's not personal just this other group had this idea what do you think of it let's add it to your conversation and so by constantly cross-pollinating between these chat rooms these Echo Chambers you can break down those intellectual barriers and those emotional barriers in order to have better conversations now this was a relatively small experiment but I think that I think that you'll probably actually see even greater impacts with larger groups and more sophisticated approaches now one thing that I want to point out is that this study is done in part by Carnegie Mellon University so these are serious people uh serious researchers promoting this idea so next up we have this is the big one so this one as soon as I saw this I was like okay whatever this is nothing new but then I got to reading it and they started referring to this thing called artificial neurons okay so taking one big step back what is theory of Mind theory of mind is the human ability to model what is going on in someone else's mind we have the ability to keep track of the contents beliefs and state of someone else's brain and so obviously theory of mind it doesn't work best over text however it is more of a longitudinal or it has a temporal component meaning that the longer you interact with someone the more you can gauge kind of what they believe what's going on in their head so on and so forth so what this paper does is they talked about applying theory of Mind tests that are used on humans so human-based theory of Mind tests to a variety of language models including Falcon and llama and a few others in order to see to one measure their performance which you can see by this graph none of them perform as well as humans yet which okay that's not what we're measuring here we're not we're not trying to compare their performance to Human Performance we're trying to characterize how these language models have theory of mind and these are relatively small models we have 40 billion parameter models and 13 billion parameter models so these are very small and they're still demonstrating some theory of mind now the most interesting thing to me is that this emergence of what they call artificial neurons or selective embeddings and so basically what this is and this was actually a little bit harder to believe because there's been rumors you know from the neuroscience and AI Community for actually several years that there's often some convergence between the way that uh deep neural networks process information and the way that some very small circuits in the human brain process information and so we usually see that more in visual processing so for instance in all the language models uh image detection and object recognition the way that those surface features such as edges and colors and shapes is actually really similar to how the optic nerve in the human brain processes uh and and and breaks down images but that's okay that that kind of makes sense right you know human human eyes human brains through the process of evolution our optic nerve is going to find the most efficient way of decomposing images that we take in likewise you have the same problem space of image decomposition you train a neural network to do the same thing with the same kinds of data maybe you're going to have some convergence in the way that they process and of course of course I'm not saying that it like it created a virtual optic nerve that's what I'm saying that's not what I'm saying at all what I'm saying is that kind of what these circuits do at an emergent level what they do mathematically to that information is there is some convergence now this is an entirely new domain because it's one thing to process image data it's another thing to process beliefs what is a belief how can you represent a belief mathematically and what this paper does and this is the part that is just it's it's still kind of blowing my mind what this paper does is it shows that there are these emergent neural circuits or these selective embeddings that specifically pay attention to True beliefs and false beliefs and whether or not the information and question being asked pertains to a true or false belief and what they furthermore say and these again I want to point out who is uh promoting this this is Harvard this is MIT this is um this is the health sciences and technology department this is the medical school and program of Neuroscience so uh it's one thing if I say hey there's some convergence between artificial neural networks and organic neural networks it's an entirely different thing if Harvard Neuroscience says that there's some convergence and so this is why I'm like visibly excited because this is something that I've been thinking about for a while what is the nature of super intelligence once it emerges and one thing that a lot of people are really skeptical of and hostile about is saying ah well we have no idea how super intelligence is going to work it's going to be completely alien it's going to basically be like talking to you know the there's just going to be no similarity between the way that we process information and our goals and you know I'm not sure that I believe that I I I personally think that uh that there are one probably diminishing returns uh because think about think of it this way the human brain is 30 smaller than the neander Neanderthal vein but we are objectively smarter than the Neanderthals were based on the sophistication of our art and artifacts that they left behind uh and so it's size isn't always everything now these are still very tiny models in the grand scheme of things so they haven't quite gotten up to the optimal size but at the same time there's probably going to be diminishing returns in terms of intelligence once you have the the raw materials of intelligence the raw machines you can do more of it you can do it faster but there's probably limits to the to the way that information is processed in the brain now that being said there's obviously a huge variance between humans there are some humans that can do calculus at light speed there are others that cannot do calculus at all and so on and so forth so we should still expect to see uh superhuman abilities emerge but the point here is that from a mathematical perspective where all Universal Turing machines we're all dealing in the same realm of physics with the same underpinning laws of physics matter and energy allowing us to do these computations and so basically the the takeaway here is that maybe the human brain already found some of the most efficient ways to do processing and so what we're seeing here is the first evidence that neural networks artificial neural networks are converging on processing theory of mind in a similar way that human brains do so the net result might be that AI thinks more similarly to us than we realize or at least has some of the same underpinning neural capabilities now obviously we can reshape artificial neural networks any way that we want we can slice them and dice them and recombine them in in ways and structure them in ways that that is not possible for human brains but then again the human brain has a lot of plasticity and literally quadrillions of connections inside of it that can create virtual circuits on the fly so it remains to be seen obviously don't read too much into this this is a very early study but I've seen this trend for several years now I used to listen to this podcast called neuro-inspired um or brain inspired anyways they talked about this kind of thing uh for the last five five years or so and then finally the last paper that I want to cover today is the koala paper the cognitive architectures for language agents paper now uh the reason that I wanted to cite this paper is because one it came from Princeton so another uh ivy league aside from Harvard but another thing that this paper does really well is it introduces all the background of cognitive architecture and gives you some of the the ground rules the how and why of cognitive architecture and the history of cognitive architecture and so this is of course something that I've talked about for a long time I've written three books on cognitive architecture now and but this is a little bit more validation and Vindication from the academic establishment and you'll notice that the uh the cognitive architectural diagram they have here is basically the same as the sore cognitive architecture which has been around since what the 70s 80s is when it was really kind of being worked on um so the book that I wrote a symphony of thought and natural language cognitive architecture both proposed more sophisticated cognitive architectures or at least some cognitive architectural paradigms um so this paper doesn't add too much but if you're not familiar with cognitive architectures it's a really great entry point that's why I wanted to share this one and they have a very linear kind of process here observation proposal evaluation selection execution that's great it's a relatively simple linear thing um there's a few things missing from this namely this paper does not address ethics morality or even really decision making Frameworks so you can create an autonomous agent with what purpose what mission they don't really talk about how to deeply integrate that into the design this is more of a general purpose slightly biomimetic cognitive architecture so I'm not particularly personally impressed but I'm glad that this paper exists to bring more of the conversation at a high level into the space of cognitive architecture and large language models and autonomous agents okay so those are the three papers let's just do a quick recap and as we wrap it up for the day um so CSI the uh the the Swarm intelligence uh this is going to be really useful I think for open source science policy and consensus what I'm really looking forward to is the CSI paper to be integrated as a feature into Microsoft teams and slack and Discord where you can just say hey automatically surface the main points uh you know as these threads go and share those points with the rest of the of the community now again this is something that's relatively simple as long as you have API access to chat GPT and Discord you can implement this today uh and it's it's super simple it's just on a minute by minute basis or on a regular time basis you take a roll up you summarize kind of the key points that are being discussed and you share it to the most relevant nearby groups super brain dead simple there's all kinds of ways you can make it more sophisticated and more complicated but the fact that you can get such good results with it such a simple algorithm is really encouraging number two theory of mind and artificial neurons this is really profound evidence of neural convergence which I think is going to be a really big topic as we approach AGI and super intelligence we might discover that human brains are already the most efficient way and so that as machines get more and more intelligent they actually become more like us so uh you know there's there's instrumental convergence that Nick Bostrom uh proposed that maybe as uh AI becomes more autonomous there's going to be several goals that all AI Converge on which when you take a step back it's like all AI is going to want resources well all humans want resources all AI is going to want to self-preserve okay all humans want to self-preserve so basically like as more time goes by I'm less and less impressed by uh by Nick bostrom's work and I know that that's like okay you know I'll probably catch some plaque for that but if you look at an at a machine as an agent in the realm of physics and energy yes it's going to have some same some similar uh needs as us now what I'm talking about here though is not just from the matter of energy and material what I'm talking about is the way that it processes information and perhaps you know maybe maybe just maybe a few billion years of evolution has already discovered the most energetic uh method of achieving some of these things and so I anticipate we will probably see a little bit more convergence but like I said it's not it's not that it's functionally or physically processing in the same way but rather some of the Transformations that are happening to the information and the way that it's the way that the neural networks the artificial neural networks are treating the information that flows through them is similar enough to some of the ways that various brain circuits organic circuits treat information if this continues this will have massive massive implications for the way that we think of super intelligence and artificial intelligence moving forward now again this even as a as an I.T guy as a software engineer as an architect I will say that okay great even if on a on a you know microscopic level you look at a model under a microscope it does something similar to the human brain that in no way says that that they're going to be agentic just like us that they're going to have a sense of self-preservation just like us that they're going to have the same goals and morals and values as us that's not what I'm saying at all all I'm saying is that the way that they process information the cognitive tools that are emerging in artificial neural networks are similar enough to us that maybe there's a possibility that um that that we will actually be able to understand each other uh for the foreseeable future it's also entirely possible that they will evolve entirely new cognitive Machinery that we just don't comprehend that they will come up with novel information processing schemes that our brains are just not capable of intuitively understanding so one example could be um it could be exponentials no matter how much training you have human brains are just simply not equipped to intuitively grasp exponentials we live in a geometric world and so we're able to throw footballs back and forth we're able to you know kind of anticipate where something is going to land if you throw it or if it's dropped but if you look at an exponentially changing thing your brain just does not have the neural Machinery to uh to understand that or to intuitively grasp it no matter how much experience and training you have and so in that respect it's entirely possible that artificial neural networks will continue to evolve in that direction where they will be able to understand things because of the underlying neural Machinery that they possess or that they are able to emerge to surface in their training regimen then we that we just are not capable of I don't know it remains to be seen because again there's a huge amount of plasticity in the human brain and it would be incredibly premature to assume that machines that super inefficient machines are capable of even forming circuitry that our brains are not already capable of and possibly have already mastered now either assertion requires evidence so it remains to be seen and then finally a quick recap of the koala paper it's relatively simple simplistic take on cognitive architecture but again my biggest takeaway is I'm glad that more people are talking about cognitive architecture in the context of large language models and agentic Frameworks because like it or not autonomous AI is coming and one thing that I want to point out is that in the next few weeks my paper on the autonomous cognitive Entity framework the ace framework is going to be finished and published and this is going to knock your socks off so stay tuned and thanks for watching I hope you got a lot out of this cheers thanks and have a good rest of your day