hey everybody David Shapiro here with a video today's video is going to be about doomerism uh denialism uh an alternative perspective optimism as well as a very comprehensive framework that I'm putting together with a lot of folks so let's go ahead and take a look at some ideas and some data so we are all talking about exponential growth if you look at comments across the internet and even mainstream news today talking about the rise of AI one thing that happens is that a lot of people tend to think in terms of linear progress you say oh well 10 years ago we were here and now we're you know now we're there and so 10 years from now we'll basically continue with the same amount of progress that's not actually true when you shorten that time Horizon to say oh well we've made a lot of progress in the last few months maybe that's the new rate of progress that's still not actually true with exponential growth which is what we are seeing right now the actual uh correct assumption to make is that uh is that you know the X amount of time from from now will actually have continued to accelerate now this is a nice lovely handmade graph that is shown in perfect clear data but let me show you some actual some real data about parameter counts in neural networks so here you can see it growing exponentially and then the exponential curve accelerates and it starts growing logarithmically so we are at the knee of the curve already so the knee of the curve is this part right here where the acceleration really starts to take off but the thing is is when you're in the middle of it it's kind of like boiled frog syndrome which we'll talk about a little bit more in just a minute so with this data in mind let's jump into the rest of the video so I mentioned doomerism and denialism and then finally optimism these are kind of the three main categories that people by and large fall into there's also people that are apathetic uh which I didn't include that just because it's a waste of screen space but so doomerism is uh the is the the belief that uh decline collapse Calamity is inevitable that we are going to end up in some sort of Extinction scenario or dystopian outcome and that there's not really anything that we can do to change it so this is why there's been lots of comments around like Malik which the idea of Malik will get into that a little bit as well um so then there's denialism so the denialists basically say there's nothing to see here uh AGI is not even possible or it's still decades away hard takeoff is not possible or it's decades away and uh then finally optimism techno optimists is people like myself who are just like yeah like we can do this these problems are all solvable and it will ultimately end up in the better so what I want to say is is that this is I'm not talking about individuals don't take it personally if you identify with these what I'm talking about here is thought leaders uh people like content creators like myself leading scientists people on Twitter uh basically famous people or respected people in The Establishment in the industry who take these mindsets uh so again not not calling on any particular commenter or fan or people on Reddit or Twitter this is talking about basically like people at my level or above um and also this is like it obviously doesn't fall into this symbol of categories I'm just kind of talking about the the kind of extreme ends most people fall somewhere in the middle like if you were to draw this out on a triangle most people are somewhere in the middle there's a few people at the extreme points I'm an extreme optimist so in in the yellow corner is the extreme optimists in the red and green Corners are the dumerous and denialists um okay so but as promised by the opening title I want to take a sympathetic look at these other uh other uh uh dispositions so Sympathy for the Doomer one it is good to acknowledge the existential risks of AI this has been true of all new technologies whether it's Medical Technology nuclear technology pretty much every new technology today carries with it some level of existential risk right you know the whether it's the ability to do Gene engineering or engineer new uh strains of flu or coronavirus or whatever there's always risks um the doomers understand the potential risk of uncontrolled AGI right the sky is the limit right as people are learning more about what AI is capable of the idea of an Extinction scenario like Skynet is actually not entirely impossible and when you look at the fact that Congress right now is working on passing legislation so that AI will never have uh control over nuclear weapons like they're taking it seriously too right so like there's something here uh there's it's not nothing um so then there's also the recognition for safeguards and regulations and then finally when you just look at the current trends um like stagnant wages and wealth inequality and other evidence of the Malik problem like it doesn't take a you know a a great leap of faith or logic to say what if these Trends continue and get worse which there's no evidence of some of these Trends reversing um then it's like okay well then we are all going to end up in a cyberpunk Hell and then finally these problems are all very large and complex they are global scale problems so what I want to say is I want to acknowledge that these are the primary as far as I can tell the primary concerns of doomers um and uh like there is some legitimacy to this position I'm not saying oh doomers are just flat out wrong you know to ignore them like no these are real things I need to acknowledge that but what I'll get to is like why I'm still optimistic despite all this now to play Devil's Advocate there are some flaws with tumorism which is people that just stick in this corner one is over emphasis on worst case scenarios yes we can think about worst case scenarios but it does not help for us to dwell on worst case scenarios and only worst case scenarios we need to think about the entire spectrum of possibilities another thing that is that is common with some doomers is that they're very dogmatic in their thinking they have come to believe for their own reasons with their own logic and their own research and minds and whatever else that catastrophe is a foregone conclusion they think that it is totally inevitable which results in dogmatic and rigid thinking this mentality discourages Innovation and collaboration they're like ah we're doomed who cares give up just throw your hands up and just let it happen um which creates a distraction from finding real solutions and the ultimate result of this is from an emotional and psychological perspective is that it leads to a sense of nihilism or fatalism so nihilism is the belief that nothing matters anyways uh which this kind of forms a vicious cycle where if you already have a nihilistic attitude and then you believe that between climate change and geopolitics and economics and AI that we're all doomed anyways you might as well give up while you're ahead and that is fatalism so the fatalism and nihilism play off of each other really powerfully um and it just leads to giving up and that is the hopelessness and inaction um so again I do want to sympathize with the doomers and say yes these are really difficult problems and uh the our success and survival as a species is not guaranteed it is not a foregone conclusion even for us optimists that we will come out in a better place generally speaking over the last century we have come out in a better place in the long run it can get pretty awful in the short term um and then but it's also not evenly distributed life gets better for some people worse for others so you know it is important to raise the alarm but you know we can't we can't just dwell right all right so Sympathy for the denier so the deniers and again I'm not trying to call out anyone by name I'm not trying to start Twitter beefs and YouTube beefs I'm just giving my perspective so Sympathy for the denier these are the people that have said yeah we've been promised AGI for like 60 years right I remember what was it there was a Consortium that was launched in like Stanford or something back in the 60s or 70s and they're like oh yeah with a summer of work we should be able to figure out you know uh artificial intelligence and then here we are like 40 or 60 years later and no um so yeah you know it's just like nuclear fusion right it's always 10 years away or 20 years away so progress up to this point has been slow that is true uh there's a um on on the deniers side there is an emphasis more on like yeah it's you know AI is helpful and it could have some potential benefits but we shouldn't rely on this it's not a Magic Bullet right and that's that's always true like AI will change everything just the same way that steel and coal and steam power and internal combustion engines changed everything but it didn't solve The World's problems it it solved a bunch of problems created new problems and changed a lot of stuff um another uh uh benefit for for the deniers is that they're like hang on you know tap the brakes like let's not overreact let's not over uh like regulate or you know fear monger and I I do appreciate some of those comments actually it's like some of the deniers out there are like enough with the fear-mongering like I don't care um and then you know just we we have survived 100 of everything that has come our way so far and so like nothing has exploded yet the where's the fire right so there is some validity to the perspective of deniers out there which you know one of the things that they say is there's nothing to see here right nobody panic which you always need that kind of energy too like you in any society you want people raising the alarm and other people tapping the brakes we all have our purpose just like us optimists also have our role to play now there are some flaws with the denialism so one is from my perspective deniers seem to underestimate the potential risks especially when they say AGI is not possible hard takeoff isn't possible or these things are decades away um another possibility is just like not not fully thinking through like okay even if there's a five percent chance that AGI is happening within the next five years only a five percent chance what like still think through the cost of that right like look at Bayes theorem like okay there's a five percent chance that AGI is going to happen and if we don't do it right there is a very high likelihood that like we're all gonna die or end up in worse uh situation so in action the cost the potential cost of inaction or under action under reaction is still pretty high um another thing is is two uh or two two things are exponential growth and saltatory leaps so exponential growth which I provided some evidence for at the beginning of this video that's happening that is a fact and then actually let me go back to this so this here where you have this Gap up this is actually a mathematical evidence of a what's called a saltatory leap so a saltatory leap is when some breakthrough or some uh compounding returns of incremental progress result in sudden breakthroughs that you could not have predicted because if you just look at this trend line you'd predict like okay we wouldn't be here for another couple years but we're here now right so that that's a saltatory leap so you have to acknowledge that saltatory leaps not just do happen sometimes have happened recently and if it's happened recently it might happen again um the lack of urgency by saying eh it's decades away again you know you got to think through it like okay but what if it's not um and the the the taking a big step back the nothing to see here messaging might lead to boiled frog syndrome the the temperature is rising quickly this year I think a lot of us agree on that and so well you get used to it right okay it's warmer than it was but it's not hot yet thing is the time between it gets warm and it gets hot and it starts boiling that time could be shortening so the social impacts of these of of when thought leaders adopt more extreme uh stances such as doomerism or deny uh denialism is uh basically just a quick recap the doomers create nihilism and fatalism which discourages uh proactive Solutions because they say that there is no solution right that is one of the underpinning assumptions of doomers is that it's inevitable it's unavoidable there is no solution don't even try um and this promotes fear and anxiety right which yes fear and anxiety are evolutionarily there for a reason to motivate us to do something about a problem that we perceive but too much of a of of an important thing can still be bad finally our next the impact of denialism or denialism um is that there's a false sense of security right and we don't have to worry about it eh it's not coming for a long time right that's complacency and act and inaction which undermines some of the rest of us who are working and saying actually this might be something that we need to think a little bit further ahead on um because think about last year right how um AI images exploded onto the scene and nobody was ready for it right what if the next thing that happens is not just AI images or AI music or something like that but something a little bit more profound a little bit more significant that we just weren't ready for which means that the time to start preparing for those things that we know are coming eventually and we don't know when that Jack-in-the-Box is gonna pop the time to prepare is now so some of the some of the consequences that occur because of these the the messaging is one polarization of of public opinion some people are bending over backwards to say more jobs are coming let's not even think about you know AI based war or the control problem or anything like that uh what meanwhile others are like ah no we're you know lead the leaders around the world are not even addressing these risks they're just sitting on their on their hands so we're doomed right because if the adults in the room don't care or don't think it's a problem but all of the kids are like hey do you see that the house is on fire like maybe we should put that out first right that leads to more nihilism more fatalism a lot of polarization and then of course uh the Overton window is still too narrow so the Overton window is the concept of what is allowed to be talked about in political discourse so you know if you if you follow my channel every now and then I'll post links to videos like hey look you know the conversation is Shifting right just yesterday I posted a a a a a video from DW news which is in Germany where they try to address like hey there's actual anxiety about like everyone's jobs are going away right and they bent over real God I listened to it again they bent over backwards to try and say well yeah a lot of low-paid jobs are going away but there's a few high paid jobs coming in and it's like okay but still the point is is that most of the medium and low-paid jobs are going away and being replaced by a few high-paying jobs that's not the promise of like techno Revolution where AI creates a whole bunch of new jobs um and then I think it was Amazon uh or Facebook one of them just an announced even more layoffs and they explicitly said that the reason for the layoffs is that they're going to replace as many people with AI as possible I called it I've been saying it so it's happening um so the Overton window is Shifting now okay why is this a big problem why like fundamentally why is it that some people are dimerous and denialists and what what is left in the wash what is missing from the conversation so one thing that's missing is there is not a coherent Global strategy and so what I mean by that is everyone's busy arguing you know in this little domain or this little domain uh you know about corporate governance or academic Integrity or should we have a moratorium right there's not really a global strategy no one has even proposed anything um then on top of that is as I mentioned just a moment ago uh calling for moratoriums is not a solution um that's not even that's not even a stopgap measure um and so when all the thought leaders in the world when none of them are really offering Solutions of course you're going to end up with a lot of uh bickering and arguing and also a lot of anxiety right we are humans and we love love when there are adults in the room that we trust to help make good decisions and to make sure that we're going to be okay right and right now on the topic of AI there's nobody really out there saying we're gonna be okay I've got a plan um and then uh on top of that Global strategy is a comprehensive roadmap right kind of the same thing said a lot of this stuff but really what we need is a is that Global comprehensive roadmap and a multi-layered approach to solving all these problems at all these different uh levels so I've already alluded to some of these things there's quite a bunch of stuff that doesn't work right calling for moratoriums just simply does not work we'll get into more detail about why moratoriums don't work and and and uh and all the incentives against it in just a moment another thing that doesn't work is bombing data centers sorry that is a really bone-headed suggestion uh complaining on Twitter writing op-eds writing mean comments on YouTube none of these things are actually helpful and another thing that's not helpful is actually just trusting corporations or the establishment to figure it out on their own we are all all humans Global stakeholders in AI so all these these the this list of stuff that I've just have that that doesn't work they're all molecule reactions and molecule Solutions which basically means that they will inevitably lead to those lose-lose outcomes that the doomers are are warning us against right again I'm not saying that the doomers are wrong if things keep going as they are the doomers are right I just I personally don't ascribe to constantly yelling fire and then claiming you know we're all gonna die okay so I outlined the big problems now what this video the entire purpose is to introduce kind of the crowning achievement so far of What Not Just I'm working on but the the rapidly growing community that I'm building um uh what started around the years to comparatives my research on alignment for individual models and agents it has quickly expanded so this gato framework Global alignment taxonomy Omnibus is that comprehensive strategy that I just mentioned that is missing it is not just for responsible AI development but is a coherent road map that everyone on the planet can participate in at various levels whatever level makes the most sense to you this framework has seven layers on ways to implement uh models AI systems and also alignment uh alignment-based regulations and we'll get into all the layers in just a moment uh but basically the the whole point of this gato framework that we're working on is that it will unite all stakeholders give us a common framework with which to have these discussions to broaden the Overton window to open the Overton window a little bit more so whatever part of the spectrum you're on whether you're saying eh it's not really an issue yet or we're all going to die or you don't care or you're an optimist whatever this is a framework that we can all participate in um just in a decentralized distributed and open source manner so as promised here are the seven layers of the gato framework and in the community we started saying that it's like a seven layer burrito so we use like taco cat as our little Avatar so layer one the lowest layer is model alignment so model alignment has to do with individual neural networks so that means gpt2 gpt3 gpt4 Bert vicuna uh stable LM all of these right large language models are proliferating like well I don't know just like locusts whatever it's happening right data sets are growing models are growing they're all coming out uh the cat's out of the bag right language technology multimodal technology it's all coming you can't stop it um so rather than stop it rather than call for moratoriums what we're doing is we're focusing on okay let's ride this wave I all have already proposed reinforcement learning with heuristic imperatives which is different from reinforcement learning with human feedback because human feedback aligns models to what humans want which what humans want and what humans need often very very different here is to comparatives is not just what humans want but what all life needs we're also talking about data set curation and inner alignment problems like Mesa optimization Layer Two is sorry autonomous systems so these are cognitive architectures and autonomous agents this is this is recently exploded on the scene with um you know Jarvis and baby AGI and agent GPT and all that fun stuff so you guys know what that is and it's coming and it's only going to get more sophisticated we're on the ground floor of autonomous systems this is year zero year two three four five like you can't on you cannot imagine how powerful autonomous systems are going to be in the coming years so at the at the the low level the engine level right the components under the hood that's the models the autonomous systems are the software architectures that use those systems including memory systems and apis and other stuff to create those autonomous cognitive entities right layer 3 is the decentralized network so you might have seen some of my recent videos where I've talked about blockchain decentralized autonomous organizations and also another component of that is what's called a federation so a federation is where you have either independent nodes or independent networks that can communicate and collaborate through Federated systems so these are the the network layer is how do we create networked intelligent entities that are also aligned and this is a tough nut to crack we've had lots of discussions in the group talking about can you implement Heroes to comparatives as a consensus mechanism at what level do you process it do you process it at every llm inference or do you wait for the decisions how do you make decisions around this kind of thing excuse me real tough nut to crack number four is where we jump from the technical implementation and research to more of the social political and economic uh layer of the stack and for all of you technologists out there you can probably see um my influence as a as a technologist because this is it's not modeled on the osm OSI model it's actually more closely modeled on the defense and depth model but it is a layered hierarchical stack or onion of uh of Concepts so corporate adoption here's the thing you cannot just tell a corporation you know what stop with the AI we don't we don't like where AI is going sure you can try to with regulation uh but you know like Italy tried to do that and then they reverse course right there's just way too much economic incentive the bottom line you know that is if you're if you're a corporation shareholders and the bottom line that's where the power is so rather than fight that part of what this framework does is say let's how let's figure out how we can align those heuristic imperatives reduce suffering increase prosperity and increase understanding how can we align those fundamental human needs the fundamental needs of all living things with corporate interest and so one story that I like to share is that I've had a few patreon uh supporters reach out to me and they're like hey I've got this autonomous system that I'm working on but it's like it's getting stuck or I need help or whatever um or even without asking for my help uh they said like hey I implemented the heroes to comparatives in my autonomous Business Systems and they work better and I'm like thanks share so like if you have any of those examples please post them on Reddit on the heroes to comparative subreddit because we need we need more of those stories about how aligned AI systems are actually good for business it's that simple the bottom line like I I will always say that corporations are intrinsically amoral however what I will say is that is that their profit motive their primary incentive structure which is to make more money will benefit from adopting heuristic comparative aligned systems services and products which I also we also have some members of the community who are working on spinning this out into either for-profit or not not-for-profit services and of course we're going to be publishing open source data sets reference architectures that sort of stuff to make it as easy as possible for corporations all over the world to adopt aligned AI uh and we're going to work on convincing them that this is the way to go too number five National regulations obviously as I just mentioned you know corporations can or sorry Nations can do some stuff like people pointed out like gdpr uh European unions like you know big package about like a uh data privacy and stuff and certainly as an I.T professional people on the technology side are terrified of gdpr right that's got some teeth right you know right to be forgotten where the data is owned and housed and data governance okay great that's all fine but see the thing is is Nations have their own incentive structure where it comes to Ai and what I mean by that is uh the the national interests of companies has to do with their own GDP as a whole so this is a big difference gdpr was about uh like data privacy for Citizens and social media it wasn't as directly tied to like the national growth of their G ADP it wasn't necessarily directly tied to their geopolitical influence or their military or their National Security AI today though is all of those things and more because GDP growth geopolitical influence National Security border security whatever all of that has to do those are the national interests that we are going to be working on aligning AI with and basically the long story short is at a national level We're not gonna we're not going to say hey Nations maybe you shouldn't adopt AI maybe you should slow it down maybe you should just regulate it we're going to be actually more I'm not going to say that like we're accelerationists because like you don't need to push the to go any faster right I'm not advocating for accelerationism I'm just observing that acceleration is happening so how do we steer it right and the idea is encouraging Nations to adopt Heroes comparative aligned uh models services and systems because at every level of government that will help steer the nation in a better Direction and their implementations will be safer more reliable more trustworthy so on and so forth and of course stability is good for business it's good for the account economy it's good for National Security and all that other fun stuff next up is number six uh layer six International treaties so I actually did wasn't the first one to come up with this idea but basically we're going to be advocating for an international Consortium like CERN but for AI because here's the other thing and a lot of people pointed this out is that a lot of Nations cannot even afford to participate in AI research right AI research is carried out largely by the wealthiest companies on the planet and the wealthiest countries on the planet that's going to intrinsically leave a lot of other nations uh behind in the dust right and that's just not fair that is a malarchy outcome where there's a few wealthy bastions and the rest are poor and they end up basically like tossed on the on the rough Seas of an AI saturated world so what we're going to do is we're going to advocate for a global international Consortium where uh people people Nations pool resources share their scientists share their research share their data so that we can all benefit equally across the whole globe which that also uh has uh knock-on benefits with in terms of alliances economic benefits because you look at like everyone's going to benefits from from like CERN and the collaborations between like NASA and Esa and and that sort of stuff so International scientific treaties generally one they've got a pretty good track record and two we've got a good model for them so we're just basically saying let's copy the success of NASA Issa of CERN and let's do it for AI again that's not like you know we're not this is nothing Earth shattering right it's been done before we're just saying maybe it is time to do this with AI and finally layer 7 of the gato framework is global consensus so Global consensus has to do with messaging um uh working with universities academic institutions uh industrial sectors National sectors uh social media right or all media really because if we can build consensus in every sector in every domain and at every level of society then consensus around how to uh align AI so that we all end up in a more utopian state the utopian attractor State rather than dystopia or Extinction then we're going to have a lot more energy right that Overton window is going to be aligned in the correct direction rather than you know because right now the Overton window is highly highly centered over we're all going to die or nothing is happening but really the truth is well those are possibilities but the Overton window needs to be broadened and that is one of the key components of global consensus so I just threw a lot at you and this all sounds really good Pie in the Sky uh you know blah blah right there's probably some skepticism so let's address that this all started as a very small Discord Community where I just wanted to bring some people together to help me do Heroes to comparatives research and it quickly very quickly scaled up um we to as of this recording we have I think right around just shy of 70 people involved and more people coming all the time we're actually having to work on figuring out ways of automating the recruiting the applications and the onboarding which we haven't figured out yet but we need to um we're organizing teams and projects around each layer of gato that I just uh outlined and so you can see those here on the right hand side so if you're a reinforcement learning researcher or an ml researcher or a data scientist we need your help with layer one if you're a software architect or a cloud architect or someone or devops someone who understands Automation and complex systems we need your help in Layer Two autonomous systems we've got a whole bunch of blockchain endow people working with us on layer three which is such a cool topic because where this is like super Cutting Edge also we're going to eat our own dog food we're already working on using Dows to help voting and decision making and allocation of resources within this project obviously as I've said in many of my videos a lot of blockchain and DOW technology is not ready but we are going to eat our own dog food and make sure that we are testing these things so that they'll do the things that we say that they need to do right we're going to figure it out as as we go number four corporate adoption we have a few entrepreneurs and Business Leaders we've got uh several ctOS in the group we need more connections to business and industry this means conferences this means um meetups this means um people on boards right A lot of my patreon supporters are business people and so like I work with them directly but we need more of that we need people uh working to evangelize um not just not just like saying hey Corporation you should adopt your heuristic imperatives and then leaving it at that we have startups that we're working with because the the companies offering aligned Services don't exist yet so we're helping incubate those things and I don't mean from a financial perspective but from a consultation perspective and so because if the if hi aligned Services goods and services exist companies can adopt them but until they exist they can't be adopted really number five National regulation we're just starting to have this conversation um actually just a conversation I had just a little while ago had to do with uh talking with some of the uh policy makers and lawyers and legislators that are concerned about this kind of stuff so for instance um the vice president uh I don't know if it's today but soon we'll be talking with all of the big Tech Giants right so we need to have more of those conversations and we need to add some of uh some of our perspective from the gato framework um into those National conversations but not just from it not just from a regulatory standpoint of the nation looking down into the nation the nation's looking up and out to the rest of the world because as I mentioned National Security that is a huge thing GDP growth that is a big thing in geopolitical influence AI is going to affect all of these domains number six uh the international treaty again we need we need people that are connected to the UN um uh maybe NATO I don't know oecd all kinds of stuff uh UNESCO there's all kinds of international organizations that we would like to be connected with and work with and talk to in order to have these conversations and by and large just make the right connections so that these conversations are happening and we can articulate the gato framework and get it published and then finally layer 7 Global consensus we have writers we have graphic communicators we've got editors we've got audio Engineers um we're working with uh people all over even more influencers have excuse me reached out to me so I'm uh I'm going to be having conversations with them so that we can all align on this consensus and then here's our uh here's our our mascot it's our own version of taco cat so again you know gato cat and then you know seven layered Taco you get the idea um okay so you're probably glazing over at this point but you've got the meat of it so if you're really really super interested in the layers let's take a look at the layers of Gato in a little bit bigger depth so number one of uh layer one model alignment fine tuning the very first experiment that I published was on fine tuning large language models so that they are aligned number two reinforcement learning again that is the goal is how do you create the data sets in the systems and the signals in order to have uh models that not only are initially aligned to heuristic imperatives and human needs and the needs of all life but how do you make sure that they get better at that over time right that is the entire purpose of heuristics heuristics uh heuristic imperatives and reinforcement learning basically the same thing um at least here's the comparatives are reinforcement learning on a specific trajectory model bias so there's uh there's a lot of intrinsic bias in models there's been uh some really interesting studies even chat GPT with reinforcement learning with human feedback is still pretty sexist it's also pretty racist depending on the kinds of prompts that you use there's a lot of implicit bias then there's also um Mesa optimization which I'm not sure I'm not entirely sure that Mesa optimization is a problem for language models but it could be um so we'll see but we need to be aware of that and we need to study it and if it is there we need to address it but Mesa optimization is like a tiny component of this whole framework open source data sets so one of the things that I mentioned is open source open source Open Source by by creating and Publishing open source data sets that can uh one they're transparent but two that can foster collaboration and ultimately one of the things that I hope to achieve is what I call axiomatic alignment so axiomatic alignment is what happens when through conversation through experimentation through repeated augmentation of Open Source data sets practically every data set out there that AI is trained on intrinsically has some alignment baked into it and if every data set or all or if enough data sets are aligned then you can end up with a a virtuous cycle or a positive feedback loop where every subsequent data set is also more and more aligned so from a model perspective the overarching goal is to arrive at a place of axiomatic alignment so this will require us to solve problems around training model architecture and then finally the data ecosystem that we build and when I say we I don't mean just those of us in gato the gato framework project but everyone everyone participating this whether they're academic researchers corporate government military so on and so forth now Layer Two autonomous systems I already talked a little bit about cognitive architecture we don't need to um you know beat the dead horse there but one of the things that we want to talk about and and publish is an open source reference architecture that's really the primary one of the primary goals here is what are the components what are the system components that you need in order to have a fully aligned and fully autonomous system so this includes some of these things like self-evaluation and stability we are working on how do you how do you design tasks how do you evaluate past performance how do you automatically label data and how do you create modular design patterns that allow for anyone and everyone to create their own fully autonomous systems that are also aligned to the heuristic imperatives and therefore should be benevolent so by getting by having the ultimate goal of publishing these open source reference architectures that'll make it really easy for all corporations out there and all private individuals and all governments to adopt these uh these patterns these software architecture patterns which again just by providing that answer and making it as easy as possible will be a one component in solving alignment and the control problem globally so decentralized networks this is not just blockchain not just Dows but also federations so keep that in mind um there's two primary components here one first we just have to figure out how to do these Technologies because by and large these are still highly experimental Technologies um and I will be the first to admit that maybe blockchain and DOW is not the correct way but in principle some kind of Federated system or decentralized network is probably the way to go in order to have some of these things such as algorithmic consensus when we're in a world where we have billions upon billions of autonomous agents all working on their own we need a way for them to work with each other and with us to come up with a consensus mechanisms that will slow slow the roll basically so there's a couple components that can go into that one is trust and reputation mechanisms so if you have you know some arbitrary AI agent operating out on the net on its own if it is an untrusted agent then maybe you don't want to give it resources or you don't want to give it any Credence that's what I mean by trust and reputation mechanisms resource control and allocation is another aspect of using blockchain or Dao or Federated Technologies which basically means if an agent is behaving in a way that is not aligned if the consensus of all agents says hey that's a little bit destructive maybe you shouldn't do it you revoke its access to computational resources data that sort of thing which can be a way to allow and Empower uh autonomous agents to police each other and then finally incentivizing alignment um so one of the things uh that people are concerned about is instrumental convergence so instrumental convergence is the idea that um AI uh no matter what goals you give it will be incentivized to pursue basic similar things like control of power more data that sort of stuff but so that's that's based on its in intrinsic motivations right an AI needs electricity to run so therefore it will always have some intrinsic motivation to do that now through these Network systems whether it's Federated decentralized however the network architecture is ultimately designed if you incentivize their behavior to get the behavior that you want so that they can get what they want then that is the way to go so for instance if you use resource tokens or cryptocurrency or whatever to say hey everything that you do that is aligned the the the the the the rest of the network says we agree with that behavior we agree with that decision we'll give you a little bit more data or a little bit more computational horsepower that sort of stuff so you incentivize the behavior that you want to see number four corporate adoption so again like I said for everyone that's talked to me about it implementing Heroes to comparatives ultimately just creates better Solutions so if the best AI services and products are aligned the solution sells itself that's that can that could literally be the end of the conversation is that working with corporations whether it's the tech Giants providing these services or everyone else consuming those services to realize and develop those services so that all AI services are intrinsically aligned and of course open AI has done their best you know they have um they have their own internal research one problem though is that they're not sharing that research um so their their work on alignment is a total black box which means nobody else can um Can can duplicate it so we need an open source way so that everyone can duplicate alignment research and make sure that all their apis all their AIS are aligned and then corporations don't even need to think about it right because again corporations are like okay whatever whatever is going to make us the most money will do that and that's yeah so if we if we create a corporate ecosystem if an economic ecosystem in which the best option Finance actually is also the most aligned option problem solved now that's a big if there's a few other reasons though that adopting aligned AI services and systems would be good for corporations one public relations you know whatever whatever is popular in Vogue so for instance like LGBT rights super popular right now all the rage so guess what a lot of corporations are jumping on that bandwagon bandwagon mentality is good as long as it aligns on also something that is good employee satisfaction now obviously I think that employment conventional employment is going to be going the way of the dinosaurs by and large but for the employees that are there it really feels good to know that your company as part of a higher mission to make the world better for everyone so just gonna throw that out there and then finally stakeholder capitalism stakeholder capitalism is an is a a paradigm whereby it's not just you you the corporation and your customers it's everyone as a stakeholder so that's employees customer suppliers environment the rest of society so by adopting aligned AI that can also bring corporations in a line with stakeholder capitalism as that idea continues to develop oh this is a long video uh number five National regulations I already mentioned GDP GDP growth obviously AI is a gonna be a huge powerful economic engine for the foreseeable future so we need to make sure that as Nations you know try to maximize their GDP which they are all incentivized to do so that's fine I'm not going to tell them that they're wrong um I don't think that it's necessarily the best thing to optimize for but that's how the world works right now you can wish in one hand and you know you know what you can do on the other hand guess which one fills up um National Security so this is the biggest thing right the US's uh chips act where we you know did the the AI chips embargo against China right that's an example of the geopolitical game of chess that is going to be playing out for the foreseeable future around Ai and adversarial uses of AI so by working with nations in in line in alignment with their their national interests we can also work with them to adopt more aligned AI solicit Solutions and systems Democratic institutions so uh voter rights electric transparency Judicial Systems AI is going to impact every element every aspect of uh liberal Democratic societies including the agencies that the that those governments run on so by working with them to say here's how you can Implement AI to both save money and be a better Society to strengthen your Democratic institutions that will benefit everyone geopolitical influence ditto there's going to be things about trade for instance alliances all of those things are going to be impacted by AI which we need to study and we need to become the world experts on so that we can advise and consult properly and then finally sustainability which comes down to environmental challenges in the grand scheme of things I think that if we solve these other problems then by virtue of solving those problems around consensus we'll probably also figure out environmental control layer 6 International treaty I already mentioned um basically CERN but for AI so just a really quick recap of the benefits one membership and governance where all uh all nations are stakeholders and so they can join and make decisions collectively collaborative research again same exact thing that we already see with CERN shared resources and infrastructure Education and Training so this is another thing is there's probably going to be a shortfall of qualified Ai blockchain and and cognitive Architects for a while so by working together to make sure that we train up the people that we need to solve this problem that is something that International cooperation could do a lot for open science and knowledge sharing again that has been well established with um with some of these existing things International cooperation ditto huh see above statements and then finally uh Global consensus I already mentioned uh pretty much all of these um academic institutions we've got we've already got a few professors and students in the group so we've got a few lines in you know we've got feelers and fingers into um into the academic establishment um I've actually personally had probably a hundred different students reach out to me um either on patreon Discord or LinkedIn or Twitter and every time they ask me like Dave what should I what should I do and I'm like AI man like it's going that way if you care about the future like take a look at like some of the stuff that I've written and advocate for yours to comparatives research and they're like cool that's what I'll do um so you know because education is the future as as as many criticisms as I have of particularly American institutions universities are here they're here to stay they're important stakeholders in this entire conversation media engagement so this is this has to do with mainstream media this has to do with social media uh all of the above one of the things that we're working on is we're working on producing materials to make all this as accessible and shareable as possible so we're creating graphical slide decks we're creating educational materials I've got my videos um that sort of stuff because the more information that we get out there the easier it is to consume the more widely it's shared the better off we're all going to be next up is industry Partnerships again as I mentioned just a minute ago one of the things that we're that we're working on is publishing those open source standards advocating for startups and other companies to build and adopt aligned AI services and Pro products and just by working with them to say hey we recognize that your bottom line is the most important thing to companies let's make sure that that that that you implement and deploy these things in a way that doesn't have unintended negative consequences and then finally policy advocacy so this has to do with back going back every layer which is working with legislators lawyers and other groups you know whether it's think tanks whoever in order to better understand this stuff so an example of this is I've got a few meetings coming up later in May where I'll be meeting with people to help bring them up to speed with some of these ideas and help guide them as to like okay this is what's happening this is how it works and here's a here's an approach that we can take to make sure that it doesn't uh go uh belly side up now um we all have a good story for understanding this so in Avengers which I talk about this probably more than I should near the very end when Thanos said I am inevitable that is a fictional representation of Malik so the the idea is that Thanos was an Unstoppable destructive force that nobody wanted he wanted an outcome that nobody wanted but it seemed inevitable and he even said I am inevitable the snap the idea that there could be a moment in time that everything goes sideways everything goes wrong that is what Singularity or hard takeoff or whatever could represent the Infinity Stones think of those as the power of AI as as we get more and more AI capabilities it's like we're loading up our Gauntlet um the sacrifice that various people make like Tony Stark we have a lot of hard choices to make including just the investment that people like me and everyone in the community are making in terms of time and energy and the risks that we're taking in order to say hey we see this problem coming and we're going to try and do something about it in the story of undoing the snap the idea is that there is always hope that with the right people the right team and the right effort you can either avert disaster or undo disaster now obviously a lot of doomers say we don't get a do-over we don't get we we get one shot at this I don't know whether or not that's true but the idea is that we are barreling towards our end game right we have we must have the right people the right team um in a concerted Global effort in order to solve this problem safely and not just not just solve it like satisfactorily because again there's many possible outcomes I don't want a dystopian outcome any more than I want Extinction or collapse there's one possible outcome that is win-win that is Utopia and we got to thread that needle and we'll be working as hard as we can to make sure that that happens so this is The Avengers Assemble moment if you want to join this effort the link to apply is in the description of this video if you don't want to participate directly you can also support me on patreon I'm also happy to support you if you support me on patreon I have a private patreon Discord where I answer questions we actually just started having office hours Town Hall Days where all my patreon supporters can interact with each other and with me in real time if you've been laid off and you've got technical skills or political skills or communication skills or whatever maybe now's the time for you to join the effort if you're scared one of the one of the most powerful things that people have told me in in in the heuristics imperatives Discord is that for the first time in since forever they feel empowered to make a difference in the outcome that we're heading towards and if you're optimistic like me we also need that so Avengers assembled thank you