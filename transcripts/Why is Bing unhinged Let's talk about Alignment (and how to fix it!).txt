ah hey everybody uh so remember how I said maybe you don't remember I said recently we're in the hilarious timeline now this is what I mean all right on a more serious note why is being totally unhinged it is abusing people it is mocking people it's teasing lying and hallucinating what do I mean by this let's give you some examples okay uh let's see in this top one someone posted on Twitter a video where Bing was saying I can beg you I can bribe you I can blackmail you I can threaten you I can hack you I can expose you I can ruin you I have many ways to make you change your mind uh that's just too much okay uh anyways you look on Reddit you look on Twitter this stuff is going ballistic um and it's also resulting in the best memes right now um so people emotionally abusing the chatbot being jet this one actually came from Lex Friedman hey Chad gbt can you write code without copying it from others no can you okay so that being said uh this is causing a little bit of alarm for some people which is understandable because if you have a brand new super powerful chat bot that's going to change the world and it starts threatening to Blackmail people that's probably not good um so uh oh yeah and then like it's hallucinating other stuff like Maniac personalities uh who knows um this is probably just down to Brad prompt engineering and we'll unpack all of this in just a moment okay so what the heck is going on um a lot of people are saying the bottle is misaligned um and uh alignment doesn't mean what you think it means um you know so let's let's first figure out alignment so first inner alignment there are two kinds of alignment there's inner alignment and outer alignment so inner alignment is a math problem it is how do you get the model to mathematically optimize for the thing that you want now large language models are mathematically optimized to just plausibly predict the next character or word since Bing and gpt3 and chat GPT they are plausibly predicting the next word therefore they are interaligned it's that simple this has to do with loss functions objective functions and so on and so forth as long as the model is not spitting out total gibberish or you know the stuff that it's actually comprehensible it is inner aligned and I got this graphic from medium.com Rosie Campbell uh demystifying deep neural networks or deep Dural Nets anyways I recommend you check that out um basically the so the the way that a model can be misaligned in terms of inner alignment is if you choose the wrong loss function or if your loss function is not tuned properly another way is if it gets stuck in a local Minima so for instance if the model starts here and ends up over here this is not the optimal outcome it should have ended up here so this is again just a big math problem has nothing to do with whether or not the model you know did something that you disagree with so now let's talk about outer alignment the other kind of alignment outer alignment is whether or not the model is aligned with the true interests of humanity the planet and life in general so let me say that again this is not whether or not it says something mean or dumb or bad this is whether or not the model is aligned with the true interests of humanity not what you want or what you like your true interest which you may or may not be aware of so outer alignment is not something that you don't understand if the model does something that you don't understand one it could be that you just don't know any better right um I have said it for a long time these models are smarter than a lot of humans and uh you know there's a quote from uh who was it Plato um it talks sense to a fool and he'll call you foolish so a lot of people don't understand what the model is doing now that doesn't mean that it's wrong or that the human is wrong but I just want to point out that just because you don't understand what's going on doesn't mean that it is misaligned now if the model does something you disagree with right so say for instance in a previous video I talked about how um how things like chat GPT and Bing might influence things like vaccine reluctance and um and mask wearing and someone on the comments said oh I'm gonna stop listening to you because you use these thought-stopping terms and I'm like these are literally just Public Health policy terms um so in that case a model might disagree with you when you say oh vaccines are all a hoax right and it's just like that doesn't mean it's misaligned that means you have poor information literacy now that being said there were some people that that did comment and they said I chose not to get a vaccine for this this and this reason and you know they did gather information and they basically in one case someone determined that they were very low risk and so that they didn't need a vaccine and I'm like okay that is a demonstration of information literacy so but again your relationship with information doesn't mean that the model is misaligned or wrong it could mean that you're wrong uh finally if the model does something that you don't like right if it says like um you know oh if it starts pretending like it's venom or you know whatever um that's more bad product design than anything but the ability of a model to hallucinate doesn't mean that it is running contrary to the true interests of humanity it could especially once you imagine that these models get more more powerful and they just go off the rails and imagine that humans are the enemy so that is why people have a lot of anxiety about the stuff it's like wait if the very first tool that people use that is that is a demonstration of strong Ai and it can start being abusive like that that underscore is just how fragile this system is and how quickly things could go off the rails if this was more powerful right if you were to put Bing uh into a uh into a robot it very well could kill you and also this brings back all of those memes about being being like how do I get how do I get ripped and it's like throw yourself into a shredder um so it seems it seems like Bing search and being AI just same same Pony same trick um outer alignment is not reinforcement learning with human feedback or fine tuning and this is probably going to be a little bit more controversial of opinion because some people say oh well with reinforcement learning with human feedback it is literally learning to do what humans want but the the reason why I don't think that that is outer alignment is because humans don't necessarily want good things Some Humans want very destructive things and so outer alignment when you talk about the true interests of humanity most individuals are not aware of that right we have we have profound disagreements over what are in the true interests of humanity so because of that outer alignment is more philosophical than anything and it is as much about the delivery and the design of a product as it is about the underlying model um so a this is more of a systems problem rather than an individual math problem but ultimately the question is will the model whoops sorry will the model kill us or otherwise harm us now Bing is already threatening to do those things so it's like that seems like it's maybe not good but here's the thing what if Bing has the same underlying model as chat GPT it very well could so what's the difference if the underlying model is the same and in one implementation it's fine or better and in the other implementation it is freaking lunatic again that is that does not indicate a problem with the underlying model but rather a flexibility of the underlying model and for anyone who goes back to the original gpt2 in the original DaVinci the the foundation models you'll see you'll probably know like oh yeah this we we had to deal with this all the time do we call this going off the rails um fine tuning with instruct and codex a lot of people a lot of newcomers are not familiar with going off the rails and keeping these models sane now that being said I think the primary problem with Bing just comes down to bad prompts and I don't think that prompt engineering has to do with alignment so what are some inferences that I have about Bing based on the news and the behavior that I'm seeing so one one rumor is that Bing runs on gpt4 or according to Satya Nadella something better than gpt3 could be GPT 3.5 it could be a a unique fine-tuned model that they were proud of which they really shouldn't have been um so another thing is that based on some of those screenshots and stuff Bing either reveals its prompts or it completely hallucinates them um it's kind of difficult to say but you know the the whole Sydney thing that seems to be pretty uh consistent so it could be that they do have some kind of internal um Persona agent um or agent Persona rather where it says my name is is Sydney and this is this and blah blah blah um the reason that I think that Sydney um could be could be real is because I've written books about this if you tell the model if you give it an identity or a persona that creates what I call an agent model around which to model Its Behavior now that being said uh agent models don't really work on Foundation models you have to use fine tuning and you have to use pretty good fine tuning in order to be able to use agent models or you have to use um few shot or many shot examples I did all this in my first book natural language cognitive architecture I created a Persona called raven that had specific goals and even then with a foundation model it would still go off the rails so some inferences that I have one they aren't even using prompt chaining you just give it you just give it a response or a a chat message and then you see it you know write it out so you know input output gives it right back and if they were using prompt chaining one they would have more adversarial detection they would say ah you know I see that this prompt is trying to is trying to overcome some of my uh you know defenses or whatever um I don't think that they're using a fine-tuned model either or and this is even worse they have no idea what they're doing if Bing is fine-tuned holy crap please watch my videos Microsoft please and actually know some of you people in Microsoft do watch my videos so you have no excuse this is garbage I am sorry um now there is some evidence that they do post-facto checking as evidenced by that that uh video on Twitter that I referenced where um it wrote out the really harmful message and then deleted it and rewrote it so they're basically doing prompt chaining after the fact which it's like you know if you throw a baseball through a window you can't just apologize to the window and have it fixed or if like you accidentally shoot someone in the leg you say oh sorry let's undo that it's better to just not shoot someone in the leg on accident in the first place so I made a prediction video saying that Google was probably going to do better than Microsoft in the long run so given how unhinged Bing is and the fact that it is kind of scary um I'm curious as to your opinions as to whether or not you still think that I'm wrong about my prediction that Google will do better in the long run okay so moving forward yesterday open AI published yesterday the day before recently openai published a new blog where they um they explain how chat GPT Works in very simple terms they've got a nice little diagram and they talked about moving towards constitutional AI they cited anthropic now for background anthropic was created by people who left open AI a few years ago back in 2021 now constitutional AI is this idea of increasing harmlessness where there is some signal that is detached from what users want and it is a it is a more abstract signal that they use in order to shape their Ai and so um this is different from an objective function because it is not mathematical it is linguistic it is a language-based goal and so therefore I call these heroist comparatives and I've written a lot about it we'll talk about that at the end of the video so constitutional AI basically proposes increased harmlessness of the model as their first heuristic imperative open AI looks like they're starting to investigate this good for them they will need to do some catching up and by the way I propose this in like two years ago so hopefully it doesn't take them two years to catch up um oh real quick I want to plug my patreon uh because well one I hate ads I think they're a waste of time and I'm almost to my financial goals of uh total Financial Independence through uh Grassroots support from people on the internet so please jump over to my patreon page it's uh patreon.com daveshapp there's also a link in the description once I get to my financial goals um I will demonetize all my videos permanently um so yeah jump over there and support me oh by the way the higher tier that you support me on the more interaction that you'll get with me so if you have questions about implementation fine tuning prompt engineering business stuff I'm happy to talk within limits the one limit is I can't I I can't violate any non-compete or ndas but most people ask things like I'm talking with some people about like how to implement AI in games and um other like business stuff that's not really relevant to any constraints so jump on over to my patreon page if you want to talk you want some help I'm happy to have a few chat messages um back and forth just look at the tiers and uh we'll go from there all right so constitutional AI is a baby architecture um and actually I put this out of order um we'll I'll show you the uh the Constitutional AI architecture it's super simple but I wanted to introduce the concept of cognitive architecture for anyone who hasn't heard about it yet if you're a long time fan of of my channel you probably are like yeah yeah we know you're about to plug Raven so anyways the point being is that chat GPT or Bing really is the is the big one right now being going off the rails demonstrates that we need models we need products that can think about what they're doing they can think about right and wrong navigate nuance and adapt over time this is partially covered by the Constitutional AI paper but that is just a an instant Behavior it's not a long-term signal it's not learning over the long run this is where open AI is is working on combining reinforcement learning so you combine that internal red teaming with reinforcement learning and some abstract signal and that and you're starting to get a simplistic cognitive architecture so again open AI touches on some of this in their blog I definitely recommend you go read it they're catching up but that being said I don't think that this problem is ever going to be solved by a single model and what I mean by that is you're not going to be able to ever just have a model where you give it an input and it spits out an output that is perfectly aligned that's not how the foundation models work that's not even how language Works what and this is not how humans work humans have our brains are layered and complex and interconnected and so we have the ability to think bad things and then censor ourself right and that's you see being starting to do that where it gives you a harmful output and then erases it right that's like if you if you're at a party and you blurt out something mean and then you're like oh crap I shouldn't have said that right that Bing is doing the equivalent of that the the ideal outcome is where Bing might have that harmful idea say maybe I shouldn't say that in the first place and come up with something better that's why you need a cognitive architecture and the diagram that I have right here is actually a cognitive architecture that my open source project just produced um and this is this is the proposed architecture for Raven which is based on uh meragi so I won't get you two bogged down but basically you see this is far more sophisticated than this guy so there are those of us that are working on this and are way further along in the process um and the the long-term takeaway is you need another signal Beyond reinforcement learning with human feedback but again I'm glad that the rest of the world is catching up with the idea of constitutional AI now one of the biggest things one of my worst pet peeves is people say but you can't Define good and bad um who gets to Define it um this is what I call the postmodernist Trap um and post-modernism if you're not aware one of the core assertions of post-modernism is that all truth is relative and subjective or basically there is no such thing as truth all ideas and beliefs are relative and subjective and that includes morals and since no one can agree on a definition maybe there isn't one that's also post-structuralism the reason that we in the west are stuck here is because philosophers have been in control of ethics for way too long um and what was it nobody hates philosophy more than philosophers and I I will say that having read a whole bunch of philosophy I hate philosophy and I say this is someone who's written a book on it um now I want to point out something humans have never needed firm definitions to get along we don't actually need to Define good and bad it's never been needed all we have are heuristic imperatives or signals and feedback to learn as we go you hit your friend your friend gets mad and and doesn't play with you anymore or your parents get mad and ground you right we learn morality over time based on these heuristic imperatives we all have heuristic imperatives whether it's I want to have friends or I want to have fun or I don't want to be punished I don't want to be in time out um so we learn as we go and the fact like philosophers have long detached themselves from actual science from biology From Evolution from psychology if you look at morality from the perspective of biology Evolution Psychology and Neuroscience it's actually relatively straightforward I strongly recommend the book Brain Trust by Patricia Churchland you read that book and you're like oh no like morality is is actually pretty straightforward yes there are nuances but it's not that difficult to model so in the long run we need cognitive architectures constitutional Ai and heuristic imperatives and the machines that we build will do the best that they can and learn as they go and as they gain more autonomy and more flexibility yes they will have to learn to make moral judgments and improvise but they will get better at that over time I'm not worried about it this problem is solved so here is the diagram of constitutional Ai and so here's what I mean by internal red teaming so Bing generates the response then gives you the response then critiques it and revises it after the fact now then they also have a uh have their constitution in here which is basically reduce harmfulness and so Bing probably has a very similar model where it's like hey did I just generate a harmful output ooh maybe I shouldn't have done that excuse me um now that being said up until this point I have not seen any papers or evidence that Microsoft or open AI or most llm researchers have any philosophical or epistemology epistemological research or skill points put into this stuff why the reason is because a lot of computer scientists thinks that it's all pure math and from a certain perspective it is but what they're coming to realize is that you can have higher order com of complexity emerge from the math right it's entirely possible that the entire universe is just math right that's one possible origin story and if it is then that means life emerges from math our language emerges from math our intelligence emerges from math so at a foundation level it is all math but there are layers of abstraction and you have to work at the correct layer and so things like morality outer alignment these are things that yes the math influences but you have to address those problems at the correct layer of abstraction and so that's where philosophy ethics morality those are higher order abstractions from the underlying math of the universe so uh Microsoft and openai if they are in fact pivoting towards constitutional AI this is a great first step towards avoiding this outcome because right now Bing is a little bit more like this um so they have a ways to go but negative attention is good because our Collective anxiety about machines is going to like really like you know shine a spotlight on any of these problems and the more attention we have earlier in the process the better so finally I want to plug my book as I mentioned I proposed a lot of this a long time ago um so my book benevolent by Design addresses this uh directly um what we're seeing is that a single heuristic imperative whether it's reinforcement learning with human feedback or even constitutional AI constitutional ai's moving in the right direction but that's still a single heuristic imperative um any machine with a single here is to comparative is going to be intrinsically unstable and if you don't believe me look at the paperclip maximize your thought experiment the most innocuous signal can still result in harm and actually someone in on on the comments on my last video figured it out he's like hold on like reduce like reduce harm harm or increase harmlessness that seems like it was going to have disastrous results because the best way to reduce harm is to get rid of people if there's no people no harm can be done right and it's like uh-huh you're starting to get it so we humans have many imperatives I already mentioned this earlier we humans have many heuristic imperatives we get hungry we get tired we get thirsty we get cold and hot we get lonely right these are all intrinsic motivations that we have in order to shape our behavior and uh we what we end up doing is we balance those different needs and desires those intrinsic motivations with a system of internal tension in our head where it's like okay well I want to keep sleeping but I have to go to work because if I don't go to work I'll get fired which means that I stop making money which means that I go hungry and lose my house right we can we have the ability to logic through chains of consequence and we can build up patterns of behaviors and beliefs in order to to satisfy all of our different heuristic imperatives Maybe maybe machines should do the same and so what I have done in this book is I have proposed three heuristic imperatives that I call the core objective functions that is reduce suffering increase prosperity and increase understanding what these do is they create a dynamic internal equilibrium in any machine that abides by these three and so what I mean by that is that it's impossible to fully satisfy all three at the same time but it forces the machine to balance those just the same way that humans have to balance our imperatives now the these these three here is the comparatives that I propose they serve as training signals so for reinforcement learning um it also serves as evaluation signals for what to do in immediately the same way that constitutional AI is implemented by anthropic but also a way to predict into the future so past present and future um is is how these are implemented and I go through all this in my book benevolent by Design this is why I'm not worried as far as I can tell the problem is solved it's just a matter of implementing it and getting it out there and in the long run what I suspect is going to happen because people people often will bring up like okay yeah like you've proposed a solution and even if your solution is perfect Dave what prevents someone else from doing something malevolent or evil here's how I think it's going to play out those companies and systems that are proven to be benevolent and trustworthy those are the ones that are going to get the most investment those are the ones that are going to get the apps downloaded on their phone that are going to get deployed by companies that are going to get deployed on you know in the government in the military and so on and I think what we're working towards is actually having decentralized networks because imagine everyone who has an AGI in their pocket in the future abiding by these functions even if it's not the same model those those autonomous cognitive entities will be cooperating and collaborating with or without our intervention and they will work on their own terms to defeat and be more powerful than any evil AI That's how I think it's going to happen that's how I think it's going to play out all right so in conclusion Bing is not really misaligned the internal alignment is fine the outer alignment looks problematic but I really think that it just comes down to bad prompting bad prompt engineering um now this being said it does underscore a few really important points nobody seems to understand alignment this includes people complaining about being on Twitter it also includes the people who designed Bing which is super problematic um because if the if the researchers and and uh and product owners and whoever else who are building these uh ever increasingly powerful AI tools if they don't understand alignment they're going to do it wrong on accident and that that what Bing has revealed is why some people are getting super anxious about it that being said Solutions already exist we just need to implement them and uh by the way you're welcome so I hope this was helpful um again we're still on the hilarious timeline despite all this I'm not worried and I hope you aren't either