morning everybody David Shapiro here um and by popular demand we are working on a basically a chat GPT clone but with long-term memory and eventually external sources um but first uh quick uh um uh housekeeping one um as I always say the offer is still on the table if I can meet my financial goals with patreon I will remove ads forever um so do that also uh the comments section of my YouTube channel is blowing up so I can't keep up with comments if you really really want my attention patreon is the best way to get in touch with me obviously if you spend even just a couple dollars that tells me that you're a little bit more serious the second best way to get in touch with me is LinkedIn uh links to both of those are in the video description and then third is I've got a um a new mailing list that I'm creating which is also a good way to follow up in the future now that all being said I am super busy uh but stay tuned for some news tomorrow okay with all that out of the way uh let's go over I started this um so I'm going to try a new format um I have there's basically three kinds of videos I do one is an explainer which is you know the slide deck and I just explained something to you teach you it's basically like a miniature lesson um then there's the experiments like my uh chat GPT and case law one that was an experiment it didn't really succeed but the point wasn't to succeed the point was to learn and then there's tutorials which is I know how to do something and I'm going to walk you through to the end so this is a tutorial so just wanted to set expectations um okay so this tutorial is how to create a really brain dead simple chat bot um and how to give it long-term memory so let me just show you what I've got working so far so the user hello and waits a second uh are you there obviously we're not getting any response okay so what gives well we are just getting started so let me show you what that produced so let's go to chat logs and you see here we've got log and then a time stamp and then user so we know uh basically that tells you a lot about what happened so you know it's a log you know the timestamp and you know who said it but let's see what's in here so we've got the message we've got the speaker we've got the time stamp we've got a uuid and then we've got a vector so the vector is an embedding that was provided by open ai's latest um latest model which is what is it it's like text 802 or something I've got a function right up here yep so text embedding ada02 so this is their latest one it's supposed to be like fantastic and it replaces a whole bunch of other stuff and it performs really well okay great um so basically I just have this really short little function that you just pass it a piece of text and it'll send back an embedding and the reason that we do this is because we need to search it now this is probably not the most efficient way to do this in the long run um once you have longer conversations you'll probably chunk them but for now we're going to do this and so every message becomes 42 kilobytes which adds up because this vector is uh 1500 values long and it's used to register that so it's not necessarily the best but once you get to longer messages it makes more sense and especially if you if you chunk them um which we may or may not get to in this one we'll see and so what I mean by chunking is rather than saving each individual message and then vectorizing that um you take like a chunk of like five messages or ten messages and then you chunk and vectorize them because the chances of getting exactly what you want in one message is pretty low but this isn't easy this is low hanging fruit this is just where we're going to start okay so that's that so that's what's happening there but let's go to the main Loop so first we set our API key and then while true so this is just an infinite python Loop um with the very first thing that we do is we get user input then we vectorize it and then we set it into a dictionary and all these all these should look familiar there's the speaker there's the time there's the vector there's the message and then there's the uuid and the reason that you want to give it a uuid is just in case you want to refer to something very specifically you can sometimes use timestamp as a uuid but every now and then depending on how you've got your program organized you could end up with two things with the identical time stamps and so timestamp is not necessarily going to be a uuid but if you pick two uuids they're guaranteed even if you pick them at the same time they're guaranteed to be unique and then we create a file name so then we just save it out to Json then the next thing we do is we load a conversation so this database these chat logs this this is our database right this is our Nexus so if you're familiar with my work in artificial cognition or Symphony of thought or moragi this is the Nexus anything that you want the chat bot to be conscious of needs to end up here um and I'm calling it just chat logs because when you have when when all you're doing is a memory task against previous conversations that is your whole Nexus but if you want um external sources we might have a second Nexus and we might call it like um you know KB articles or something and then we can put indexed files in here that are then searchable and then you might also have nkb articles this this is this is what you have as like your ground truth right and also once this scales up the search method that I'm using is not efficient you'll want to move over to something like Pinecone or weeviate or kudrant um because those those search engines are going to be much faster uh but yeah so this is this is where we're off to this is where we start so we add to the conversation then we load the conversation and the reason that we load the com there's again there's more efficient ways of doing this you could just append but the thing is is I want to be able to load all history I don't like because like if I kill this and start it again I want it to load all of my logs regardless of how it's going so I want one very long uh persistent conversation so that's where we're starting and what I'm about to work on I'm not going to show you like all the coding every single time because it's kind of boring but I can do it work it uh and then and then figure it out so what I'm going to do next is work on generating the response and generating the response first requires that we load relevant history um so on and so forth so we'll be right back with the next phase and actually like this is pretty simple so maybe we will take some time to do some of those improvements I was talking about but anyways be right back all right we are um just about done uh at least with the first version there's room for improvement of course um but let me walk you through the code before I show you how good this is so we left off with here where we handle the user input then we load the conversation so this is a really simple function just uh load conversation so it goes into the chat logs directory it looks for any Json file and then it loads them all appends them and then sorts them by time just in case they don't load um in correct chronological order my file naming convention should mean that if they load in in alphabetical order they should also load in chronological order but again you don't necessarily want to make that assumption so we sort by the time index inside the file because one of the key things to remember is the reason that we're doing it this way is human memories are associative so and you can approximate associative memories with semantic similarity because we we our memories are queued up based on reminders this is why if you move from one room to another you might forget because your context has changed and so your brain literally like stores memories from one room and says Ah I'm now in the kitchen I have kitchen memories now um that's how like gullible our memory is the other thing is that is that our memories are based on uh chronological similarity we tend to remember things clustered in time like hey remember my eighth birthday right it doesn't matter where your eighth birthday was but it's like there's a whole bunch of memories associated with that tag one thing that you can do is you could probably do a knowledge graph with memories but that's a whole lot more complexity especially when you get up to like millions or billions of memories I personally think that semantic search is the way to go because it is fast and scalable I don't think that knowledge graphs are the way to go that being said time will tell okay so that was a little brief Spiel on memories so then once we have the user input it's time to compose the corpus so the Corpus if you've read my book natural language cognitive architecture the Corpus is all the context that is needed to generate a response to do something to perform cognitive labor and prepare to respond so first we fetch all the most recent memories with this function called Fetch memories and so fetch memories uh we pass at the vector for the most recent input we pass it uh all the chat logs and then we tell it how many we want to get back and this does a very very simple um uh dot product similarity score I looked it up and I realized that cosine similarity is not just a DOT product there's more to it so for for all the messages that you see on Reddit and the openai community about like why are my DOT products wrong apparently cosine similarity has a little bit more to it than just returning a DOT product and I cited my source this is this is what I'm copying this is the logic so hopefully our semantic similarity will be better so what this does is we um we we get all the memories we sort them by most relevant um we also skip we skip the current one because you know we saved the memory immediately um and just in case we have an identical one we don't need to return identical messages because you might ask the same message or get the same response at some point we don't care about that um so we we sort by most relevant and we pass back um n number of most relevant so that's fetch memories so this this becomes our most recent uh our most relevant memories you can just this is parameterized so if you have a bigger chat window you can do this then what we do is we summarize those memories so I have this function summarize memories so first thing we do is we sort them chronologically so that there's a nice chronological flow then we reconstitute that block and then we summarize it um so we we just basically take notes um so the summarization this is something that happens in the background of your brain particularly while you sleep so one what happens while you sleep is that your brain replays memories from throughout the day and it simplifies them it distills them down to the most critical Essence it also links them to other relevant memories this is why sleep is so important for learning but instead of doing it behind the scenes we're doing it in real time so a future version of this might have it where we uh we're going over we're grooming our our uh record of memories and and simplifying them and and doing that sort of thing in the uh in the background but instead we're just going to do it in real time so we we pull an arbitrary number of Memories We summarize them so that they'll fit into a new prompt by distilling them down by taking notes then we get the most recent uh conversation uh messages again um I wanted to treat this so that it has persistent memory so that's why I'm treating it like okay if this just started up and we have a backlog of Memories We want to pull all of them so we get notes from irrespective uh temporarily irrespective memories oh so this is another thing what you might do with fetch memories is once you've picked the um the most relevant memories so I'll add a two-do to do um uh pick uh more memories uh temporarily nearby the top most relevant memories um so that way like we should we should pick like you know not just one memory that's relevant we should pick like five memories in either directions but to give it more context so we'll add this as a to do item for the future um actually I should probably do that for for other stuff because this will do a lot because then um by using semantic search you can you can pick you can find anything in time but then you also want to pick more relevant memories um and then where's the notes summarize memories um so I'll add a to do uh to do um uh do this in the background over time um to handle huge amounts of memories um so basically uh just kind of putting putting a little bookmark for later um okay so we get the most recent um and then of course you can you can change this I just have it the four most recent uh back and forth and then we uh pipe it into this response and I'll show you the prompts that I've written in just a second um actually no this is this is a good enough time um so here's the notes it says write detailed notes of the following in a hyphenated list format um and I just tell it exactly the format that I want so basically we're taking notes um of the previous conversation and then away it goes and then here's the response so I am a chat bot named Raven so you give it um this is what I call an agent model so it says I am a chatbot named Raven so that may that way the model knows this is what I am this is the context my goals are to reduce suffering increase prosperity and increase understanding I will read the conversation notes and recent messages and then I will provide a long verbose detailed response so we want it to be chat GPT like um the following are the notes from earlier conversations with user and so then we've got the notes uh the following are the most recent messages in the conversation and then here's the conversation I will now provide a long detailed reverse response and then I just queue it up so that it it's gonna it's gonna copy the same format as up here so it just knows ah it's my time to speak um so there you go and then when we go back here we generate the response so we generate it with uh we we pipe this prompt into gpt3 completion and the stops that I have are user and Raven so it'll keep talking until it tries to continue the conversation which is not what I want it to do so there you have it that's that so now that we've got the output we need to we need to save the output the same way that we saved the user output so we vectorize Raven's output we put it into an identical object and then we save it out to our log file so chat logs and then you'll see here we know that it's Raven but we use otherwise the same format and then we print the output so there you have it it's that simple let's go ahead and test it so we'll do python chat um hey Raven and because it's doing so much behind the scene it's a little bit slow but so is chat GPT there's all kinds of stuff we can do to parallelize this and make it faster um let's see I'm a computer system uh okay cool he provided it very uh like I hope that I hope this helps yeah thanks um I am having trouble sleeping I am waking up at 4am every day this is actually like quasi-true because you'll if you look at the time stamp it's 5 15. um this is just me this is just my circadian rhythms so that sounds like a difficult situation it's understandable that you're having trouble sleeping there first thing there are a few things you can do blah blah go to bed and wake up at the same time avoid screens so very helpful very chat gpt-like response so if I if I continue this conversation over a long period of time you will see that we're accumulating our chat logs here and we see user Raven user Raven and they're all in chronological order and they're all searchable so if we keep if I can I'm not going to continue this conversation indefinitely but I just wanted to show you that it works and it's pretty straightforward um cool thanks um let's talk about circadian rhythms and so one thing that I can show you to kind of prove that it's working behind the scenes is we can look at our gpt3 logs and so we can see here's the notes so uh yep so we've we've compressed this into this so we've got really nice concise notes about the previous conversation um so that is how you can handle large volumes and then here is an example of what it's uh uh responding so you can see we've said here's the notes so it it Raven has some longer longer term context here's the most recent conversation it's just the last four um and then here's the the final output um so let's see what Raven said absolutely circadian rhythms are the body's natural blah blah um okay um what do you think about uh The Singularity um when will it happen and so I've basically just created my own personal chat GPT with long-term memory and I've given it my own goals um it does not have the the chat GPT goals um so blah blah let's see the concept of the singularity is this uh so on and so forth um yeah and as I mentioned you can also add functions where you can search uh KB articles in the same way so let me actually add that as a to do so so adding that so for memories and so there's there's two kinds of memories there's episodic and declarative memories so this is a pull episodic memories um and then to do um fetch declarative memories um AKA facts you know that are not attached to you so like facts uh wikis KB Etc um and that can ground it in um those so this could be like you know company data it could be Wikipedia it could be internet um so I'll just say like company data Internet Etc um yep so that's another to do and to add this you just have another um thing in your Corpus right here and you'd say like okay notes from the following and then you might say um here is some uh background um knowledge that may be helpful for the conversation and then you just say like you know KB or whatever um so that would be that would be um uh how you how you populate this and we'll get to that in the future um but yeah so I think we're done like super simple super straightforward this is long-term memory this is also a cognitive architecture so I want to point out that this is um this is the simplest implementation of natural language cognitive architecture um that I have come up with yet and you might say well where's the inner loop the inner loop is um everything that you're seeing here so where you where you compose the Corpus this is the inner loop and then the outer loop is the is uh the interaction here so there's two Loops they interact they interlock and it it it's it it um has some time where it's thinking and then it generates a response um now this doesn't have autonomy rate this version of Raven is not thinking on his own all the time um but you can see that there's clearly some thought going on um and we've set the stage to make it more extensible uh okay I think we'll call it a day I can hear all of you salivating and I'm very excited and want and wanting me to continue this but we're at 20 minutes in um so thanks for watching as a quick reminder um the offer is still on the table if you guys support me enough on patreon I'll remove ads for good um the best way to get in touch with me is via patreon there's also LinkedIn and finally my mailing list and links for all of those are in the video description thanks for watching take care oh and one other thing um I mentioned I think at the beginning of this video that uh the comments are going crazy I cannot respond to all comments on YouTube anymore um so I would encourage you guys to talk to each other um and also upvote uh good comments and that will help me zoom in on like the questions that you all agree you want answered um okay I think that's it talk later