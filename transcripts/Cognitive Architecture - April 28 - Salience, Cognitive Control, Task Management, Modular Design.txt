all right cool we are recording so just quick introduction um this is uh these are some of the guys from the heroistic imperatives uh uh Discord the research group um they're building a cognitive architecture um so I'll let you guys kind of if you want to introduce yourselves not required but uh yeah let's jump in and just kind of talk about where we're at in terms of cognitive architecture and and some of the stuff that you guys have been working on yeah sure um I'm database um I'm on the the uh cognitive AI Channel too y'all see me all the time I'm the one who's always arguing with everybody um Angela if you wanna yeah well uh I'm Ansel I'm on the same Discord I'm a computer science engineer and I just I'm really fascinated by this uh subject so I just had to go and dive in excellent excellent and so uh give us a tiny bit of background actually because um you guys um or a database I remember your story you you jumped into cognitive architecture what like just a few weeks ago right yeah basically um so it was about the time that uh Auto GPT went viral I was like this is really cool I don't want to have to pay for it so I uh started trying to convert it to uh to a local model which um we ran into uh we ran into some issues with that um but that's you know before then I hadn't really done a whole lot with programming or python at all um and so you know most of what I've learned in the past three weeks now I guess has been primarily through Chad GPT nice excellent yep that's that's the case for many people and Ansel you're you you're you're you've been a developer or an engineer for a little bit longer yes yes right right um I've been an engineer already for uh like three years I have years of experience mostly in c-sharp that's like my main language so yeah I I got into this project like three weeks ago with data and same with python so basically I've just been uh converting everything I know from C sharp to python and it's it's been a great experience learning a lot of things and it's pretty cool excellent so you know obviously uh Auto GPT baby AGI chaos GPT all that is relatively new gpt4 just came out you know what four or five weeks ago um and so uh like walk us through the the the process because I've I've had the privilege of kind of watching you guys figure stuff out from the beginning but can you tell us a little bit about um like where you got started what were the problems that you've you've overcome um and and what problems you're working on now yeah right the uh oh God the biggest problem that we ran into to start with was just finding a a host for a language model that had a working API um we you know we started with uh llama CPP but it's python API didn't work on uh x64 windows for whatever reason um there were a couple of others that we worked with and then eventually we came around to uba booga because and it's really they've got the API fix now we have people using it um right the the the real advantage of uba Booga is that it gives us um it lets us host multiple language models so we're not just stuck with um you know with llama or alpaca um the the new vikuna model is working in there it has some of the science um and coding language models available to it as well we haven't done a whole lot of testing on that we've switched over to open AI on the 3.5 model just to rapidly iterate on the development process and save some resources at home Mexican and by the time we did that was mostly because we were stuck with the uba Booga API and openai was working so we just like if we want to develop we need to switch fast to that and then the auto GPT code base was let's just say it was not ideal I spent an entire day trying to fix that yeah so so we ended up moving over to Baby AGI we got baby working on uh on uh open Ai and Uber booga temporarily and then uba Booga broke their API um for like two weeks which was wild but they've got it back working now um the people who are using our our new project some of them are using the Uber um API for and running it against vicuno which was the original gold so it works we're trying to keep it stable as we as best we can for those people and uh we're you know super excited honestly that all of this is coming together excellent uh data database and Ansel you guys have been working super hard on what you call the salience uh module um can you talk us through that a little bit because that that is a pretty fascinating set of problems so I have it loaded up right here uh as you can see it's just a simple python script uh one of the main reasons we decided to work on our our own project was because autodpt was not made to be um extensively modified I spent an entire day just trying to modify it myself and I had a pretty bad time I'm not gonna lie so we decided to just take the minimum viable version there was and start our own architecture so we created this little salience Loop here um we basically start by instantiating all the agents that we have um and then we start looping through it loading a task list and everything um so let me just run for you first see python we can edit this out in post right we can edit this out in post yeah the uh the the logic Loop that we built okay cool yeah basically okay and one Saturday okay got this pulled up um yep wait I'm watching it can you zoom in a little bit and kind of tell us a little bit about what this diagram is there we go right so this is this is the the logic Loop and one of the really helpful or not helpful but the one of the things that we've kind of designed from the get-go on uh on on this uh system was to try to make it um easy to build agents and logic Loops in a very short fashion so that you can iterate quickly um as opposed to you know if you wanted to to change the logic that open AI uses or even baby AGI really it really you really have to understand the entire code base that they're using and how different things interact we're trying to build a streamline flow where you can just write a logic Loop for the the actual interaction where it takes the prompt and then decides where to store it and you chain them together um so in a similar fashion to the way that uh that auto GPT Works um right now in the salience agent we're working with a predefined task list um baby AGI generates the task list automatically and later we're going to do a hybrid where we integrate both but for now we're just working with the a predefined task list so you know we sort the task list because every time every time you make an edit to a an item in chroma DB it falls to the end of the table or collection is what they're calling Vector databases um but we sort the list we filter it during the sort we filter for any completed tests past the current task to the job agent um so what the job agent does is it searches the vector database or the results database or any any results related to the current task summarizes those passes them down to the Java agent the job agent then passes the the contacts so that's your current task the um the related results from the summary um and a couple of other like metadata related things and it runs it runs in the the execute process which is right now identical to what baby was using but we've broken the The Prompt for that out into a easily editable Json file that um so so that you can you you can change that prompt on the Fly okay the execute agent pulls pulls the prompt from the juson runs it and then passes the results back to the job agent um we we don't have frustration implemented yet but that's going to be based on the uh write-up that you did on Reddit the other day yeah um into kind of check things out and then once it passes frustration or skips frustration then we pass those results back to the results database and it goes to the analyst agent so the analyst agent is here do I have everything there we go the analyst agent is here and it's a little bit more complex actually even though it looks simpler than that loopy stuff um we're actually taking multiple items or more items and adding them to to the uh prompt so let me basically ask do it let me let me pause you for a second and and ask how much of this architecture did you borrow from other projects and how much of this is stuff that you guys have have uh created on your own so right now the only thing that borrowed is the execution and it's right now essentially just the prompt so okay we what we did was we took we took baby agi's architecture and broke it out into lots of different pieces and then we changed all of those pieces completely um and re-imported them or reintegrated them as classes in multiple um in multiple uh agents right originally Baby AGI was only um was only one script and it was less than 200 lines that was the entire purpose of baby AGI right um we're over 3 000 lines on this code repo now um yeah we're averaging about nice up counting stop counting so so you've you've taken kind of the initial idea and really you've gone 10x on it you've made it more modular you've added a few new ideas new Loops um you know it like the one of the modules that you haven't haven't integrated yet I think earlier you said you haven't seen a need for like the frustration uh signal yet um but yes so I haven't gotten into into very complex tasks yet so maybe that's why we haven't right uh needed frustration yet but I'm I'm guessing we are gonna need it at some point yeah that's fair so for absolutely for any viewers who um who aren't familiar with with the write-up that I did on on frustration as a signal basically it is you keep a ratio that the simplest version is you keep a ratio of successes to failures and if the failure rate goes up then that tells you to maybe switch to a smarter uh smarter model or you can back up and try a different approach or eventually you can just stop and ask for help there's any number of things that you can do with the frustration signal but basically uh this is a frustration is a key component of cognitive control which is how humans say hey I expected to be making more forward progress and I'm not and so if you're not making the progress that you expect then it's time to either change your approach ask for help or change something so thanks for giving some context did you want to go ahead and jump into your your analyst agent yeah yeah absolutely um so the analyst agent is [Music] um he is a little bit more complex because we're taking you know we're taking the results the the most recent results and the current plus we're getting the summary uh from uh from this agent of the job agent or salience agent as well and we're we're feeding it the the uh a little bit more context on the task we're asking the um the the gbt agent to decide is this job completed yet and if it's you know if not then what we can do is send if it's if it's reached the frustration level we can send that task back to the task creation agent um to generate sub tasks potentially or new tasks um and then we're we're also um this weekend going to set up sending feedback back into the agent and you'll see what that feedback the feedback I think is going to really change the game on getting uh fewer fewer runs to to complete a particular task and then of course if it does complete it and we update the task with the task scheduler um and um and and tell it to move on to the next task um did did you get the terminal resize yeah absolutely I'm already sharing my screen again yeah yes I could not figure out how to get it recycle what I actually did was resized uh your monitor display yeah all right I actually increase the the scale of it anyway just zoom in um all right uh Ansel you ready uh uh database you were used sufficiently yeah yeah okay that's where we're at right now okay cool enough in the future so Ansel um give us a little bit of uh a little bit of background what is it that you're about to to demonstrate for us well I'm about to run the salience uh loop uh as you can see it's asking me a few things it's asking me if I want to start from where it left off basically restart the previous date and if I want to allow auto mode or manual mode for now let's just go with manual mode so and it's starting to initialize the agents here if you want to explain go ahead data well I was going to say um the the restore from previous state is to my knowledge not something as present in any other uh any other autonomous AI system out there yet so really cool stuff yeah so tell us a little bit about that feature because it sounds sounds valuable like you can uh you're in the middle of cooking dinner and someone knocks on the door and you come back and remember where you were right right well you could always just leave it sitting there because the system has this as you can see it it has a auto mode or a manual mode since we set manual mode every Loop is going to stop and ask us for feedback um all right um but if you have to turn your computer off because the FBI show up then um then uh you you know when you power it back up um the uh the the eight all of the task statuses or and results are stored persistently in chroma DB so you never lose any data unless you tell it you want you don't want to restore from a previous state you want to go ahead and go ahead and so in this case I basically wipe this memory it doesn't know anything uh I'm just gonna say yes just to keep it simple for now and it started to run the salience agent and a salience agent is going to call the execution agent to start uh running the task list for now the task lists are defined uh from our Persona Json so what we wanted to do was separate the code from the prompts so whoever wants to develop agents can simply look at two things uh the code of the agent what it's supposed to do logically and they can uh alter the prompts on the Json file so I'll show I'll show you the Json file in a second right now the first task is well to develop a test list to achieve a goal which in this case I don't believe I'm printing it here um yes it's supposed to create a program for an AI to search the internet so it did the first task let's just go ahead and let it run again it's going for the second task right now it's only showing you the the info like what you really want to see it's doing you're not seeing all the processing behind there's also a debug mode you can do um that starts showing the prompts what's going in what's in the database I can run another loop later uh with that turned on uh to show you how what it's doing internally and it'll it'll just keep going uh apparently it's reflected on the task so yeah let's see us leave it in auto mode I can change it to auto mode and it's now in auto mode it's you can see it says you can press escape to return to Automotive so at any point in the code if I press Escape it's just going to say switching back to auto mode and it'll stop and ask me uh for feedback again excellent at at this point I don't know if it's gonna say if it completed the task or not because the last task is to add out the the task list it made but it very much depends on the of the status agent if it decides it did it or not in this case it says it completed it and it also the good thing is it it provides a reasoning for why it decided it's completed or not so you can also uh get more information as to why that um it took that decision and help you do better prompt engineering so you can change the prompt easier so and if Let's uh let's pause for a second because um I I think that there's there's a tremendous amount of significance here so like walk us through the steps like it from from scratch it it had a what what appeared to be a metacognitive step where it said okay what am I doing and then it designed its own task list followed through the task list and then also evaluated its own performance to know when it had finished correct okay yes so here it's basically where it starts let's see yep at the beginning so the first thing it does is start to develop a task list and it's searching for results in its memory since it's wiped it says there is no results uh it sends that results to summary since no results uh exist yet it just says no previous actions have been taken because this is this is fed to I don't remember if it's fed to the execution ages or if it's fed to the status agent one of those two diplomatically oh both of them okay um so that the agents at least know at what step of uh the process uh they are or what any other actions they've taken before it's basically like a very primitive sort of memory as to well okay what have I done before right right and basically we send uh all of that information to the agent it runs and here's the result we get from the uh llm which is well it did create a task list so at that point we sent those results to the status agent and it's going to check if that task has been completed in this case it says yes the task has been completed and it provides the reasoning right here or why it thinks it's done so and at that point we go back into the loop so it's going to ask me again do you want to uh continue or not go ahead data yeah so I I want to come back to to the uh status agent so not only are we returning the completed or the status of whether or not it's completed we're also providing feedback so if you get a status that's not completed and this isn't in yet but this should be relatively simple to incorporate one of the things that we can do is provide additional feedback to the execution agent the next time it runs so um and in this situation um what we would want to do to to help automate this is pass it the um the feedback from the status agent so the status agent is going to act as kind of like a project manager um where it has all of its little Dev Minions that go out and do their their things and then they come back with their results and they print or present a a a summarized result of some kind now um that execution agent needs to be expanded upon so that we can get web searching for research and code execution to or to do code testing and stuff we may even have a code QA agent separately um but but um we we can take that feedback from the project manager status agent and push it back in then if if the agent fails so that it can do better the next time and it's not always running into the same results yeah um so let's let's unpack that real quick um yeah because I think we should switch over to the um to the personas default just on I've got this hey hey Data before we before we proceed because uh uh what you're what you're talking about is is like cognitive neuroscience and a lot of people are going to be lost so the status agent that's the supervisor right that kind of sits above the other things and it dispatches let me make sure I got it right it dispatches the execution agent says hey this is your set of tasks to go do and then that uses the salience loop in order to Loop through uh and ensure that that task gets completed is that the is that the correct well it's it's the salience it's the salience loop that calls the execution agent and Status agent separately okay so so the salience is what binds it together right exactly the same okay this one together correct okay and so by by iterating on that you are you're checking multiple times um for uh whether you're whether you're being successful the feedback that you're getting and it can because it's iterative it can adapt its strategy as it goes right so and it's kind of talking to itself in this sense so it's really more it's it's less like corporate developer type of environment and more just like internal monologue like if you were you know building a um if you were building a raised garden bed for example now you're like okay first thing I need to do is take measurements and then I want to cut wood and then I want to nail wood together finally we put it in place and then we fill it with you you you kind of like outline that and then you know as you're cutting wood like okay is this actually gonna work are these gonna fit together is this nail gonna stick through so it's we're trying to replicate that that internal monologue yeah so it shouldn't it shouldn't theoretically take any jobs from any developers anytime soon at least not any more than that GPT is already sure the point is is that you you have a self self reflection and and kind of a set of medical metacognitive functions that are evaluating what it's doing and it's thinking through what it's doing every step of the way iteratively all right cool all right so take it away I think you said you wanted to move over to personas or something else yeah so um all well you were asking about how these all um interconnect so I've got the Persona just on file pulled up here um the the the primary goal is for anybody who's just picking the software up that doesn't want to learn how to program um the the most that they're ever going to have to touch if they just use the pre-existing agents is this file and the main Loop um so this file is where we set down our prompt so you know the system prompt is just like the system prompt in um in open AI um and then these other prompts go in as user um so we're taking it kind of like the way that you send chat history into chat GP or gpt4 when you're using a a home bot um in in the same way we're we're using these these variables here to inject data from other agents into this bot um in a in a human readable format and then every bot we can we can set whether it's going to use uh GPT 3.5 or 3.4 uh the plan is to also be able to leverage local models if you're running Uber um or you know you we can we eventually plan to incorporate hugging face um and um you know maybe maybe at some point Lane chain we'll see um but but the each each agent gets its own parameters for sending to the for sending to the language model um and you can't minimize here but but and we can you know um each each agent has its prompt exposed without sure without any code and of course the main Loop is so simple um I'm going where is Salient so here's salient um so if you ignore the like the the beginning section here you can see that our entire Loop is five lines of code commentation comments and yeah the it's it's super simple for anybody to kind of go in um and build this uh you know this logic Loop um one issue right now is is that the um where are you uh so the Salient agent run kind of has agents embedded in it so you can't modify it but you can also just use those um agents outright so our main pie function this is actually this is actually the uh baby AGI that's um been rebuilt into our system um so you can you know you have a really simple starting point right um and then when you go into um when you go into the agents like that's getting a little bit more complex um but you should be able to also build agents without having to [Music] um without having to to dig into the rest of the code base um so all of the the loop of the agent could all be self-contained and most of these functions are already templated out for you so it's almost Plug and Play we're not there yet but we're getting there so let's uh let's talk about this architecture real quick because you've got you've got several layers of nested Loops you've got a modular and configurable architecture and like you said the goal is to get to Plug and Play So and you're almost there so talk us through a little bit about like how you came to some of these architectural decisions because for someone who's only been you know coding for three or four weeks you've kind of unpacked a lot of really important Concepts and development um and I know that you've had uh Ansel to help you out so maybe you can't take full credit and then chat GPT as well but so you know you guys you guys have uh I I see you like in chat like all day every day some days um so so talk us through like like how you came to some of these ideas and and what like what is what is the overall status of like how how close are you to having something that is it looks like a semi-autonomous or fully autonomous what are the missing steps and you know who do you want to get in touch with you and and so on and so forth where do you need help well you know Bill Gates wants to give me a call I will totally take his call now I only charge two thousand dollars uh an hour which you know is is pretty sure um the uh yeah the loop is there the the loop is already there um at least when you compare it to um Auto GPT which kind of has a functioning Loop too I haven't looked at their stuff since I I started this project and even even before I went like you know we had we had chaos GPT was looping at least so so it Loops um so now what we're doing is expanding functionality and um and uh for expanding functionality and we're expanding uh compatibility with these other systems we've got AP uh open AI working we've got Uber working which should work with any other language model um chroma is is done manually so it doesn't rely on Lang chain but if we incorporated Lane chain that would that would open up functionality in a much broader capacity right yeah I was just about to ask for extensibility for instance like you know can this right what code can it talk to apis yeah go for it Ansel now now that you mentioned extensibility that was the main reason for developing this because like I mentioned before I try to mess with auto GPT to get it to like do something I wanted just for testing purposes and I I wish it wasted my time trying to modify it because it's it's it's I don't want to say it like like the develop didn't pay attention to it but it's just the structure of it is is not user friendly at all so we when we went into this the the main goal was to make it extensible organized uh and very easy to know what's going on where um in their defense I don't think that they were writing Auto GPT for other people to come in and and mess with it and make it do what they wanted um you know we from the ground up we've been building this so that other people can come in behind us right and really what we need uh more than anything is for people to come in and and build new agents for example this execution agent right here um the the execution agent needs to be expanded in a much more broad fashion um the and then we need to work on the task generation right so um baby relied heavily on task generation to determine if it had completed a step um and and that ended up with it redoing a lot of work which we didn't like but um I think that that could be solved with um with with a um with a set in stone primary goals that and allowing it to generate sub goals so if somebody wants to take a crack at task generation we actually have a a kind of uh prototype outline although this is a you know we haven't actually done any work on this stage yet but those are those are the two the two threat things that we have going forward to improve its um capacity is building out execution agent um rebuilding this task generation and then um you know a few odds and ends here and there just to to make it run a little bit more smoothly um I find that the biggest gains are are usually from the uh from prompt engineering though and sometimes even small changes to the the text and the uh in the Persona wording is everything yeah yeah wording is everything um like let's see if I can find yeah so here here's the status agent and it just does not yeah it's too long but yeah we've been a good hour working on the prompt for status agent and um ultimately we we we still ended up having to switch over to gpt4 for this so yeah the running version that we have right now is using two separate language models oh cool it can handle multiple language models um so you've got you've got a lot of the extensibility and and configurability figured out so you mentioned earlier that you're getting close to making it plug and play and and like you said uh all the all the different agents are configurable just via Json um and and you you're getting close to adding some more extensible uh things so how do people collaborate with you do you just all go through GitHub um how to how should if anyone wants to jump in what's the best way for them to to get in touch with you guys so we have a we have a project set up in the community project section of the cognitive AI Discord um specifically for agents um but if there's other stuff that you think that that you can contribute to for example we'd love to have a graphical interface right on the UI it's similar to Lang flow um to to make this even lower code for people um and then you know of course the you know the uh if there's a um a new model that comes out or a new host somewhere and they have their own API that isn't open AI compatible um writing um writing API interfaces for that so for example the the current V2 version of um of the Uber Booga API was not actually written by us this was written by um that's so strange that he's not listed as a contributor here um I think his name was Max though it was Max something but he fixed it yeah oh uh mitchko in mitchko so he did get credit for this one but he wrote um he rewrote the the interface for the uba Booga API and people are are out there using this and it's working you can see the the old codes listed here um right because another thing you can do is also host the your local model on a Cloud Server and use the API to connect to it and you can just run the the interface on your phone or web browser or whatever interesting you can see the um the endpoint code is listed here as a variable and what we'll do later is come in and set this as configurable within the ini file um right oh yeah that's that's something else I I should just touch on briefly um things like determining um which language model you're using which database you want to use you're in embeddings um function that you're using all of that is um all of that is configurable here in the config.ini file as well so yeah we for we really try to get as much of the configuration as we could out of the code and into um into files that that people are gonna find useful wonderful so what's what's next I know you guys have outlined a lot of open problems and some things that you need help with and I think there's quite a few people that are expressing some interest um so where where do you guys want to take this project like what you know given a month or six months or if you have 20 people jump in What do you want it to be able to do well this weekend we are taking it to a hackathon and we don't know what the objective is yet but we're going our objective is to win the hackathon with this out of left field uh AGI uh framework that nobody has ever seen before but beyond that um beyond that I'd like to see this see this just used more broadly um you know it this is all um this is all in a what what uh be about there it is I missed but this is all in the gnu general public license just like Linux so anybody can take this right and use it for commercial purposes um with you know with restrictions um and you know me personally I would love to once we get this model pounded out and and set in stone I I would love to just build and bespoke AI cognitive systems for for companies on either a direct um you know direct to um Market kind of solution or even in a research capacity you know my you know I've already built some agents or at least one agent and then some made some modifications as well um to work on the heuristic imperatives project that you've got going Dave yeah um and I I really want to be this the I want this to be an engine for your project so that you can iterate quickly and you don't get hung up in in the details of you know working manually or writing your having to write your own code um for every single little thing that you do we can just have an agent and plug those agents in spit data at it and see what comes out there you go is basically like empowerment right like human language has given us the ability to do everything we've done up until this point and we've basically given that uh ability to machines now and with this kind of architecture we can give that to everybody as long as they have a model to run they could have some kind of agent working for them so it might sound science fiction or outlandish but it's basically like constructing your own Jarvis right that's the goal that's the goal yep yeah I do not want to be the person who develops the uh the the the next Raven though let me just say that like you know I'll be a part of it but the uh a autonomous assistant is going to be a lot of work oh definitely that's why I said the ultimate goal yeah just looking at what we've done already um a you know like a better uh Siri for example is a a mini person project not just two guys in a garage yeah yeah so this is like the very first you know 2022 there were very few of us even talking about cognitive architecture 2023 hits and suddenly like everyone's a cognitive architect but you guys are definitely like lunging ahead forward um which is just incredible to see um so thanks for sharing your work so far um and I know that you guys have have a lot of work that you're you're working on this this is uh it's getting close to being uh semi-autonomous it doesn't doesn't quite come up with its own objectives but it can autonomously work through a lot of uh increase it's coming it's coming and it but it can it can think through open-ended objectives that you give it which is really incredible so um any final thoughts from uh from you guys or anyone else um before we wrap up today's tonight's recording yeah well if you had told me uh in December that I would be diving into a project on GitHub about cognitive architecture I would have spat my beer at you so everything is changing really fast it's getting really silly it is impossible to keep up and it's exciting and it's just it's just thrilling to be working on this yeah absolutely yeah and possibly keep up is definitely definitely the the situation right now even even with the past week or so where AI news has been relatively slow there haven't been a whole lot of interviews here lately um I've still got dozens of tabs open of AI stuff that I just haven't been able to read yet um I I really think that um a lot of people are are getting a way more concerned about artificial intelligence than they need to be we still need to be concerned we need to be working on on these things but we're not we're not quite in runaway AI yet I hate to disagree with you Dave but um you know it's it it if we told AI to build better AI that would be like telling you to do brain surgery on yourself so we don't need to bomb the data centers no not but yeah we should definitely have bombs in the data centers just in case just in case okay and local backups for your your shows or anything you want to watch after the apocalypse so there you go just etch it out on clay tablets all right gang thanks everything thanks for everything um and definitely keep up the good work and uh yeah I hope that you get some more folks reaching out to you we will of course have ongoing updates maybe not weekly we'll see um but you know we're doing more live streams lately um just to keep everyone up to date because as you guys mentioned it's happening so fast um and it's good to show people what's possible so I'm gonna go ahead and stop the go ahead one one more thing I think that getting everybody involved in talking about this is really the the best way forward yeah um because you know up until now all of the discussion about Ai and ethics and morality of AI has been siled and gate kept by the you know a few people at these major corporations yeah and you know you and the communities that you've helped get get off the ground I think has really opened it up and is going to be one of the most important things for the future of AI it's definitely already having an impact well thanks so much and I'm glad that it's definitely having hopefully a positive impact in the long run yeah we'll see yep yeah all right gang I'm gonna knock off the recording have a good night everybody and we'll talk again soon cheers