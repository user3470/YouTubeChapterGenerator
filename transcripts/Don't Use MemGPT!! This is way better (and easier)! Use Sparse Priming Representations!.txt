all right so I know that M GPT and a few other things are all the rage right now and I've gotten a lot of comments of people wanting me to talk about it and I haven't talked about it and the reason is because I figured this out months ago and I posted about it and people seem to have forgotten so I am here to remind you that this is a much easier solution than mem GPT and it is much more powerful and you can use it right now and it is called sparse priming representations so you can see that I first uh recorded sparse priming representation 7 months ago you can come out and read the repo it's all free it's under Dave shap sparse priming representations the tldr and I'll go into a little bit more detail in examples in just a moment but the tldr is that language models work in a very similar way to human brains in this one particular way and that is that they are uh associative so semantic associations mean that all I need to do is tell you uh let's say um the Golden Age of Rome so three words golden age of Rome and what does that do do that conjures up millions of other ideas facts and images for your brain because those three words are associated with a lot of other stuff in your head this is what we call a mental model now large language models have lots of mental models as has been proven by the scientific literature they have theory of mind they have planning they have reasoning they have logic and yeah I know that there are some flaws with the logic but you know what humans have flawed logic too so that's an arbitrary standard anyways so because large language models have semantic associations just like human brains all you need to do is give it shorthand notes in order to remind it of stuff that it already knows and so with stuff like mgpt where you're constantly doing these really complex Loops to try and distill stuff and figure out what you need and blah blah blah like no that's so dumb what you're trying to do with memory with retrieval is you're trying to create an internal state in the language model that is good enough to be useful in the future and so the idea is well here let me just show you the um the uh CH the custom instruction that I got so I'll just read this to you real quick because this is kind of the depth of the theory so sparse priming representations there are only a handful of ways to teach large language models and all have limitations and strengths first is initial bulk training this is ludicrously expensive and not practical number two is fine tuning fine tuning is not necessarily useful for knowledge retrieval maybe that changes in the future as fine-tuning techniques advance but you don't use fine-tuning for knowledge retrieval people have by now it seems like they've accepted that because now the focus is on retrieval augmented generation rag online learning so online learning is the idea that maybe the language model will continuously learn based on what you feed it we'll see if that pans out as far as I know there's no commercially viable options for that yet and then finally in context learning in context learning is literally the only way to teach models anything that is outside of its training distribution everything else is just window dressing um so because of all this uh rag retrieval augmented generation is all the rage right now tools like vector databases and knowledge graphs are being used but you qu you quickly fill up the context window with dumb retrieval and so what I mean by dumb retrieval is you just try and match whatever is in the context to an arbitrary number of nodes in your knowledge graph or your vector database and then you just fill it all in now in the long run I suspect that we're going to have language models that are big enough that you can just say like here's 10, KB articles uh read them all and tell me what's relevant you know you look at LM infinite you look at uh Microsoft Longet eventually we're going to have that but there's still always going to be costs associated with those gigantic context windows so you're always going to need to figure out how to distill knowledge so one of the most common questions I get is Dave how do you overcome the context window limitations the sword answer is you don't stop wasting your time trying to do it it is a algorithmic limitation you have you know 20,000 tokens or 100,000 tokens tokens you have those tokens that's it stop trying to get around it um it's like trying to stuff 10 pounds of stuff in a 5 pound bag you know the saying it's just not going to work um now okay there is one giant asterisk here though most of the techniques out there do not make use of the best superpower that Lage language models have latent space no one else seems to understand that there is one huge way that llms work similar to human Minds associative learning here's the story I realized a long time ago that with just a few words you could Prime the llms to think in a certain way I did a bunch of experiments and I found that you can prime models to even understand complex novel ideas that were outside its training distribution for instance I taught the models some of my Concepts like heuristic imperatives the ace framework terminal race condition and a bunch of other stuff that I made up that is absolutely outside of the uh training data these sprs are the most token efficient way to convey complex Concepts to models for in context learning what you do is you compress huge blocks of information be IT company data Chat lock specific events or whatever into sprs and then you store the spr in the metadata of your uh Knowledge Graph node or in your vector store whatever and then the spr is what you inject at inference time not the human readable data so then I I created a uh a a system window that you can use to compress uh pretty much anything into an spr now the um let's see where is it so you can read the original Theory under sparse priming representations um I did add the system message here as well so you can just copy paste this into um into the chat GPT either the custom instructions or the system window in the API subm Mission you are a sparse priming representation writer and spr is a particular kind of use uh uh is a particular kind of use of language for advanced NLP nlu and nlg tasks so again this is itself a sparse priming representation I'm giving it a term and I'm labeling it I'm saying spr so now it knows what an spr is and I'm associating spr this novel term with NLP nlu and nlg so it understands okay cool we're we're very deliberately activating certain associations inside of GPT uh particularly useful for the latest generation of large language models again I'm giving it an association you will be given information by the user which you are to render as an spr Theory llms are are a kind of deep neural network GPT often gets llms wrong because for whatever reason open AI decided not to teach it about AI cuz you know maybe it'll escape the lab whatever um they have been demonstrated to embed knowledge abilities and Concepts ranging from reasoning to planning and even to theory of mind so I'm I'm giving it an une unequivocal assertion this is what an llm is I'm not telling it you're an llm because then it'll get all bent out of shape and say well as an AI language model but I'm like no this is what llms are and this is what llms are capable of these are called latent abilities and latent content collectively referred to as latent space the latent space of an llm can be activated with the correct series of words as inputs which will create a useful internal state of the neural network this is not unlike how the right shorthand cues can prime a human mind to think in a certain way like humans llms are associative again very very specific use of words by by giving it the word associative and you know human minds and all these other things it's now thinking in a certain way internally meaning you only need to use the correct associations to Prime another model to think in the same way uh methodology render the input as a distilled list of succinct statements so this is again uh very very specific verbs and adjectives rendered distilled succinct statements assertions associations Concepts analogies and metaphors the idea is to capture as much conceptually as possible but with as few words as possible write it in a way that makes sense to you as the future audience will be another language model not a human okay so I just gave a lot to you here's an example so I took that mission statement on gp4 temperature Z and I gave it about half of my Ace framework so you can see here's a tremendous amount of text and then what it spits out is just a list of statements and this list of statements if you read it you like okay cool you know Ace framework blueprint for Architects task prosecution layer so then what you do is you take this SP which took you know you you can get you can in this case it looks like you get about 20 to one um compression and so then you take the SP and you say okay cool let's clear all this out and then you say um um uh let's see the following is an spr sparse priming representation um and I what I'll do is I'll actually add a second uh system message by the time you see this so that you can actually have a have a system message to unpack sprs um unpack this into its full content and so you can see what it's doing is it's basic basically a form of semantic compression um so because because large language models are associative all I needed to do was give it enough details about something and now it can reconstruct the original idea this is far and way the best token efficient way of uh of conveying things now obviously this is not quite as long but you can also then ask questions um let's see how does uh it handle ethical uh decision making ethical decision- making is primarily handled by the aspirational layer uh this layer provides it blah blah blah now there are a few details um that are that are missing from this thing but again um when you get when you get a a a representation that is that efficient um you can play around with it and you can also have multiple sprs Associated uh with with various Concepts or nested in the metadata so anyways thanks for watching I hope you got a lot out of this uh yeah now stop asking me about mgpt come on this this is this is kid stuff sprs are the way to go bye