hey everybody David Shapiro here with a video so um one I've been scarce and I apologize I am feeling better um recovering from burnout although I still need like some days just doing nothing um but anyways um so y'all are really clamoring for me to continue the um the Q a chat but not that one um and then the salience and anticipating um you know and auto Muse and all that fun stuff so all these chat Bots um I will continue working on them but I kind of got to a stopping point where uh basically the problem is memory right so whether you're looking at hundreds of scientific articles or an arbitrarily long uh chat conversation or an entire novel um semantic search is just not good enough breaking it up and chunking and and stuff so we need a more sophisticated a more organized uh memory system for AI for autonomous AI and so this is what I proposed um and so basically there's there's there's uh episodic memory there's two primary kinds of memory in the human brain there's episodic memory which is chronologically linear so that is the lived experience the live narrative that is the a linear account of the sensations you know your external senses and your internal thoughts um those are the two primary things that you got Sensations thoughts and then in thoughts are um decisions uh memories that have been recalled so on and so forth but you forget most of this most of this is noise right you don't need to remember that you remembered something at all times you just have like oh I'm thinking about you know that time I went to the beach right and then you know anyways so you don't necessarily need to record all your thoughts but you definitely need to record uh to a certain extent what's coming in and then you you slot that into some kind of framework um so this is going to be the underpinning uh work and I have written in all three of my books so far that like I was putting off memory systems because it is a super non-trivial problem and it turns out it's now the problem that like we all have to solve so I'm working with um a few people uh on various cognitive architectures and we're actually going to have some demos coming up in the coming weeks um because fortunately I'm no longer the only person working on cognitive architectures yay um the idea is catching on um so with that being said though um the this is this is a very difficult problem and so the idea is Okay so we've got raw data coming in right it's it's unstructured the only well it's it's semi-structured the only structure is you know what time series it has but other other than that you don't know what um what the topic is going to be and the topics are going to change right and there might be gaps in the time so what we do is we take a chunk of logs an arbitrary chunk of logs based on that are temporally bounded and you get an executive summary of that information and in this chunk so this is like going to be another Json file or whatever you have pointers back to the original log so that you can reconstruct the memory because using sparse pointers is actually a big thing that human brains do um and so then this is basically a a very sparse summary and I'll show you what I mean by sparse summary and then finally as you accumulate more of these summaries you eventually merge these into a knowledge graph or a cluster them and then use that clustering to make uh to make like Wiki articles or KB articles and give me just a second sorry I needed my coffee okay so anyways um yeah so this is the scheme and I spent a long time talking through this with chat gpt4 so you can see this is a whoops this is a come on Why is the why is the okay it doesn't want to scroll anyways you can see it is a very very long conversation I talked through code I talk through the math I talked through the concept and so anyways at the very end of it I said can you write an executive summary of the problem we're trying to solve here and so this is just taking a step back for a second I am using gpt4 to help solve the problems of AGI artificial general intelligence or what I call autonomous cognitive entities so the problem at hand involves designing an efficient memory system for an autonomous cognitive entity or an ace that can manage a large and constantly growing Corpus of text Data generated through thoughts inputs and outputs this data can accumulate to hundreds of gigabytes per year potentially reaching millions or billions of individual logs the primary challenge is to organize and compress these logs into a manageable set of knowledge-based Articles while retaining as much meaningful information as possible this is such a concise summary I could not have done better our proposed hour see it's it's already the the collective because where it understands that we're collaborating our proposed solution involves a multi-level approach with the first level being the consolidation of raw logs into roll-up summaries so that's this um these Roll-Ups serve as compressed representations of the original logs reducing the total number of Records then we employ a gating or threshold function to determine whether a roll-up is semantically similar enough to an existing KB articles or if it if it should be added as a new article this approach allows the KB to adapt organically to the evolving data while maintaining scalability the key aspects to consider in this solution are the choice of similarity threshold and semantic similarity measure as well as the balance between number of KB articles and their quality periodic evaluation and fine-tuning of the system will help ensure its continued Effectiveness as data grows okay so this is a very very condensed text summary of this system and then so I mentioned sparsity right so I've been reading this book behave so as always neuroscience and life inspires what I'm working on and one of the one of the experiments or actually several the experiments that he talks about in this book has to do with linguistic priming and so an example of linguistic priming in humans in Psychology is that if you use just a few words um kind of placed arbitrarily it will really change someone's cognition so one example was they did a test with Asian women and if you remind the Asian women of The Stereotype that Asians are better at math before giving them a math test they do better if you remind them of The Stereotype that uh that women are bad at math than they do worse and then of course if you just give them neutral priming they kind of you know perform in the middle and there's plenty of examples of priming um Darren Darren Brown the the British dude The Mentalist he used a lot of priming to get people to like do all kinds of cool stuff this was back in the 90s um but like one one experiment that he did was he had a bunch of like marketing guys and he put them in a car and drove them around town and he drove them by like a specific set of billboards and so they were primed with images and words and then he asked them to solve a particular marketing problem and he had almost exactly predicted what they were going to produce based on how they had been primed now I noticed that large language models can also be primed and so what I mean by primed is that by just sprinkling in a few of the correct words and terms it will then be able to reproduce or reconstruct whatever it is that you're talking about so what I want to do is I want to show you that because this this really high density way of compressing things is what I call sparse priming representations is going to be super important for managing uh artificial cognitive entities or AGI memories because here's the thing large language models already have a tremendous amount of foundational knowledge so all you need to do is prime it with just a few rules and statements and assertions that will allow it to um just basically kind of remember or reconstruct the concept so what I'm going to do is I'm going to take this and put it into a new chat and we're going to go to gpt4 and I'll say the following is a sparse priming representation of a concept or topic um oh wow they they reduced it from 100 messages to 50. I guess they're busy uh unsurprising um please reconstruct the topic or Concept in detail and so here's what we'll do so with just a handful of statements and assertions I will show you that gpt4 in the form of chat gpt4 is highly capable of reconstituting this very complex topic just by virtue of the fact that it um it already has a tremendous amount of background knowledge and processing capability um okay so there we go so the autonomous uh cognitive entity is an advanced artificial intelligence system to design it yep okay there you go um so it's kind of it's It's reconstructing what this multi-level approach so what it's doing here is it's kind of re restating uh everything um but what you'll see is that it will be able to confabulate and kind of fill in the blanks and so by having a sparse representation it kind of guides how it's going to confabulate and this can be used for all kinds of tasks right so some of my patreon supporters I'm not going to give anything away because I respect my patreon supporters privacy but they ask me like how do I represent X Y or Z and what I'm going to say is this is a way to represent a lot of stuff um what whatever whatever your domain of expertise is you can ask it to do what I did in there which is say just give me a short list of you know statements assertions explanations such that a subject matter expert could re um could uh reconstitute it um there we go and so here here it's it's figuring this out as it goes periodic evaluation and necessary to continued efficiency this may be involve adjusting the similarity threshold refining semantic similarity measure modifying other aspects sparse priming representation is a technique using conjunction to fill acetate knowledge transfer and reconstruction spr concise statements are generated to summarize yeah so it even understands just by virtue of saying this is an spr and a brief definition it understands the implications um there you go so now that it has has um has reconstituted it we can say Okay um great thanks um can you discuss how we could uh go about implementing this for a chat bot and so again because um because this uh because gpt4 already knows a whole bunch of coding and data and stuff it's going to be able to talk through the process so this is going to okay I don't think it fully I gave it very simple instructions let's see where it goes because often what happens is and someone someone pointed this out to me is that it'll kind of talk through the problem and then give you the answer so I learned the hard way just be patient what it's basically doing is it's talking itself through um the the problem in the solution so anyways excuse me I don't know why I'm so hoarse um but yeah so this is this is what I'm working on right now and this is going to have implications for for all all chat Bots but also all autonomous AI because again um you know this is this is like the first two minutes of conversation but what happens when you have a million logs what happens when you have a billion logs so one thing that I suspect will happen is um the number of whoops nah come back no um I suspect that the number of logs will go up geometrically but what I also suspect is that the um is that the number of KB articles will actually go up and approach an asymptote how do you get it to stop there you go so I think I think that this is kind of how it'll look where like when you're when your Ace is new when it's young it'll be creating a bunch of new KB articles uh very quickly but then over time the number of KB articles will taper off because say for instance there's only a finite amount of information to learn about you and then there will be a very slow trickle as your life progresses right and we can also exclude KB articles about basic World Knowledge right all it needs all your Ace needs is KB articles about truly new novel and unique information it doesn't need to record a world model the world model is baked into gpt4 and future models now one other thing was because this is kind of incrementally adding the KB articles um let's see what it came up with okay so talk through the problem um one thing is that I asked it for the pros and cons so right here uh using a gating or and this is this is how sophisticated it is um using a gating or threshold function to compare Roll-Ups against existing KBS can be a viable alternative to clustering so basically what we were exploring was what if we use a clustering algorithm to to um figure out the chunks but then I was like okay but we're not gonna We're Not Gonna regenerate the uh the KB articles every single time because that's going to be prohibitively expensive so what if we treat it more incrementally um let's see this approach involves comparing semantic similarity between a new roll-up and existing KB articles if it doesn't meet a predetermined threshold okay so the pros it's simple this approach is conceptually simple and can be easier to implement compared to clustering algorithms yes scalability as new Roll-Ups are processed individually the computational complexity of updating KB articles grows linearly with the number of Roll-Ups making it more scalable Dynamic growth the number of KB articles can grow organically with the addition of new rollups and then the cons it very rightly identifies sub-optimal organization because we're not using Global clustering that's fine redundancy there's a risk of creating similar KB articles um depending on the semantic similarity and then parameter sensitivity so on and so forth now that being said there is a final step that we were that I was going to talk about which is every now and then we should do a re-indexing event and so basically what that says is when you're when your Ace is offline during the dream sequence right so real-time learning it can update the KB articles in real time but then the dream sequence it will delete all the KB articles cluster the chunks based on semantic similarity and then based on those chunks write a whole new set of KB articles and so every now and then your autonomous cognitive entity is going to update its entire internal Wiki and then these internal wikis are going to be the primary source of information for your uh for your for your cognitive entity and so instead of searching millions of logs you're going to be searching hundreds or maybe a couple thousand KB articles which is a much more tractable problem um to find the correct thing and also they can be cross-linked to each other right because these KB articles these wikis um can be nodes and a knowledge graph which means it's like so my fiance was like okay so I was explaining it to her and she's like so what if it has what if it has a um an article on me and an article on her would it link the two of us and say that like we're engaged and you know our relationship has been ex long and I'm like yes we could probably do that it might also topically um so in terms of the kinds of topics here's another important thing in terms of kinds of topics we're probably going to have have it focus on people events um things like objects um as well as Concepts so a concept could be like the concept of the autonomous cognitive entity so people events things and Concepts and included in things are like places right so like the year 1080 the the place Paris France right so those are all viable nodes for a Knowledge Graph so that's that's kind of where we're at um yeah I think that's all I'm going to do today because like this is a lot and you can see that this conversation was very long um and uh but yeah so let me know what you think in the comments we are continuing to work um I had a few other things that I was going to say but I forgot them this is the most important thing and this is this is the hardest problem I'm working on and once I unlock this it's going to unlock a lot more work because think about think about breaking what if these logs instead of like our conversation what if these logs are scientific papers or what if these logs are scenes in a book right pretty much everything can be represented this way I think and then once you have these higher order abstractions and all of them point back so here's another really important thing that I forgot to mention is that there's metadata attached with each of these entities that points back to the original so you can you can still reconstruct the original information so if you have like you know a topical article here it'll point to all the chunks that were in that cluster that um that helped create it and then each of those chunks will point back to the original logs so you have kind of a pyramid shape um yeah so that's what I'm working on uh that's it I'll call it a day thanks for watching