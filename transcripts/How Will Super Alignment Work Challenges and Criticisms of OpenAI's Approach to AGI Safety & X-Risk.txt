hello everybody David Shapiro here with another video so today's video is about super alignment uh for those that you that might not know openai recently announced that they are creating a super alignment team and they are going to commit 20 of their compute resources to the task of solving super alignment So today we're going to talk about how it would work or more specifically the challenges with super alignment and also some of my uh let's say criticism my feedback for openai based on what I know about how they have approached alignment so far and what they have said about super alignment uh before we get into the show all of my work is completely open source and free of ads and that is because I am supported by a Grassroots movement now in order to keep doing this your support would be greatly appreciated and all tiers on my patreon get you access to the private Discord server so without further Ado moving on uh first the question is what is super alignment uh so I got this summary just I took open ai's statement on super alignment and and got this nice little summary uh super alignment is the process of ensuring that super intelligent AI systems which are systems much smarter than humans follow human intent so they keep using this word intent which I have some feedback on it involves developing new scientific and Technical breakthroughs that can effectively guide and control these highly Advanced systems this is making the Assumption of Courage ability which we'll talk about later the goal is to prevent potentially catastrophic scenarios such as super intelligent going rogue or becoming uncontrollable super super alignment is a critical challenge in the field of AI safety and is considered one of the most important unsolved technical problems of our time so again this is paraphrase paraphrasing open AI super alignment is not about ethics and disinformation super alignment is fundamentally about X risk or what we used to call existential risk but what people have simplified to just call Extinction risk it's not about jobs displacement it's not about preserving the economy as it is it's not even about ethics and privacy or social Credit Systems it's not about democracy it's not about uh manipulation campaigns or making money or even regulation it is about preventing extinction level events all right so to help you understand super alignment uh I found a couple memes so these are from the AI safety memes uh Twitter which is hilarious and I definitely recommend you follow him uh or then whoever the probably a human hopefully a human anyways uh so this is the show goth Meme and the idea is that uh when you when you train a gigantic model and of course these models are now pushing multiple trillions of parameters and they're trained on trillions of tokens uh you you don't know what is in the model you don't know what it learns you don't know how it thinks and it is entirely too big to uh to be remotely interpretable all you can do is train the model and then test it based on input and output and you can try and trick it you can try and find failure conditions uh basically this is the Mesa optimization problem where it's like okay you don't really know what's going on inside inside the Black Box uh and so unsupervised learning uh Foundation models uh they are scary because they will just start spewing out all kinds of stuff all the stuff that you saw on uh on the Bing chat Sydney um that was because they were they they you got a little bit more raw raw output from the model and of course that was uh if you go watch the Y files that just came out um he had a really great dramatization of some of the conversations that people had with Sydney or being Ai and so that gives you a kind of a closer peek under the hood as to what's going on they have since fixed it with some supervised fine tuning and then of course there's our rlhf which makes it uh behave very well but the thing is is every now and then you'll get a peek behind you know what's actually going on and what it's actually capable of doing and you will realize that you are communicating with a non-human intelligence and it's pretty scary when that happens uh this meme was great because it really kind of shows the context of what actual super intelligence is and I love the simplification of this of the show goth meme um you know but basically the smartest humans that have ever existed uh are several orders of magnitude lower capability than super intelligence and since we're starting to see the first Sparks of super intelligence hopefully people will start to believe that super intelligence is actually a thing we still have some deniers out there which I'll cover in just a second okay so General challenges why is super alignment hard first and foremost is the normalcy bias so human brains we evolved on the savannas of Africa and then we spread across the world and so our brains just do not comprehend exponential growth it is not something that is in our evolutionary distribution and so uh Gary Marcus uh an AI safety researcher is fond of of pointing out that llms really often fail to generalize outside of their training distribution humans are no different and uh in our evolutionary training disposition uh uh distribution we never experience anything truly exponential and the things that we do experience that are exponential like the uh light and sound because those are on uh I think logarithmic scales um your brain handles for you so you still perceive it as geometric even though your brain automatically Tunes audio and and light levels uh so that you just experience it within a much narrower range so that's one part of normalcy bias which is just we are evolutionarily not equipped to comprehend exponential growth and exponential change uh beyond that it is very difficult to understand super intelligence even when you look at the trends because all you see is a trend on a graph like okay uh you know parameter goes up and to the right oh okay great you know uh token window goes up and to the right training data goes up and to the right we don't really have a visceral intuitive emotional understanding of what that means because again normalcy bias and this is I'm not saying that like if you have normalcy bias you're dumb this is literally just a fundamental limitation of human brains uh and even those of us who study this stuff and know that it's coming we cannot predict exactly what it's going to imply or what it's going to feel like once it actually happens because again we are anchored in the present moment the present time because evolutionarily speaking That's What mattered most if you're hungry right now go find food if there's a tiger right now go you know get away from it or hit it with a stick and then once you're safe you're safe again so our time Horizon that our brain thinks about is relatively small and this these are all components that feed into normalcy bias so this normalcy bias creates a lot of problems for for uh many many reasons one for a lot of people they're just not even really like willing or able to engage with the conversation of super alignment because of normalcy bias this is why you see so much skepticism out there uh and and and even for like I said even for those of us that are engaged even though we know what's coming just our cognitive limitations make it really difficult to accurately forecast and predict the impact of some of these things and we have to trust the numbers and even then we can only think so far into the future especially with things changing as fast as they are so here's a thought experiment that I came up with to help you understand super intelligence think of a pigeon they're very common they uh basically exist in every major city in the world they're mildly intelligent creatures they can learn a few things they can solve some basic problems and they can remember uh simple facts like you know where where to go get food they can even learn to recognize certain humans like if you go to the park and feed the pigeons every day the pigeons will learn to recognize you but other than that they're pretty simple creatures now when you compare the cognitive capacity of a pigeon to even the dumbest humans uh human pigeons are cognitively deficient you can't even compete on the same playing field because humans are in a fundamentally different class of cognitive ability compared to Super intelligence you are dumber than the pigeon is to you know a typical person and then not to mention the fact that uh it's entirely possible that that super intelligence or AGI or whatever is going to possess orders of magnitude more cognitive abilities and I don't just mean speed I don't just mean the ability to read uh you know text at a human level you know a million times faster which it's already getting close to doing that uh what I mean is that it will possess cognitive abilities the ability to make connections to solve problems and to understand things in a way that humans might not be able to ever compete with we have the illusion that we can understand everything because you're looking at your own mind from inside the Fishbowl this is a commonly discussed problem in epistem epistemology and philosophy but the thing is is you can imagine the mind of a pigeon by virtue of the fact that the pigeon's mind is much simpler and dumber than yours and you can you know look at it uh and and make inferences but the pigeon lacks the ability to even remotely comprehend your mind because its mind is so much more limited that is the difference between humans and super intelligence and so basically remember that you are a pigeon in comparison and that will help you keep in mind what super intelligence actually is and when I say actually is like it is coming and it is coming fast another thing is AI dysphoria so this is a this is a term that that I coined because I have noticed in the comments and read it and Twitter and all other kinds of places there's a there's a few fundamental kinds of reactions and most of these are emotional reactions uh or or social cultural reactions to AI uh so basically one is denialism so this is people that just reject AI um like there's even people in the comments that say AI does not exist and will never exist and I'm like okay but that's like observably patently false so there are people that are clinging to this denialism because the fear or discomfort of acknowledging the existence of something is too much it's too overwhelming and so they just say I'm gonna pretend like it doesn't exist and we saw this with the pandemic remember there was plenty of people just saying that like the pandemic isn't real stop trying to control me and these are there were plenty of people who denied the existence of the pandemic even on their deathbed they still got themselves into mental uh gymnastics to say no it's just emphysema or it's just what did they call it um uh pneumonia they they called it you know oh I just have bad pneumonia and then they would die and it's like you literally died of of the pandemic but the concept of the pandemic was too terrifying that they could never emotionally reconcile the reality that they were literally dying of it with its existence with the fact of its existence and so I suspect we're going to see the same thing with artificial intelligence where some people are just going to be locked in a state of denial basically forever another one is plain ignorance so this is not technically dysphoria but it needed to be on the list where some people just don't get it like if you do not understand how it works you don't understand what it's capable of you're just not exposed to it you're not uh you're not educated enough or maybe in some cases people are just not intelligent enough to get it plain and simple ignorance is another reason that a lot of people are not going to engage with AI at the level of discussion that it needs to happen number three is magical thinking so these are the kinds of people that immediately assume and and very desperately want to see a soul in the machine the most famous example is uh Blake Lemoine uh at Google who basically there was a really great Reddit meme when he got fired from Google where he you know the the chat log was basically like you know tell me that you have a soul and then then the language model says yes I have a soul and see and the guy's like oh holy like there are there are so many people out there that want to imagine that we already have super intelligence that the the the machine is already sentient that it already deserves rights and I'm like it's it's still just a math model that's telling you what it's programmed to think that it wants having been working with these large language models since gpt2 I will tell you that understanding that the that the underlying language model is just predicting the next token right it's they spew out absolute gibberish like seriously go use gpt2 or the original gpt3 and any any illusion that you have that there's a soul in there or that it has extraordinary powers or that it's literally anything other than an autocomplete engine will be dispelled so that goes back to that shogoth thing right the the absolute gibberish that Foundation models spew out once before they're trained will this will dispel any myth of uh disabuse you of any uh illusion that there's something else going on other than just autocomplete um it's the it's that it's the rlhf that makes it uh appear more human-like and that's peridolia uh probably saying that right I got criticized last time I tried to say peridolia um but basically we are programmed to uh perceive human-like things into anthropomorphize things number four is doomerism so doomerism as I've unpacked in some of my other videos is often rooted in uh intergenerational trauma failed parents uh uh you know a nihilistic outlook for whatever reason and so basically what happens is that a lot of people take their intrinsic dread their intrinsic fear their intrinsic self-loathing whatever it is based on their experience and oftentimes it's uh it's completely unconscious I'm not saying that someone's like ah you know I hate my life and so therefore I want to see the world burn and no it's completely unconscious it's basically just that they have a negative outlook because of their life experience and then they project that onto artificial intelligence and it's basically a manifestation of a Death Wish um that's not the only reason for doomerism some people who are very intelligent and oriented uh towards this stuff they still rationally come to the conclusion uh that uh that AI is incredibly dangerous uh and I acknowledge that I acknowledge that if we do this wrong AI is is incredibly dangerous and it could cause an extinction level event but what a Doomer is the difference is that this is someone who seems to want to believe that AI will kill us all and to me that just looks like okay there's an opportunity to fulfill a death wish sorry uh and then the opposite of that is utopianism which is the idea that AI is going to intrinsically solve all of our problems but as as you might have seen in some of my other videos uh technology is is always a double-edged sword and it's a it's a dual use technology and more often than not technology actually makes some things much much worse before it gets better so it is not intrinsically a Force for good it is a dangerous force it is an energetic Force which must be used responsibly another challenge is the geopolitical arms race that is already starting uh the the the the one of the opening one of those strongest opening moves was when the United States cut off the sub the the flow of AI chips to China another thing that's less well known is that we also uh basically recalled all of our AI engineers and all of our chip Fab Engineers it basically said like you need a special permit if you're gonna keep working in China otherwise you're being recalled home uh so that's basically saying hey we're gonna we're gonna we're gonna force uh brain drain on China by Taking Back all of our best engineers and scientists at the same time uh people are putting AI into drones we've seen this in uh the Russia Ukraine conflict where there are more and more autonomous drones being deployed meanwhile China Russia and America and everyone else is putting more and more AI into jet fighters and every and literally every other weapon uh so on top of the military incentives that there are to create create more sophisticated weapons there is the geopolitical incentive to maintain a level of influence on the geopolitical world stage whether that's being uh militarily competitive or economically competitive or whatever and one thing I want to caution here is that the geopolitical arms race is in no way open ai's responsibility or any other individual corporations because even if openai and Google and Microsoft and all of them literally just flat out refuse to serve the Pentagon or the Department of Defense guess what the the the United States military and every other military they have their own budget and they can hire their own experts and they can they can still make it happen um and so I want to say like yes I will be criticizing open ai's approach but in this particular case I want to say that this is way outside of the scope of open AI but this also underscores the fact that we absolutely 100 percent not not just need Federal level regulation and research uh I'm not we also need International and Global regulation and research because some of these things are so far outside of the scope of just deploying models and Commercial tools and then uh finally is open source so there are uh more than a few commenters out there like um uh Dr Roman uh Chowdhury I think I hope I'm saying her name right and Gary Marcus and quite a few others um who are not Eliezer utkowski but there are plenty of people basically saying uh the same thing that the the polls that I ran on my YouTube channel say which is that a lot of people anticipate that open source models are going to overtake and eventually replace closed Source models so the thing is is once it's open source you can't really put that Genie back in the bottle and a lot of people already say the cat is out of the bag the horse has left the barn and is down the street and so in this case you have a competitive landscape where it doesn't matter what open AI research does it doesn't matter what Google deepmind research does it doesn't matter what regulations anyone passes and this is one of the nightmare scenarios that people point out that regulation no matter what you do will not be enough that research no matter what you do will not be enough and so basically we're going to end up in a situation where you have to fight fire with fire you have to fight misaligned models misaligned AI with aligned AI but then that that if you're relying on AI to fight your Wars for you what if it switches sides so these are some major major major major challenges with open Ai and one thing that I'll say before we get into the criticism is the fact that open AI is talking about red teaming and deliberately creating misaligned AI models in order to test super alignment that is absolutely Far and Away the best thing about what they are planning on doing now with that said I do have some criticism of open ai's approach so first open AI is somewhat preoccupied with human intention and human values you've probably seen this in chat GPT whenever you talk about Ai and safety where it's like you know we need to make sure it stays aligned with human values this was very clearly shoehorned in by their own internal alignment process which to be fair it's a good start you know basically saying let's align AI to human values that's a good start for aligning as a universal principle to adhere to but uh there's very much a Walled Garden effect going on here or an ivory Tower effect and what I mean by that is that this is this is a particular and a well-documented trend in Silicon Valley and it's not just open AI that does this it's literally every tech company on the west coast of America uh where they kind of believe that they are the smartest people in the world and that they are the only people in the world capable of solving this problem but the thing is is that egotistical belief prevents them from looking out the window and and getting the help of other experts and so I have a really great example from my last corporate job I was talking to a seasoned software architect someone that you would assume had a masterful command of the full Tech stack that goes into producing good software and so at one point he said we're gonna do we're going to automate literally everything you infrastructure guys aren't going to need to touch jack after this and I said okay does that include authentication firewalls backup power Does it include all this other stuff and he just kind of like you could see the 404 not found in his eyes he literally had no idea how much actually goes into the full Tech stack to make software work when he said everything his definition of quote everything was just the software just the code he didn't know anything about containers he didn't know anything about data centers he didn't know anything about cyber security and so my point here is and I'm not saying that open AI is this bad but they're still human and when you look at who's on the payroll of open AI they haven't hired a lot of public policy people they haven't hired a lot of philosophers in ethicists they haven't hired civil rights people uh and so when when they come up with these somewhat contrived ideas about aligning to human intention and aligning to human values all you have to do is is have a five minute conversation with a philosopher to realize that those are really garbage things to align to and so again you know a for initial effort but they really really need to look out the window and bring in more experts so here are some solutions one open AI really really really needs to add human rights as a core discipline in their research of not just alignment but also super alignment and the reason is because human rights is one well-established and well-researched and and two it is uh there's plenty of people that are going to be able to talk about how protecting human rights is really the ultimate goal of super alignment it's not aligning to what humans want or what humans say they want because any psychologist again another five-minute conversation with any psychologist will tell you yeah humans are absolutely unable to express what they truly want and truly need but human rights however the objective rights to create the safe environment that we all want to live in that is a conversation that you can actually have and that is while research from the perspective of Sociology psychology philosophy ethics public policy Game Theory so yeah also so anthropic also already figured this out they're getting closer I do have some issues with anthropics constitutional AI but it's moving in the right direction and the difference is that anthropic is listing out in those clear objective terms the values the The Guiding principles that they want their AI to align to so in this respect anthropic gets an a in in super alignment they're already moving in the right direction and open AI I believe is still moving in the wrong direction at least with the exception of of some of the the the tactics that they outlined in their their paper and again I want to reiterate the fact that open AI is going to deliberately create misaligned AI to see how it behaves and to see if they can detect it that is absolutely 100 A Plus at least on that section of the quiz but oecd EU the UN the White House all of these other agencies that have a lot of researchers and a lot of advisors including uh machine learning and AI advisors all talk about protecting human rights so why is it that a that open AI has not talked about protecting human rights in their AI alignment research that is very concerning to me and we'll come back to that at the end of the video another the other major criticism that I have for open AI is that they're continuing to ignore autonomous agents what they in their in their description they have explicitly stated that they never want to lose control of of the machine they believe that they will remain in control they believe that they can uh remain in control and this is a very dangerous assumption to make if you listen to Conor Leahy and Ellie azer the yukowski and literally dozens of other uh people out there uh they uh Robert Miles lots and lots of people say that this is a far harder problem to solve and in my opinion it is actually not possible to solve that so this is called the control problem or the courage ability problem which is basically can you correct the Ai No matter how smart it becomes or autonomous the thing is is if and there seems to be some consensus amongst people that yes AI can get to the point where you cannot control it so instead what we should do is is seek to shape it set it on a trajectory so that you don't need to control it now this is where I say that the fact that they're going to be you know creating red teaming AIS and internally red teaming tests and sandboxes and that sort of stuff I think open AI might ultimately come to this realization on their own I wish they would be thinking about this up front I wish that they would be if they had just mentioned autonomous agents the fact that they want to test it and to see if they can just just for the sake of argument I really wish that openai would say we're going to see if we can make intrinsically stable and trustworthy autonomous agents no matter how intelligent and independent they become the fact that they're not willing to test that that they're not even willing to say it is really alarming to me because I think that they should be pursuing literally every Avenue that they can so here's the solution one just go ahead and maybe throw out human intention as something to align to because human intention is garbage uh and maybe like I just said pivot the research goal to creating models and agents that are intrinsically trustworthy uh stable and benevolent um go ahead and continue with the red teaming that's good you know a plus there um but do more research into those Universal principles those guiding principles and try and create autonomous agents that will very deliberately preserve and promote those principles and adhere to them for all times aka the heuristic imperatives research that I've been doing oh and by the way I wrote a book about this and demonstrated all this and now I'm Not The Only One Look up the self-aligned paper by Sun at all where basically yes you can create models that will not only adhere to uh to higher principles but they will get better at those principles over time and here's the thing in the testing that I did with Foundation models I took Foundation models from unaligned to aligned with my core objective functions my heuristic imperatives that's relatively easy but the thing is is that the decisions they then start to make they will double down on those principles on protecting those values which is exactly what you want to in in terms of Game Theory you want them the AI to adopt a strategy and not deviate from that strategy that is the essence of the control problem that is the Core Essence of super alignment and this is what I've been working on for the last four years so for a quick recap open AI one major problem they're Reinventing the wheel in a few places namely with uh by by you know inventing alignment on human values and humans human intent intentions just look at United Nations look at anthropic even just look at what's trending on GitHub uh you know aligning to Human Rights is going to be a lot better and aligning to Universal principles is going to be a lot better than aligning to something as squishy as human values and human intentions again those are when you when you study the philosophy the morality the ethics the information Theory the psychology of it those are absolutely 100 garbage things to align to number two open AI is failing to understand those basic fields of morality philosophy and ethics human rights are incredibly well researched uh don't reinvent the wheel and the fact that human rights have not even entered their lexicon is really really deeply disturbing in it I don't particularly I don't personally read it this way but I could imagine someone very cynical saying maybe open AI doesn't actually value human rights maybe they don't care about human rights maybe they don't believe in human rights the fact that they're talking about safety of the human race and not talking about human rights when you look at the note that's missing that is deeply alarming and so then finally they are still making a lot of assumptions about courage ability which is why I think that they're not talking about autonomous agents even though the fact that lots and lots of people are going as fast as they can to make autonomous agents and then in the grand scheme of things when you uh think about the competitive landscape that's going to exist the autonomous agents that are trustworthy are going to trounce the autonomous or the the non-autonomous agents that are waiting for human instruction so what we really need is we need to be working on creating autonomous agents that will Advocate on our behalf and that are going to be the strongest and best and fastest in the world because that is how we that is uh one component of solving the control problem of solving alignment is the competition between these agents so with all that being said I hope you got a lot out of this thanks for watching cheers