morning everybody David Shapiro here with another video by popular demand we are going to explore the reasons five reasons that gpt4 will probably be disappointing so where did all this start Sam Altman uh the CEO and co-founder of openai was recently on an interview and straight from his own mouth he said uh you know the gpt4 rumor mill is a ridiculous thing I don't know where it all comes from it's obviously pretty silly uh people are begging to be disappointed and they will be the hype is just like we don't have an actual AGI and that's sort of what's expected of us so this was kind of like the shot heard around the world so what's he referring to First there's this meme that has been circulating on Twitter for probably at least a year in various formats basically the claim is that gpt4 will be 100 trillion parameters and there's all kinds of jokes like people have just taken it and run with it one of my favorite ones was someone said something to the effect of like my wife's boyfriend's boss has seen gpt4 and it blah blah you know so on and so forth uh still you know straight from straight from the man himself Sam Altman he said that it will be disappointing or people are begging to be disappointed and that he doesn't have and that and that they don't have AGI so let's let's unpack this what's going on where you know okay why why would he use that word disappointing so first it seems like the 100 trillion parameters is just that's not happening um I even had someone point out that uh chinchilla uh a competing large language model outperforms gpt3 on some benchmarks but it's only 70 billion parameters so it's entirely possible that gpt4 could be smaller it could be it could be data optimized and compute optimized so it might be smaller who knows um so the parameter count we have no idea but that's only that's only one component of what makes a large language model good or useful so his comment about AGI was was actually to me more interesting um rather than commenting on memes or or param or implying you know something about parameter counts so let's go through five reasons that I personally think that gpt4 might be disappointing to me and probably to you uh hence why why are you watching this so reason number one is that uh development is slowing down gpt1 first came out in June 2018. so it's been less than four years it's been about three and a half years since this whole thing got started which in the grand scheme of things is a very short period of time gpt2 came out nine months later so in the span of nine months they went up one entire generation uh gpt3 came out 16 months after that so roughly doubled right nine months 16 months and then so that was June of 2020 and here we are where GPT 3.5 and chat gbt came out at the end of last year which was a total of 30 months of development time and that's not even to get to gpt4 that's just GPT 3.5 so that was a very small in the grand scheme of things a very small incremental change uh or Improvement rather with no structural or or new unique capabilities now that being said those incremental improvements enabled a technology like chat GPT they got better at fine-tuning the window size got bigger uh and so that that makes it more useful but it's not fundamentally any different so with this slowing down of development time this leads to the possibility that maybe AI is not growing exponentially maybe it follows an S curve or a sigmoid curve which if that is true then we are at a point of diminishing returns and the whole thing is slowing down which by the way all of science is slowing down uh you know I have quite a few science friends and they will say like yeah you know we publish papers today that are tiny tiny improvements over uh past papers um science as a whole is slowing down uh and it's entirely possible that AI is also slowing down so the takeaway from this is what if the advancement of AI is more like in the Star Wars Universe where droids are smart but they're never going to be human level smart even after thousands of years of development if we have a sigmoid growth curve of the advancement of machine intelligence we could be looking at that future it's entirely possible now this is extrapolating a really big thing from just a couple data points so obviously take this with a big giant grain of salt reason number two the gpt4 might be disappointing is that it's still only text when you look at the the the the proportions that we take in you and me humans and other animals and even robots right that take in information about the world the vast majority of our information is Visual and then hearing and then touch and taste and smell right so we have all kinds of Senses which GPT doesn't even really touch GPT only has a hypothetical understanding of these senses uh and so we have you know whisper and Dolly and you know they announce that video is coming uh but there is so much information about the world that is not being integrated by these models and so we're basically just getting more of the same that's kind of all there is to say about it and I know that way back in my first book natural language cognitive architecture I did say that uh that natural language was sufficient for AGI but the implication is that you also have other models to help integrate uh real world information such as vision and audio into that because the the natural language cognition a new term that I just invented NLC is able to perform cognitive operations on that information but you still need to get that information in to the model from the outside world that being said we do still need better models we do need better Vision models and Audio models both for in inferencing the world but also generating um you know we're getting pretty good with uh voice synthesis but there's still a long ways to go anyways I'm I'm chasing down rabbits so another way of looking at this it's not just a matter of Senses it's a matter of also just humans are the most intelligent thing that we have so if you just look at how we process information in our brain language is only one part of that right and it's not even you know it's not even on this uh this this pie chart that being said language is an important and diffuse part of uh of our uh neural uh processing that being said there is so much other kinds of processing that happens in our brains that gives us general intelligence right and so when you look at that like you say Okay humans are our best model of strong intelligence what does the llm do right there is some evidence that llms have uh you know theory of mind and planning and anticipation and that sort of stuff but these are only very small components of a greater whole and so this is another reason that I think gbt4 will be disappointing specifically it doesn't add up to AGI reason number three that it's disappointing it's not R2D2 yet thanks to fiction uh specifically science fiction we all have pretty good ideas as to what we want robots and AGI to look like now whether or not you classify R2D2 as an AGI is entirely up to you um AGI is a terrible definition uh it's a pretty useless term um and you know I say that often whatever don't need to get on that soapbox but point being is that gpt4 no matter how powerful it is it would only be a component of AGI it would only be a component of R2D2 you still need the body you still need an architecture you still need blah blah blah you know there's all kinds of stuff that you still need so with big picture thinking like let's take a really big step back for just a second um you know you and I the people that are lost in the weeds of large language models in AI we kind of sometimes forget to look at the forest for the trees and so what is the goal here right you know if you if you you take a big step back it's like okay you have a language model that's certainly not Ai No matter how good the language model or not AGI sorry no matter how good the language model is it will never be AGI in and of itself so the rhetorical question is like what is what is the goal here like what do we actually want to achieve and the reason that I keep saying that AGI is a useless term is because one it's poorly defined and the goal posts keep moving some people seem to expect it it's not going to be AGI until it is a world brain and supersedes every single human on the planet to me that would be ASI or artificial super intelligence um and that that's not criticizing any individual uh but you know just I want to make the observation that AGI is like a unicorn right we're just we're chasing something that doesn't exist we're chasing the dragon um so it's kind of a useless term which is another reason like gpt4 will be disappointing because nothing will ever achieve AGI because the goal posts keep moving reason number four that gpt4 will be disappointing this is probably my spiciest take depending on what your uh profession is scale is not all you need um so for those not in the know there was there's been this um this like Battle Cry almost for the last couple years especially with the progression from GPT one to two to three where it's like scale is all you need that's what people were saying because it just seemed like the more parameters you added and the more data you threw at it the smarter it got well here's the thing gpt3 was already trained on like 200 lifetimes worth of text so there's only so much more text you can read right before it starts becoming redundant um sure there's probably a little bit newer news that it can read maybe some updated Wikipedia articles a few new scientific papers but once you have a really good understanding of the world doubling the amount of data isn't going to change that much there's only so much fundamental information about the universe that is available through text right as we as I pointed out earlier there's other modalities that have we haven't even begun to integrate and so scale is not all you need furthermore people have not really put that much research into cognitive architecture there is a reason that me little old me and individual independent researcher is playing in the same sandbox as like young lacun and that is because there are so few people working on the concept of cognitive architecture um and the the biggest part of the problem this is this will be a little bit so boxy so one of the biggest problems is that there are there's a little bit of ego and you know yeah pot calls kettle black I know that I'm egotistical about this but you know the the purists um or pure that's that's too much of a loaded term the people the Specialists I'll use that word the people who specialize in algorithmic improvements and math and loss functions and uh deep neural network design they don't see cognitive architecture as a legitimate field of research they say oh well that's just an integration problem I'm not a systems engineer you're a systems engineer you go integrate it with other systems um and I actually literally had a researcher um at a major university tell me like when I was talking about cognitive architecture he's like oh that's just an ensemble of experts he just brushed it off like oh that's that's not even useful um and it's like no cognitive architecture is actually a much more complete Theory and so there's this really really deep Schism uh one people aren't just not putting in the effort to cognitive architecture but also there's a lot of prejudice against it for whatever reason and so because of that and because we're seeing these diminishing returns for uh scale scale is not all you need a monolithic deep neural network will never be AGI like I stake my career on that um it could get you know it could end up with really really impressive abilities but you need specialized motor control systems you need specialized memory systems you need offline Learning Systems there's all kinds of problems you need semantic search there's all kinds of problems that you just cannot solve just with a deep neural network uh and so this is the biggest thing that's missing in in my mind from the active research okay I'll get off my soapbox now reason number five over promising and under delivering so if you go to the openai website and look at their about right at the top it says our mission is to ensure that artificial general intelligence benefits all of humanity and then Sam Altman on an interview says we don't have an actual AGI and that's sort of what's expected of us it's like well yeah but that's because that's what you said like you you set that expectation um and now that being said I know that uh I and others are you know critical of open AI um I don't want to undersell the value that they have added to the world with their Research into large language models but that being said there is a disconnect between what Sam said on stage or in an interview and what the official mission statement of open AI is so to unpack that a little bit further Microsoft is the biggest partner of openai and Microsoft is a mature Enterprise business software company so they you know as as a as a publicly traded company they put profit and adding value to the market Above All Else researches the means to that end open AI started as a non-profit where they kind of stumbled into creating value and then they pivoted to trying to be a for-profit company so they have not fully in my opinion uh you know take it for what it's worth I you know I'm not a uh venture capitalist or anything I'm just a dude with an internet connection in my opinion open AI has not yet figured out how to be an Enterprise company they're small they're growing I think uh Sam said uh recently there's like 375 employees in the grand scheme of things that is a small company open AI is a tiny company compared to the like tens of thousands of employees that Microsoft and Google and Amazon have and so they are very very small they are still learning to be a for-profit company and so the open AI the non-profit still exists and the for-profit open AI version is a wholly owned subsidiary of the non-profit and it's a capped profit company not sure about the details of that anyways point being is that Microsoft their their their primary partner puts creating customer value Above All Else that is how Microsoft got big and stays big is by creating value for customers open AI they are still focused on more lofty idealistic Ambitions and not necessarily creating value for individuals and that's because their goal is as I said earlier to create AGI but that's kind of like chasing a unicorn so this is where I think a big part of the disappointment comes in is open AI like the left hand isn't really fully talking to the right hand so to speak so in conclusion gpt4 is just going to be more of the same as we got with gpt3 just a little bit better and so to recap the five reasons uh the development of llms is clearly slowing down indicating that we've hit a point of diminishing returns meaning we're going to need to switch to multimodal models or different kinds of models or in my opinion invest more in cognitive architectures number two gpt4 will be text only sorry I kind of bled these two together which still leaves out the vast majority of information about the World visual auditory sense of touch sense of balance all kinds of of information streams are just not available yet until we have more kinds of models number three uh it's a it's only a component of you know the droids or whatever it is that we want it's not you know it's a necessary but not sufficient component right if you want a Ferrari you need the engine but you also need the body and the wheels so the way that I look at it is that large language models are just the engine or part of the engine for what will eventually be considered AGI uh reason number four scale is not all you need you also need a theory of cognition or a cognitive architecture to wrap around it there's a common theme here and number five is open AI just simply needs more business Acumen right uh they need like you definitely keep your core values I will be the first one to say that having a lofty and ambitious purpose is absolutely necessary but you also need to support that with Better Business acumen okay thank you for watching um feel free to connect with me on LinkedIn or patreon you might have noticed that I disabled comments and that's just because people on the internet can be mean and it's not a good use of my time um so if you really have something meaningful to say uh connect with me on LinkedIn or patreon I pay much more attention to those platforms thanks for watching