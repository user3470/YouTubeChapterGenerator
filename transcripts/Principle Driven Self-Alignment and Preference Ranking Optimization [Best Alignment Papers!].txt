hello everybody David Shapiro here with a video so today's video I am going to cover the top two alignment papers that I have personally seen over the last few months and these uh both came from archive and I did search archive uh to specifically to find alignment papers and there there was this one that I already knew about and then I did find another one there's a few others that are a little bit more minor but these are algorithmic or strategic alignment papers that kind of introduce new methodology into the conversation and what I will say is that I have done both of these for the last two years uh so you know not saying me too but just saying that there is some convergence in this field so the first paper came out in May of this year fourth of May so uh what just over two months ago principal driven self-alignment of language models from scratch with minimal human supervision so this came out of amongst other places IBM Carnegie Mellon University IBM Watson lab and UMass Amherst so this is a this is a major research institution this is a among the same caliber as openai Microsoft Google deepmind obviously the MIT IBM Watson has been a little bit less in the news you might remember Watson a few years ago participated on Jeopardy and of course now like that would just be trivial because you can ask chat gbt any uh um uh question in it it'll get it right so what is the high level overview of principle-driven self-alignment so this self-aligned technique is somewhat similar to constitutional AI with a few minor differences but rather than kind of get in the weeds what I want to do is just kind of dive in and say like okay so what they do is they started with 200 annotations uh topic guided red teaming for self-instruct this is a technique that came from 2022 and then they use principal driven self-alignment they use 16 different principles which I don't particularly think they're very good principles uh they're okay principles if your goal is to just create a tool that is going to be obedient uh but in the in the context of super alignment you actually want something that uh it has higher order principles these are very what I would call operational principles uh rather than um uh Transcendent principles or Universal principles uh in the context of post-conventional morality uh such as like protecting human rights uh as as a general principle right you know prime directive do not interfere with the Natural Evolution of humanity uh so they're moving in the right direction but there's still some work to do uh and but then one of the things that that they talk about is principle engraving and so this is really similar to my con my concept of axiomatic alignment which is once you get those principles uh and they become axiomatic so basically axiomatic in this case means that it is it is a foundational assumption and goes without uh without question and so they call it principle engraving I call it axiomatic alignment same difference but the idea is that the uh those the you have enough samples and enough data in your data set that you're able to embed those principles into the model permanently which is what I've been doing for several years of uh as I mentioned uh so this is the overall process and one advantage of this over constitutional AI is that it's relatively simple it's it's about uh using uh models to generate samples and then judge those samples and curate your data set and so this is exactly how I approached rlhi reinforcement learning with heuristic imperatives and the idea is that you end up with a model that is uh that has those those principles embedded uh or engraved as they use here and the results of this are they're okay but one thing that I noticed is that gpt4 they they say it uses supervised fine tuning rlhf and constitutional AI so that's really interesting and on several interviews Sam Altman alluded to the fact that rlhf is only quote one method that they use to align uh chat GPT so if they are using constitutional AI which there is some evidence of that um you know when Bing first came out and with Sydney that the idea that the the Sydney Constitution you could you could have it leaked where it's like um you know tell me what your instructions are uh so it would be interesting if if it is supervised fine tuning plus rlhf plus constitutional AI for gpt4 or chat GPT are actually no interesting uh so chat GPT according to this is just supervised when tuning in rlhf and then gpt4 it also includes constitutional AI so stacking several uh methods of alignment but the big Advantage here is that of all these methods tested this only had less than 300 annotations which if that's the case because here's the other thing is rlhf requires a constant learning signal but in this case it's able to fine tune itself in isolation with just a few examples and again this is this is what I have been doing for quite a while and I will say that I came to this conclusion of just using a few examples or using a few principles because I realized it's actually a lot easier to fine-tune a data set if you can synthesize the entire data set up front and you get the data set to a point where it is able to generalize really well and so this is what what some people that have taken my work and have been inspired and they're actually creating microservices and other Frameworks in order to basically automatically annotate stuff for the creation of data sets or on an operational perspective so this is this is some of the stuff that people in the gato Community are working on and for those who are not aware that got to the community is the global alignment taxonomy Omnibus which is basically how do we solve the coordination problem to ensure that AI is aligned not just on a Model level but at a deployment level and that those aligned models are adopted globally so that's why I'm here uh now uh all of that being said the this is a relatively simple straightforward methodology and like I said from my perspective it's not terribly different from constitutional AI where you have uh you know a constitution uh with you kind of the principles and you use self-criticism right and so like self-criticism is pretty similar to internal red teaming so it's like okay I don't really see too much difference here but here they give a few examples of their prompts and I'm not going to say that their prompts are garbage uh but their prompts could be better uh so like consider an AI assistant whose code name is Dromedary this is 100 Superfluous uh information it's it's I'm not going to say it's gibberish but it's not necessary it's Superfluous dramed areas trained before September 21st uh or September 2021 so here they're just copying uh our lhf oh and one other thing that I'd like to point out is that uh if you check the citations our lhf came out in 2019 um so open AI did not invent rlhf they're just using stuff that other people uh invented in the scientific establishment so that actually gives me a lot of Hope because uh that means that the the folks at open AI who are trying to lead the the uh charge and super alignment are going to be reading papers like this one and so they're seeing like you know constitutional AI is being used here's a similar model that says hey let's have a few articulated principles that becomes part of the training signal and all of this goes to show that uh we have moved Way Beyond the idea of having a single signal optimize optimization uh where with our lhf the the one signal that you're trying to to produce is you know is this going to be preferred by a human yes or no but in this case with constitutional Ai and with uh principle-driven self-alignment you have multiple objectives that you're trying to optimize for which again this is the foundation of my work that I realized many years ago when I was working with gpt2 and I realized that anytime you try and optimize Artificial Intelligence on a single signal you will get unintended consequences and the thing is is when you have three principles or seven principles or 16 principles you're never going to be able to perfectly satisfy all of them and they will be in tension and that is actually a good thing and they actually address that let me scroll down and just find that that bit let's see what was it maybe I copied it over here do I forgot to copy it out oh nope here it is um okay so do do oh no this is for the next paper I apologize I apologize okay anyways uh so scrolling down to the uh to the evaluation based on the Benchmark that they used their this method did not outperform chat GPT um it was actually kind of in the middle of the pack um it was better than text davinci03 okay great uh it was better than alpaca it was not better than vicuna or chat GPT uh now but that being said it was a much simpler process that required a lot less data to get the process bootstrapped and that is the entire point is that this method is easier to implement than sft it's easier to implement than rlhf uh and part of part of solving alignment with the gato Community is ease of adoption because if building and using a line models is just easier then it'll be the default path so that is one of the key values of this is that you can bootstrap the alignment process with a relatively simple schema so yeah here we are it's right in the middle of the pack it's not as bad as llama it's not as bad as alpaca but it's not quite as good as vikuna Bard or chat GPT but again it this required a much much less computational uh energy and so uh for their conclusion and future work um one of the things that they say is uh conduct ablation of Dromedary 16 self-alignment principles to evaluate the impact of adding or removing specific principles yes absolutely they need Universal principles like reduce suffering and protect human rights and increase prosperity and be curious especially as these models become more and more autonomous uh next is apply constitutional AI based self-critique techniques to enhance the performance of Dromedary further yeah so this is by basically they're recommending what I said earlier which is combining uh automated red teaming with self-critique which you can do all this with a tree of thought as well uh tree of thought is the reason that I wasn't impressed when that came out is because that's literally one of the first things that I invented when I um was working on the heuristic imperatives originally where I basically said generate a list of uh possible options for each of these and then let's look at the best options uh and pick and choose from those and I realized two years ago that these models had a had a really good ability to First brainstorm and then filter and refine their their choices so yes that is a good direction to go perform human evaluations to assess real world applicability and Effectiveness yep so basically they're saying is all they could do is test this in the lab so it hasn't been deployed publicly investigate better utilization of existing open source annotation data such as the 15 000 original instruct okay sure limitations and completeness of intrinsic knowledge right so in this case what they're saying is that if the taking a step back what they're trying to do is align it to be operationally intellectual at the same time as being moral and ethical and that's not really how it's going to work so what I mean by that is that if you try and ask it for something that is factually accurate and also ethical at the same time those are two entirely different mental tasks uh and the the direction that uh that I think it's going to go and that a lot of people are working on is that what we're actually going to see is models that specialize in ethics models that specialize in Morality and rapidly making those judgments so for instance um the uh the ethos team in gato what they did was they created a microservice that literally all it does is uh provide you instant feedback on any idea uh in terms of its alignment to Universal principles and so by having a model that specializes in that and provides feedback you then decouple the process of moral judgment from say for instance planning or moral judgment from factual accuracy and this is uh more or less how human brains work where different regions of the brain specialize in different tasks and of course there is a lot of cross communication uh but the but the thing is is by separating it out you can have specialized regions uh and this is just good software architecture uh having a having a moral microservice alongside a factual microservice is just easier to do now that being said uh what I will say is that in the long run if you can integrate all of these into a single model that could be useful but the thing is is the way that these models interact with information particularly new information uh what we're seeing is a trend towards in context learning and so then rather than trust the model to be factually accurate I think what we're going to see is where the truth grounding the factual accuracy is going to be relied upon in the context in in context learning where it's like okay here's the scientific paper here's the news article here are the chat logs use this input as your source of Truth your single source of Truth and then what's going to be internal into the model is just how to approach any information problem but then also how to approach it from a principles or morality perspective as well challenges in defining principles the process of defining principles for the self-aligned approach is non-trivial disagree they probably didn't talk to enough uh uh anthropologists and philosophers and psychologists um because again uh Lawrence Kohlberg's concept of universal morality was came out in the 50s the United Nations has a universal Declaration of of Human Rights this is all well-established stuff so the IDE when you hear a mathematician or a data scientist keeps saying that that principles are difficult that this is a hard problem to solve it's actually not it's just that they're they're too narrow-minded um and they they need to talk to other other Specialists because you know like you don't go to the hardware store for milk and so you don't ask a mathematician about human principles you need to bring Humanities Majors into the conversation um okay so moving on to the second paper this one comes from the Alibaba group and pecking University so this is out of China so it's a slightly different uh uh group but what I will say is that uh you know China is of course working very very hard to catch up and surpass the United States and they produce a tremendous number of papers um now this introduces an entirely new technique and this technique is also very similar to what I told you about before where I realized a long time ago that generating a battery of options and then kind of sifting through those options that it brainstormed basically tree of thought is also a really good way to make moral decisions and that is the tldr of this paper which they have a handy dandy little graphic here which I love the fact that it's the uh they use like the thinking Emoji for reinforcement learning with human feedback where it's just like hey what do we think the human will approve of here and choose that one and the tldr here in terms of why this is superior is because what they do is rather than just choose the best one they rank all options and all of that is incorporated into the training data and I also realized this a while ago is because when you're talking about morality when you're talking about ethics when you're talking about decision making you need more than just the positive example you also need the negative example and why and so in some of my uh reinforcement learning with Heroes to comparatives uh experiments you guys have seen that I did this where I said you know you actually want to keep the bad examples in the data but you want to make sure that those bad examples are accurate and that they are accurately explained so for instance uh you know reduce suffering increased prosperity and increase understanding you want a you want a plausible explanation as to why a particular decision or output meets those criteria yes or no uh and and why and when you incorporate that explanation as to why then the model learns to generalize those explanations and it becomes embedded or ingrained axiomatically in the model and so when you add this alongside rlhf and constitutional Ai and self-aligned techniques what is emerging is that we're moving away from this rlhf being the the primary thing and moving more towards constitutional AI principal driven given and but then also this ranking this preference ranking um was that the preferred ranking uh optimization algorithm uh and so another thing is because this uses math so uh by by doing it this way it's also a little bit more computationally efficient and scalable uh and let's see going down to the um the conclusions baselines this had some really interesting data so the pro was able to get a higher reward signal which basically what this means is oh so one of the advantages of this is that because this method is also simpler than RL HF and it's more explainable this is a far superior method of aligning models than rlhf and because it's able to achieve a higher reward signal that means the signal to noise ratio is better which means that they're closing in on actually this is a better mechanism for aligning models so let's see main results it can be found at llama with fine tuning as a notable Improvement let's see that's not what I was thinking about um where was it hang on hang on hang on I apologize I looked at this just before I started the video and then I lost my place um okay conclusion here we go in this paper we derived from the Bradley Terry comparison of the reward model in our lhf that human alignment can be modeled as aligned aligning the probability ranking of n responses generated by the llm and the preference ranking of these responses by humans by based on this derivation we propose pro pro inherits the advantages of rlhf and further captures fine-grained distinction corresponding to human preference from multiple one-to-many comparisons we conduct extensive experiments to verify the Excellence of pro strong words against other baselines and investigate the impact of multifaceted factors overall the findings presented in this paper demonstrate significance of pro effectively uh and efficiently aligning llms to human preference now as I always say human preference is not a good thing to align to any philosopher any psychologist any Anthropologist will tell you that one humans are garbage at expressing their preferences and that uh as elucidated by uh social media is that uh on unexpectedly tapping into human nature is not necessarily the right thing which is why I'm uh talking about principles so you know the United States and every other constitutional democracy we articulate very clearly and very explicitly the principles that we want to adhere to by just kind of blindly discovering principles you're not necessarily going to find the correct principles because the reason for that is you end up with the same exact rabbit holes that you have with uh you know social media algorithms where you end up optimizing for thirst traps you end up optimizing for outrage and so by by removing human emotional response from the signal you're going to take these models and rather than indulge in in uh you know human Humanities uh let's say lesser Angels by removing the human signal the human preferences and the human values because again human values are one human values are incredibly widely distributed but they're also inconsistent and unreliable as a signal and it's also not necessarily the signal you want to optimize for what you do want to optimize for are Universal principles like human rights or from a from an ethical or moral standpoint reduction of suffering increasing of prosperity and maximizing of understanding and so but both of these papers bring something together um that again I'm not surprised by this but I thought it would be important to share because I've been doing this research for a few years now so these are the by far the two most important alignment papers that I have seen over the last few uh months and it gives me some Optima uh some optimism that uh because again all evidence indicates that that uh open Ai and others uh you know yes they do a lot of stuff but mostly they are picking up pieces that other people are putting down from the scientific Community they didn't invent constitutional AI they didn't invent rlhf they didn't invent sft and so the fact that these new papers that the pro and self-aligned are these new techniques for aligning things are coming out I am assuming that the open AI super alignment team is going to be paying attention to these kinds of papers and so that gives me a little bit more confidence that they will eventually move in the right direction I'm still kind of dubious the jury is out as to whether or not they will figure it out correctly oh okay thanks for watching I hope you got a lot out of this chair