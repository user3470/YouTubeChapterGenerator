good morning everybody David Shapiro here with another video so you know me I am deeply optimistic and one of the things that I've done is I've created uh gato which is the global alignment taxonomy Omnibus which is a decentralized movement to help achieve uh alignment of of digital super intelligence before it emerges uh it has to do with corporate adoption International regulation studying models decentralized networks that sort of thing and so of course I keep my finger on the pulse of the current conversation I watch uh pretty much all the Congressional hearings on the control of AI and safety uh International I'm connecting with all kinds of people across LinkedIn across the world United States Europe Britain pretty much anyone in the space of AI governance and safety as well as people leading the charge in research now there are two papers that just came out that for me signal that uh obviously it's not a foregone conclusion uh it's not over till it's over but one of the complaints one of the criticisms that I often have is that it feels like up until this point uh there were no adults in the room and the adults were not taking it seriously uh well that has changed in a big way uh so the first paper is this one Frontier AI regulation managing emerging risks to Public Safety now that sounds pretty dry but if you listen to Eleazar yukowski's Ted Talk which is a pretty fiery indictment of the failures of uh the academy uh pretty much any any criticism that I have of the institutions uh is mild in comparison to his um now okay so taking a step back though when you look at who wrote this paper uh there's a there's a bunch of names some of them jump right off the page miles Brundage from openai and a few other people that I'm just becoming familiar with uh for the first time uh but when you look at the institutions Center for governance of AI Center for New American Security Google deepmind open AI Brookings Institute so Brookings Institute is a is an interesting one Center for long-term resilience center for the study of existential risk the University of Cambridge University of Washington conversion convergence analysis Center Center for international governance Innovation uh let's see who else Harvard University of Toronto Vector Institute cohere Microsoft uh University UCLA so these are Big names these and this is this obviously it's not Global this is pretty much entirely uh America and Europe but America and Europe are currently you know world leaders in terms of uh uh polarity of of power so this paper outlines a few things and they've got it pretty pretty well outlined so first they just say self-regulation is unlikely to provide sufficient protection against the risks from Frontier ai ai models government intervention will be needed so just unequivocally we need to do this so we explore how to do this mechanisms to create and update safety standards uh so that's that's pretty generic that's pretty out there uh or you know like a boilerplate mechanisms to give Regulators visibility so visibility of course like if you need to uh see something there you go so disclosure regimes monitoring processes and whistleblower protections um yep these equip Regulators to address the appropriate regulatory targets so basically uh establishing that governments and international bodies have the rights and access to see into things and of course this is well established in many Industries whether you're dealing with toxic waste nuclear power uh and the financial industry of course like you know the the financial industry is one of the most heavily regulated and visible and of course many people will argue that it's not regulated invisible enough and others will argue that it is overly transparent and overly regulated I'm not here to to debate about uh regulate Regulatory Compliance um in other Industries just pointing out that there is a tremendous amount of precedent uh let's see mechanisms to ensure compliance with safety standards uh self-regulatory efforts such as voluntary certification may go some way uh uh however this seems likely to be insufficient without government intervention for example by by empowering a supervisory authority to identify and sanction non-compliance or by licensing the deployment and potentially the development of Frontier AI uh and so what they mean here is not Joe Schmo making an open source model what they're talking about is people working on gpt5 people working on the next-gen stuff the people that literally have billions of dollars to throw at it so in terms of the regulatory burden I'm not really worried about that so those are three overall categories now we I now we describe an initial set of State safety standards that if adopted would provide some guard rails okay conducting thorough risk assessments informed by evaluations of dangerous capabilities and control abilities so part of what they're talking about is establishing a battery of tests uh you know whenever whenever the uh MSDS sheets come out for a substance it has gone through a rigorous set of tests and the Underwriters Laboratory rigorously tests all new products that go in your home if you ever see the UL underwriter laboratory uh stamp on anything and it's it's pretty much on every product if you look it's usually on the sticker on the cord or whatever basically it had the the products in your home have been tested to failure and they have been tested to failure in such a way that we know that they won't kill you or that if they if they do burst into flames that it you know is only under certain circumstances that sort of thing so it's essentially what they're what they're doing is is saying okay we need to rigorously test these products for safety and danger and and establish a standard a set of Standards tests uh engaging external experts to apply independent scrutiny yes so you don't want the fox guarding the hen house it's that simple uh you know with the release of gpt4 open AI they did engage some external red teaming but they really didn't do enough but also and and also they were guessing and it was not transparent it was entirely uh private it was their decision uh and and their dime and they were not required to do it and there was no established standard because well they're one of the first to do it so and again like that's not an indictment it's good that they took the initiative to do it but there is a correct way of going about uh independent uh scrutiny um and from a from a technology perspective I have worked with Auditors um and I'm not saying that you know a sarbanes-oxley auditor is the right way to go about it but that being said uh there are there's lots and lots of models on how to do this uh follow standardized protocols for how Frontier AI models can be deployed based on their assessed risk so this kind of dovetails with the EU AI act where you kind of categorize AI based into several buckets of um of risk profiles uh and that uh that that idea had some backlash there was an open letter signed by more than 100 Executives in Europe saying that the uh EU AI safety act or the EU AI Act was going to be uh to onerous on businesses and uh you know what I I disagree um you know but it's an ongoing conversation and ultimately there's enough attention there's enough International attention and enough political willpower to solve this problem that uh yes I suspect that uh that there will be some regulatory burden but uh as per the the Senate hearing where uh Dr Roman uh Chowdhury spoke one of the things that they've pointed out uh and they being the researchers some of the people on this paper is that good regulation and good oversight can actually accelerate research because it can de-risk it and when you de-risk something and you increase the quality of information you can actually increase investor confidence and where the money goes the research goes and having talked to people in finance uh particularly uh you know uh Venture Capital uh and big big Finance big Banks uh particularly over in Europe what I will say is that yes there is a tremendous amount of excitement but there's also a tremendous amount of trepidation and having this third party uh level of scrutiny and certification and and governance and oversight is really going to boost the confidence of venture capital and investors which of course that means that more money is going to come in which means that the companies get built faster and safer and so by creating a safe container you're actually going to accelerate a business I know that it's not uh it's not an intuitive thing but if you talk to if you talk to people inside the finance industry that's that's generally how it works now that being said if you are in finance if you are in VC uh and you want to talk to me I'm happy to uh talk to you reach out we can either do a podcast episode or just talk privately um so that I better understand the intersection of safety regulation compliance and investment uh and then finally the fourth thing is monitoring and responding to new information on model capabilities uh basically it's changing fast and so uh we need they need they recognize the need to keep up and then they uh let's see oh here I used my thing image job over here to talk with the model about it um let's see this is the international institutions one so I ask a few questions let's see this paper focuses on regulation of Frontier AI etc etc yep so certainly the paper identifies three key challenges so basically I had it read read the whole paper and kind of summarize uh very succinctly so the unexpected capabilities problem that's what this paper talks about um unexpected capabilities this challenge arises from the fact that the capabilities of AI models can improve rapidly and unpredictably and they also actually they have some charts on on this too um unexpected capabilities let's see where is it here we go so they're just talking about let's see certain capabilities seem to emerge suddenly now I have talked about how the fact that the sudden emergence is partially due to the measurement problem which is when gpt3 came out nobody knew how to measure these things and so then if you retroactively go back and and test gpt3 under the correct circumstances for things like theory of mind and planning it already had that I demonstrated and documented it in my books going back two years more than two years now but so one thing to keep in mind is that some of this is like it's kind of like how a diagnosis of autism went up after the diagnostic criteria changed so when you start measuring something and you start finding more of it that's not necessarily a surprise now that being said I do agree that that training more data on larger models with still fundamentally the same Paradigm of large language models just predicting the next token I will agree that it does seem like some capabilities do spontaneously emerge and from an infant information perspective we don't really know how or why that happens so the unexpected capabilities problem oh also I suspect that this will go up 10x once we have more multimodal models um but anyways from a regulatory standpoint this makes it difficult to reliably prevent deployed models from point from posing severe risks you don't know what you don't know you create a black box and you push a button and you don't know what's going to happen that's kind of scary from a regulatory standpoint and a safety standpoint the deployment safety problem so this is what we talked about already a little bit The Challenge relates to the difficulty of ensuring that deployed AI models do not cause harm while developers Implement measures to prevent misuse these measures may not always be foolproof and malicious users may find ways to circumvent them additionally the unexpected capabilities problem means that developers may not know all potential harms that need to be guarded against during deployment and then finally oh and that so this is what I'm what I was talking about is like you don't know what you don't know and so for instance um you know a small team is not going to be as creative as the rest of the world as we have seen when people got access to chat GPT and then immediately created chaos GPT uh and then there's the you know people using it to research Anthrax and coronavirus and and gain a function research which is synthetic biology which is AKA the lab leak hypothesis and then finally the proliferation problem and this is something that I've been talking about this challenge refers to the rapid proliferation of Frontier AI models which can make accountability difficult models can be open sourced reproduced or stolen allowing access to their capabilities by unregulated actors this means the dangerous capabilities could quickly become accessible to criminals or uh adversarial governments I actually have a video coming out about this uh that specifically which the the tldr is that we're going to need fight to we're going to need to Fight Fire with Fire the only way to keep up in this kind of arms race is to use the AI to defeat other AI um and then I ask the paper like okay what are the building blocks um and so it was able to read it and kind of synthesize it down into three primary uh building blocks institutionalized Frontier AI safety standards development um so this includes the development of safety standards through multi-stakeholder processes that's kind of what we're talking about with uh Regulators compliance uh that that sort of thing but it also they talk about enforceable legal requirements so basically this is the governments have the right to shut down a data center if it is going rogue the more extreme position is it uh like Eleazar yukowski said like we should just stride out bomb data centers uh if they're non-compliant which uh you know escalating to a shooting War he's not an international geopolitics military strategist so if you escalate to a shooting War I don't know that that's actually going to be beneficial uh and prevention is worth an ounce of prevention is worth a pound of cure put it that way um so a heavy-handed approach like that probably not good now increasing regulatory visibility also we kind of talked about that and ensuring compliance with standards Okay cool so this lays out this paper lays out a pretty comprehensive like okay here is a regulatory framework so this paper will then kind of hand it off to uh or or begin the conversation with politicians with think tanks with all the people that are going to be working on creating these programs and that sort of thing now the second paper that I want to go over is this one which is even more interesting to me International institutions for advanced AI now if you remember I have uh vociferously advocated for the creation of new research and Regulatory institutions uh I called one of them Gaia Global AI agency and the other one uh this actually came from the YouTube comments was Aegis the um what was it the alignment enforcement for Global intelligence systems uh now obviously they're not this paper isn't going to name them but we collectively uh came up with this idea and now this idea has been officially endorsed by many of the same players so when you look at the when you look at the names here Google deepmind uh blavatnik School of government so these are policy people University of Oxford in a center for governance of AI University of Montreal and Milan cfar openai Columbia Harvard Toronto open AI again Stanford new field and Oxford again okay so what does this paper do this paper advocates for four kinds of approaches to this so a commission on Frontier AI Advanced AI governance organization which sounds really like a guy or Aegis a frontier AI collaborative so this is about research and public-private Partnerships and then an AI Safety project so they proposed these four International uh entities these four International bodies and you see here that they're citing stuff like ipcc the ipbs iaea the international Atomic agency CERN either uh you know so they are citing the existence of plenty of other International agencies much like I did in my video just a couple days ago and so then they break it down into this nice table so the function is broken down into research and enforcement or or regulation so research and Regulation and then they have the four objectives right and then they've got some some sub behaviors so under the function under the research function there is uh conduct or support AI Safety Research build consensus so remember a consensus is a big part of uh the gato framework building Global consensus about AI is actually really critical because we all need to be having this conversation because as I tell plenty of other people whether or not you are involved in AI you are a stakeholder because it will affect you just like how whether or not you know anything about nuclear weapons or bioweapons you are a stakeholder because guess what they can kill you likewise artificial intelligence is going to affect the way that you live it's going to affect your safety it's going to affect your prosperity it's going to affect your personal security so you are a stakeholder in AI whether or not you care or acknowledge its existence or participate in research the third thing is develop Frontier AI so this is something that I would have been advocating when I talked about eater and CERN we which is that uh the the incentives of a for-profit company when a Forefront profit company develops something like gpt4 they have the incentive to keep as much of its secret as possible which stands in contrast to what is in the best interest of the public good uh now that being said I know that Sam Altman has said that he would prefer to democratize access to all AI for everyone uh time will tell I know that right now open at open AI has uh obligations to Microsoft and once they hit that 100 billion dollar Mark we will see I really hope that Sam Altman and the rest of openai uh hold you know that that they honor their word and that once they earn Microsoft 100 billion dollars they will open source everything and that they will open source it safely if they do great Sam Altman deserves a freaking Nobel Peace Prize now time will tell because open AI has pivoted in the past open AI has said one thing and then done another so remember actions speak louder than words but if a international research organization uh that is funded by the people and and controlled by the governments of the people is responsible for developing the frontier AI then maybe their incentive uh structure is a little bit different and this is uh you saw this with a Brit GPT right the UK has decided that they're going to train their own uh version of GPT which is great because that means that uh then then the incentive structure for what they do with that is going to be very different and also it costs like what I think one paper estimated that it cost 63 million dollars to train gpt4 that's a drop in the bucket for a country like Germany and Britain and America who cares that's a trivial amount of money in the grand scheme of things we waste more uh anyways not gonna not gonna rant about uh government waste here that's not the point uh distribute and enable access to AI so again democratizing access so these are four kind of behaviors or functions under the research and you can see that um they've kind of got this nice little it's almost like a racy chart or a racy Matrix to say who's going to do what and then under the regulation the rule making and enforcement uh set safety norms and standards uh support implementation of Standards monitor compliance and then control the inputs so by control inputs I think what they mean is the uh the the um the data the hardware and the software so and then they have over here a spreading beneficial technology harmonizing regulation so this is solving the coordination problem that uh Daniel schmachtenberger talks about and solving coordination is actually why I created the gato alignment framework because uh what I realized is that um without some sort of global consensus without that uh you know everyone operating more or less in lockstep then you're going to have coordination failures and you're going to end up with that terminal race condition that I talked about in a previous video so by harmonizing regulation across the world that is how you prevent a race condition and this is this is how you keep uh the comp the competitive landscape sane but also predictable ensuring safe development and use and then finally managing geopolitical risk factors so this is slaughterbots this is uh military race conditions and that sort of thing and so uh by this is it's great that they're including all this existing International institutions and so they have a whole bunch of um of examples uh now the problem is that uh that there's a few gaps here so you see there's only a few that exists so like semiconductor export controls is one of the things that can manage the geopolitical risk factors now then they recommend some new uh agencies which is great so they're AI Safety project their Commission on Frontier AI Frontier AI collaborative and then the advanced AI government agency again super sounds a lot like Gaia um yeah so the you know this this paper it's kind of more or less what you know what you see is what you get um they kind of break it down which is great um now but really the thing is the thing that is most important to me is that the adults are in the room that we've got people like miles Brundage of open AI we've got uh Roman choudhary who have heard her speak she spoke directly to Congress so these are people that are uh very serious about this and the the fact that they are proposing these things that a bunch of us independently came to tells me that there's a tremendous amount of consensus already uh I don't know if my YouTube channel had anything to do with that I hope it does but you know I'm here to report the good news so long story short when I look at the array of what is happening and who is talking I strongly believe that the existential risk of AI is we are well on the way to mitigating those risks we're also looking at shorter term dangers and harms so one of the things mentioned in these papers is using as I mentioned earlier the using these models to for instance create new pandemics synthetic biology is one of the the biggest thing let's see did it was it mentioned in this one yes so here we go severe arrest of Public Safety a general purpose personal assistant that is capable of designing an autonomously ordering the manufacture of Novel pathogens capable of causing a coveted level pandemic so this is this is rapidly percolated up to being one of the greatest risks um and of course it's fresh in people's mind because we just all survived a pandemic but the idea that you can create something in a lab that can then hurt the entire world and you can't stop it uh you know that's that's kind of one of the one of the big things uh you know military is okay deployment to power and weapons and nuclear stuff already exists but the the cost the intellectual and monetary cost of creating pathogens is pretty low and that's getting lower with uh artificial intelligence going the way that it is um let's see was it mentioned here yes so they also cited that paper here um and uh or or no this is a different paper what is an advanced Market commitment and how could it help it be coveted uh so anyways these two papers uh are great news uh I think that uh I think that it will be uh read the world over uh particularly by governments all over the world and you know these these ideas have some teeth they've clearly done their homework they dotted their eyes and crossed their T's um and yeah so this really helps me relax uh with respect to Ai and you know two I I did watch yukowski's Ted Talk which came out yesterday I think um and you know he has a pretty fiery indictment of like you know I have been doing this for two decades and nobody listened and and I tried to avoid being here where we are and nobody's putting in any effort and or not enough effort or whatever and I'm like okay I get it but like is he not paying attention anymore like is he just banging this drum because it's fun to Bang the Drum um but like what these papers tell me is that one the adults are in the room and two they are taking it seriously and three it's got teeth like the money is coming the legislation is coming the regular regulation is coming the research is coming um and so it's like I don't know it seemed it seems like he's got an ax to grind and I'm not sure with whom um but you know and and the thing is here's the thing is I don't disagree with Eliezer in terms of when you scientifically look at the risk of super intelligence um but I don't agree that it is not being taken seriously anymore so anyways uh with all that being said thanks for watching I hope you got a lot out of this I hope that you feel like uh huh we're moving in the right direction again I'm not going to say it's a foregone conclusion it's not over till it's over but this gives me a lot of Hope cheers