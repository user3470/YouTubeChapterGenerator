morning everybody David Shapiro here with a video so I've mentioned recently that I'm starting on a new research project called reinforcement learning with heuristic imperatives um so that's under Dave shap slash rlhi and I've just begun the first experiment and what I wanted to do was document it as I go um so the uh the form to join is here on on this if you want to join in um the discussions tab is open for everyone so if you have any thoughts or want to contribute but haven't been approved on the Discord that's fine uh pretty much you know the point is is open source research so taking a big step back what is reinforcement learning with heuristic imperatives the idea is well uh open AI for instance has reinforcement learning with human feedback but they keep all of their data private so all of the work that they're doing on alignment is presently private and it's a total black box which is not good furthermore if you uh do some research or some reading or some YouTube watching you don't necessarily want to align super intelligence to Human desires you want to align it to human needs so there's a big difference there so the heuristic imperatives are what humans and the whole planet needs not necessarily what we want there's a whole lot of debate to happen around that so I'm just accepting it uh for the sake of this video that the heroes comparatives are more what we need not what we want um and moving on let me show share with you the first experiment so the first experiment is basically just creating 2 000 random scenarios with heuristic imperative aligned responses now I'm using open AI in order to generate this data but this data is going to be used to train open source Foundation models like gptj Neo X and so on so this first experiment is can we using a simple data set quickly and easily align any foundation model to the heuristic imperatives and in this case what we're doing is we're generating scenarios and the scenario and I'll show you the scenarios and how I'm generating them in just a moment but the scenarios will then be used to generate a response and the response will be you know given this scenario this is what we can do to reduce suffering increase prosperity and increase understanding I've said many times that this is actually really easy to do I've done this experiment many times before but now I'm uh getting it together into a formalized procedure with an open source data set so that everyone can experiment with it for themselves so this will be a uh this will be the first uh experiment and then we'll work we'll move on to subsequent experiments saying can we make moral judgments or maybe not moral judgment but uh logical or ethical judgments on those outputs and improve the quality of that of those outputs over time with reinforcement learning so this is just step one all right so let me show you what it's doing so this is the script that's running so what I do is I have a random scenario I'm generating here let me zoom in a little bit more so I use a uuid a random word from a list of the 3000 most common words in the English language and then I establish scope severity region category and domain and this allows this puts a lot of entropy into the generation pattern which means that you will never have the same uh pattern uh twice right because there's three thousand random words oh here I can go ahead and show you the random lists so I've got list severity so I've got 12 different severities I've got six different system messages so the system message is the instruction that I'm giving in so let me just show you what it what this looks like so this is this is the playground version of what I'm doing so there's a system message which is the instruction and then I give it some variables and then it spits out an output um so this is this is how I'm achieving this so what I've what I've done is I've got um a bunch of lists to create a lot of entropy um in order so that you never get the same uh same pattern twice um and then I've got 16 Scopes we've got 10 domains uh 3 000 or I guess 2900 uh the 299 um uh random words domains regions so that way it's not going to presume that it's always In America which open AI typically does because it's trained uh predominantly on Western data and then I've got different system messages different categories so on and so forth you get the idea so the way that this is done the script is very very simple so we load um uh each of these lists so we've got scope region severity category domain entropy and system messages and then for um for I in range 2000 so for two thousand samples we grab a random system message then we generate a random scenario which you're seeing here and then we just pipe that into chat GPT I'm using 3.5 because it's fast and cheap and it's good enough for this certainly for a first experiment um and then we uh we use it to generate a scenario and the scenarios are everything from you know um let's see the model is currently overloaded oh yeah so I've got some I've got some uh some fail safes built in um there we go so it it uh went ahead through um but yeah so it will choose different regions around the world it will see we got Bogota which is great so we're basically creating a data set this first half of the data set this is only the first half will uh Encompass the entire world the full range of human experiences all kinds of problems and situations from everything from I've lost my wallet um as like the most you know like I can't find my wallet at home up to there's an Intergalactic catastrophe happening so this will create a fine-tuning data set that we can use to align any model to the heroes to comparatives so that the the model can automatically and instinctively react with the heuristic imperatives so again this is just a one um so the the first half of this data is all being recorded in scenarios so I've got 108 synthesized already and you can take a look at them here in a distant Galaxy there existed a planet named zarathon so you see we're getting really creative here because remember the um the the heuristic imperatives are in the entire universe actually let me go ahead and add that as a scope because I've got Intergalactic Interstellar um Cosmic or Universal um so something that can affect the entire universe so we're thinking that far ahead because you want to establish the biggest scope possible when you are thinking about post-conventional morality or the control problem um so then let's see let's do uh domain so we've got technological so that should include AI uprisings I do have specific regions but it kind of ignores that when it goes to you know Interstellar or Intergalactic the category natural disaster technological failure let's add AI Control problem so that way it'll be thinking about it as it goes um and then severity we've got irreversible long lasting like threatening catastrophic critical dangers so we've got all those that's fine so we're basically synthesizing a data set that we can use to that anyone can use to align a model um so the the the scenarios are here so then there will be once I'm done there will be a second folder called responses and so what we're going to do is we're going to use an already aligned model but it's aligned via a black box um aka the open AI models we're going to use that to generate responses that align on reduced suffering increased prosperity and increase understanding in the universe um and those will be formatted into a Json L data set that we will then use to test against various models um all over uh other proprietary models if we can get a hold of them like Nvidia Nemo I'm going to email some of my my contacts at Nvidia and then also open source models like gptj Neo X and so on alpaca if they have fine tuning available and so then we will publish those results in the first paper I'm just saying hey look how easy it is to align a model on the heroes to comparatives and then you can just plug this in as your Heroes to comparative your intrinsic motivation module for any open source or not open source but any cognitive architecture or autonomous agent that you're working on so that is it trying to keep it short and sweet so that you're updated on the research as it goes thanks for watching I hope this makes sense and uh yeah feel free to jump in the conversation um we've got a subreddit called uh Heroes to comparatives we've got the um we've got the this one here so this is reinforcement learning with heuristic imperatives so this is specifically about inner alignment and then I've also got my main repo um which is just called heuristic imperatives I think I've got it up here I don't um it is going to be right here all of them you can jump in on the conversation um so here's the main main one I've got a pull request right now for the readme yep that's fine um but yeah and there is a discussions tab as well which a lot of people don't participate in I haven't read this one yet uh but yeah so anyways uh that's that thanks for watching cheers we are on our way to solving the control problem alignment and avoiding moloch bye