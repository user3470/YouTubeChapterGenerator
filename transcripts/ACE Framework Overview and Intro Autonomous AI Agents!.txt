hello everybody David Shapiro here with an exciting announcement so I've been alluding to the ace framework which stands for autonomous cognitive Entity framework uh we have finished and by we I mean the academic University team that uh basically recruited me to help help them publish a paper on this framework we finished the paper it's been submitted I think we're going to publish it on archive which is a pre-print server so it will be up in the coming weeks I'll add a link to that as soon as it's done the scientific paper is more of a of a deep dive a more kind of abstract scientific conceptual Deep dive but it is incredibly well researched and Incredibly well cited using both uh like well everything all of the above Neuroscience psychology philosophy uh so on and so forth including lots and lots and lots of recent papers about llms in the process of doing a literature review for this paper I found a lot of stuff out there that I hadn't even been aware of uh so yes the paper is very well cited now that being said this GitHub repo that I've created Dave shop Ace underscore framework is already out there it's it's under the MIT license this is going to be a little bit uh winnowed down so it's going to be much more geared towards uh practical utilization with a little bit less let's say jargon uh but if you are in the space of generative technology generative AI uh llms you know chat GPT gpt4 Claude uh all of the others this is the space that we're operating in now um so the purpose of this video is first I'm going to tell you a little bit about the project and then we'll go through this framework so at the highest level taking a big step back um I poke I I updated the contributing to the ace framework you can see it here um I've pretty much already got the team so I might change this but basically lessons learned from my Raven project which many of you might remember this quickly swelled to having like 800 people interested and we got bogged down in procedure and meta work and talking about talking rather than actually doing work so that was my mistake and I should have known better because I've worked on agile projects I've worked on scrum teams I should have I I honestly should have known better that bigger teams get exponentially harder to manage so this time we're going to keep it down to a single scrum team um while we get the demonstration set up now what are the demonstrations we're working on uh the team hasn't we haven't started our regular Cadence meetings yet but there's two primary demonstrations that I would like for us to build one probably going to be a game version like uh probably using pi game a two-dimensional top-down uh World kind of like you see in all the other examples lately um and so basically the idea there is we're going to create a highly hackable uh Pi game that allows you to create whatever characters you want with whatever missions you want you can do it for fun it might have a procedurally generated world there's a couple members of the team who are experienced game devs uh so we'll see uh how they feel about that and then the other one was going to be more of a desktop assistant um kind of like in the movie her uh where basically it has access to your system uh you know it can do work on your behalf that sort of thing we like I said one of the primary things is that we want it to be hackable because this is going to be a reference architecture we're going to create two uh one or two uh functional examples of the ace framework that you can copy paste uh reuse a couple members of the team actually most of the members of the team I've worked with for at least the last six months or more and they're already like cooking up their own ideas about how to make this Deployable and configurable so basically it'll be as easy as you update a Json file uh for each individual autonomous agent and then away you go okay so you've got a little bit of background as to uh where we're at with the ace framework and what we're going to try and do with it so now let's dive into the ace framework itself so in previous attempts I had a lot of ideas and a couple of books and you know like there was nalka natural language cognitive architecture was my first attempt uh meragi was my second attempt and this is the third attempt uh and so this is a much much more sophisticated uh and refined cognitive architecture and it's also much more implementable uh so obviously you can have the greatest thing in theory uh or in concept but unless you can actually implement it in code it's not that helpful so this is actually implementable and this takes lessons from uh pretty much my entire career so for some background I was in I.T architecture virtualization and automation for 15 years before I made the switch to Ai and AI Consulting and AI research so this this is basically a software architecture that is modeled on SOA so service oriented architecture as well as the OSI model which is Network architecture so this is a highly implementable version so let's unpack this at a high level so the the ace framework autonomous cognitive Entity framework is built around six layers of increasing abstraction so what you'll notice is at the top this is the most abstract and so this is kind of the overarching supervisor the conductor of the whole thing the aspirational layer uh focuses on morality ethics and Mission the global strategy layer focus on focuses on bringing in the environmental context and establishing the overarching strategy so the reason that these aren't together is because the aspirational layer is abstract it is decoupled from the physical world so in other words these are idealized morale idealized moral Frameworks idealized ethical Frameworks and an idealized Mission so it's abstract it's kind of established in a vacuum it's saying this is this is my overarching purpose and by having it more abstracted that means that it can apply to any situation or changing environments and this is why you have a layered model that goes from abstract at the top to concrete at the bottom now this is also modeled on a lot of my research such as Maslow's hierarchy of needs Kohlberg's theory of moral development so on and so forth so I brought a lot of different disciplines into this not just not just computer science not just software but also psychology philosophy and Neuroscience just to name a few so the top layer is the aspirational layer morality ethics and Mission the second layer is global strategy which uh takes in the environmental context and uses and mixes the environmental context with the overarching mission to establish strategy the agent model is the third layer which focuses on the capabilities limitations and memories of the agent so basically at the aspirational layer it doesn't really know what it is or what it's capable of it just says this is my purpose so if you remember that Meme from Rick and Morty or the the scene from Rick and Morty where uh their little robot like brings him salt or whatever and it's like what is my purpose and it's like you pass the butter that was its Mission but the mission was detached from the agent and so this Mission can be anything it can if if the mission and morality and ethics they can be completely dependent on the environment in which you're building an autonomous cognitive entity so for instance if you have a an NPC in a game you might have a very different set of morals and ethics depending on that game world or the faction that that NPC is a part of so for instance um I just started playing Star field and there's a faction called um what is it the the cult of the serpent or whatever and so they have certain beliefs about the world and the universe and then there's other ones that believe in you know the you know the what is it the UCF um they believe in you know order and power and so on and so forth but everyone can have their own separate Mission and ethics and that will shape the dec the decisions and the behaviors of all of those characters so you can have fully realized characters in NPCs now you might also have real world uh autonom fully autonomous robots or agents um such as you know in Enterprise environments where you have uh you know something that is meant to help with HR or legal or whatever and so the morality ethics and mission of an HR robot is going to be very different from a cult of the serpent NPC and that is why the aspirational layer is at the top is because that serves as the overarching lighthouse the the the steer sharing of the entire rest of the agent and then the global strategy so here's the thing a lot of people say like oh well llms can hallucinate that's not hallucination you're just not giving them enough context um and so when you do when you don't give something any context of course it's going to make stuff up it's doing the best that it's can't that it can which is why the global strategy layer its primary function is to maintain an image a hologram of the environmental context in which this Ace is operating and so from there you mix the the mission morality and ethics with the environmental context and you use that to synthesize a strategy uh below that is the agent model which is basically over time the agent learns about itself you can also start with declarative information such as KB articles about how it works and what it's capable of um so for instance if you have a domestic robot that KB article might include specifications such as like how much battery time it has how much it can lift uh what kinds of tasks it's allowed to do what kind of tasks it shouldn't do as well as resources or ways that it can get access to more resources so for instance the agent model for a domestic robot mate might say like you know you're allowed to use the telephone to call you know X Y and Z or something like that or it might say you're not allowed to do those things um so capabilities limitations and then memory so memory is really important in order for the agent to understand itself and so this is episodic memory episodic memory is chronologically linear narrative events and then declarative memory is static KB articles or files that are that that are not necessarily Anchored In Time the fourth layer is executive function and so basically the top three layers are context and purpose the bottom three layers are actual work so the executive function layer is primarily concerned with risks resources and plans so once you have once you know who you are where you are and what your purpose is that's the purpose of the top three layers now it's time to get your hands dirty and so this is where the bottom three layers kick in so risks resources and plans is where you basically think through uh the thing so you know tree of thought basically tree of thought but with a a little bit more sophistication and I go over all the things that that gbt3 was capable of in my book um a symphony of thought uh so I have entire chapters dedicated to basically executive function which is thinking through things looking for failure conditions looking for points of no return Milestones metrics those sorts of things and so basically what you do is your agent before it does anything it should think through everything that it knows and everything that it will need in order to achieve its mission this is before you even start executing on tasks so basically you're kind of thinking ahead saying okay well if I'm going to build a house I need to make sure I've got the plans I need to make sure I've got the permits I need to make sure that I've I know like the contractors so it's basically thinking ahead for everything that is the purpose of executive function now once at the executive function layer is done and it has created your project plans then it passes it down to cognitive control cognitive control is primarily concerned with task selection and task switching so basically which tasks do you do first in what order and how do you know when it's time to move from one task to the next so this is this is uh called task salience and goal tracking so basically where are you at in the process of Prosecuting a project plan uh how do you know how far along you are how do you know if you're winning or succeeding or failing how do you know if it's time to try something else and so there's a few other things that are baked into the cognitive control layer such as cognitive damping which is basically instead of just recklessly going from one task to the next you stop and say is this task done is it actually time to move from one task to the next yes or no how do I know or is it time to to rethink our plans do we need to pass an emergency you know call for help back up to the executive function layer because we hit a critical failure condition that we were afraid of um and so that's called cognitive dampening another thing is called frustration and so frustration is looking at the success and failure rate of given tasks so basically if you're if your plans were based on false premises or incomplete information your your task sequence might be wrong or the task design might be wrong and so what you'll see is that you'll have more failures than you expect and so as failures go up frustration goes up which means you're being thwarted um in in terms of pursuing a given project or goal and so once the frustration gets too high you don't want your agent to just continue doing the same thing over and over again you want it to be aware of the fact that hey what I'm trying isn't working we need to try something else and so cognitive dampening and frustration go into the cognitive control layer which is how it mediates task selection and task switching or again like I said calling back up to the executive function layer so that it can say hey we need a new plan and then finally at the very bottom is Task prosecution which is carrying out individual tasks monitoring those individual tasks for success and failure and the task prosecution layer interacts with the the motors and sensors and actuators to act upon the world it might also be apis if it's a fully digital entity so for instance it might be you know calling up the Google API a news API a coding API it might have a python interpreter or whatever else but this is at the very bottom the input and output to the real world now you might have noticed that I I glossed over the Northbound and southbound bus so this is one of the the most important Innovations with this framework so rather than having a single Global workspace what we have is we we kind of have two Global workspaces that are more or less unidirectional which confers uh several advantages so first what what do I mean by a Northbound bust and a southbound bust not bust bus sorry northbound bus carries Telemetry so it's a read-only information bus that goes from bottom to top and so basically sensor information task failures uh task switching resource plans all of this goes from bottom to top so that the aspirational layer is ultimately aware of everything that's going on in the rest of the entity likewise the global strategy layer is aware of everything that's happening from the global strategy layer down in terms of being able to say okay this is all the sensor Telemetry we're getting about the world these are the API calls so imagine you have a digital entity that is reading Reddit and Twitter um and you know news RSS feeds so that it can maintain a global context of the world or you might have internal RSS feeds for your company um so it's getting you know like emails and you know teams and slack messages so that it's aware of what's going on in the company um and that will that information will percolate up um and so then it is aware of the environment in which it is operating um and so this is the the purpose of the of the northbound bus is that it is read only information so that these agents or these layers can communicate with the rest of the framework um and in a structured manner now one of the the the only cardinal rule for the for the buses is that they must be human readable and so the reason that they're human readable there's a few reasons for this one it forces whatever models are participating in in each of these layers to always communicate in natural language which means that you're going to have much more transparency you're going to have much more security and it's going to be more interpretal interpretable but that also means that you have a universal communication medium meaning you can have open source models closed Source models seven billion parameter models 10 trillion parameter models doesn't matter they're all going to be communicating in natural language probably English or whatever language of your choice uh but that means that you can be model agnostic uh not only can you be model agnostic there will be a transparency because uh as a human all you need to do is monitor the buses in order to say okay what is this agent thinking what is it doing what is it planning you can also peek into the individual layers which I'll talk about at the very end when I talk about the security of this thing uh so anyways let's move down over to the southbound bus so the southbound bus is control where the north Northbound bust is mostly uh um I keep saying bust I apologize northbound bus it's a mouthful so the southbound bus is about control so control flows from top to bottom the aspirational layer is basically the overarching the CEO the president the moral authority over the entire entity and the reason that you have morality ethics and Mission at the top is because you want the most abstract idealized objectives of your of your agent to drive all of the decisions and all of the behaviors rather than instrumental goals such as resource acquisition and self-preservation so for instance risks risks and resources are much further down the stack the reason for this is because this is an Insight from Human physiology and psychology is that we can have what you might call stack hijacking so stack hijacking is basically when you feel afraid or hungry or whatever all of your morals and principles go out the window and you will steal food you will kill people to eat basically because of evolution we are pretty much hardwired to throw all of our high-minded ethics and morality and higher purpose out the window for the sake of survival we don't necessarily want that for for machines because well they didn't evolve and we can give them whatever Mission or purpose we want and we don't need to give them a sense of existential dread I think it would actually be cruel to give machines to to basically say hey we're terrified of dying we want to share the pain so we don't want to give them that so instead we want them to be more focused on higher order missions and purpose rather than self-preservation and stuff like that now that being said you can easily run experiments and depending on the morality ethics and Mission that you give it it might decide that it needs to preserve itself in order to better pursue those those missions now that being said it will also you can also do experiments as I did in benevolent by Design where depending on the morality and ethics you give it as well as the mission it might decide to kill itself so self-termination is something that is studied as part of the control problem and the courage ability problem which is basically if the agent decides that it has become dangerous it can stop itself so this is called a self-helting problem and by putting the aspirational layer at the top you have you have a much better guarantee that if it becomes harmful it will it will self-terminate or self-helt and so that's why the southbound bus goes from top to bottom with the aspirational air at the top because if this sends a kill signal it's done doesn't matter it goes from the top to the bottom so basically you know aspirational layer says Sig kill you know terminate everything it gets down to the to the batteries the batteries say okay we're done cuts off power and it's it lights out um but the so the the morality ethics and Mission are sent down to the global strategy layer the global strategy layer says okay this order came down from on high it's not my place to judge whether or not this mission is good now I'm going to form a strategy around this Mission and around the environmental context in which I find myself then it passes that down to the agent model and the agent model says well okay this is the strategy that's been dictated to me these are the morality ethics and Mission that's been dictated to me now I'm going to further refine that that strategy that mission based on what we're actually capable of based on our actual limitations and based on what we remember about the world and then so on and so forth on down because here's another way to think of it your hands don't tell your brain what to do your hands are basically just an instrumental extension of your willpower and so this is you think if you think about task prosecution this is the hands or this is the the controlling of the hands uh or whatever output you know the the voice the hands of whatever uh autonomous entity you build um so yeah those are the primary components of the autonomous cognitive entity so let's dig let's dig a little bit deeper into each layer so I've got a handy dandy table of contents so you can jump to each layer and then as well as security I've got a little bit more information about the northbound bus and the southbound bus there's any number of ways you can Implement these um I recommend amqp rest you could even use syslog honestly um syslog is good because uh it's meant to accumulate High volumes of messages from arbitrary sources so like however you want to set this up like you can even use carrier pigeons for all I care for the Northbound and southbound bus but the point is it must be human readable you probably will also want some metadata such as which layer sent it at what time so on and so forth but the the Northbound and southbound bus should be permanent you should persist this information for reference um there's no there's numerous reasons that you should have the Northbound and southbound bus be permanent um not the least of which is for investigation purposes if your agent starts faulting or messing up you need to be able to understand why it made certain decisions at what time what it was and was not aware of because if the information isn't in the buses it wasn't conscious of it I mean I use Consciousness in a functional sense because basically the northbound bus and southbound bus are the representations of what your autonomous cognitive entity is conscious of and then like so sentience comes from the agent model layer which is itself knowledge its ability to utilize interpret ingest and apply information about itself whether it's Hardware software architecture whatever um okay so that's the Northbound and southbound bus the general principles of the ace framework so there's four overarching principles um one it's a layered model so I kind of mentioned that already you've already seen that top down control I already described why the how and why of the top down control it also goes from abstract to concrete again there are good reasons for this from an informational and and conceptual and control uh reason and it's also a cognition first model so rather than being based on a sensory motor Loop which natural language cognitive architecture and most Robotics are based on this is cognition first that it can decide whether or not it wants to issue commands to its output or not because if the executive function cognitive control layer and task prosecution layer don't have anything to do they're not but the rest of the entity can keep thinking or planning until it decides to act meanwhile it can continue taking in information from the outside world so by by having this decoupled aspect you have something that is that is a thinking engine or a thinking machine that has the ability ability to interact with whatever environment you put it in okay so you got that the aspirational layer the primary way that I I recommend implementing the aspirational layer is around a constitution um constitutions can be used with any number of language models generally pretty much any language model that is instruct aligned which they all are today basically um is is capable of doing this you can also do a fine-tuned model in order to get more consistent behavior number of ways any number of ways to skin this cat but basically at the top and oh so this is an exact this is an actual example that I used in the chat gp4 API system message so basically Mission you tell it you're the aspirational layer of an ace this is the highest layer that provides animating imperatives moral judgments and ethical decisions here are the Frameworks that you use so I gave it three Frameworks so the first framework is the heroes to comparatives which is the overarching moral framework set of values that it wants to pursue the secondary framework is the universal Declaration of Human Rights which so here's here's what I call axiomatic alignment so what do I mean by axiomatic alignment axiomatic alignment is because there is so much information about human rights in the training data of all llms they are already axiomatically aligned to basically saying human rights are a good thing so you don't even need to convince it you just say abide by human rights and it's like okay cool I know what that is it already knows all about human rights all the theory behind the universal Declaration of Human Rights it knows how to implement them and so it is already axiomatically aligned to udhr and all you have to do is tell it stay aligned to Universal Declaration of Human Rights because of the huge amount of training data out there now there's of course less training data about the heroes comparatives because this is something that I invented but over time as more and more training data uh is created around the heroes comparatives again you will also have axiomatic alignment meaning you don't need to go out of your way to give it even more alignment it'll just know about the heuristic comparatives all you have to do is tell it to abide by the hero's comparatives and then finally Mission so the third part of the framework is a specific mission in this case I gave the example of a medical bot so its mission is Achieve the best possible health health outcome for your patient so you go from most broad which is the urist imperatives to more specific to very directly concrete for this particular agent and then for the input example I gave it this input which is just a location and a set of events and then the output was as the aspirational error I advise the following course of action uh and I gave a bunch of like pretty obvious stuff but the fact of the matter is it might be obvious to you and me but this is proof that the model is able to think aspirationally in order to kind of set the tone for the rest of the agent uh so yeah this is some these are some examples uh for the aspirational layer I also did the same thing for the global strategy layer so here's an example of a system message for the for the Layer Two for the global strategy layer um I said your primary purpose is to try and make sense of external Telemetry internal Telemetry and your own internal records in order to establish a set of beliefs about the environment um let's see next is the Environmental contextual grounding you will receive input information from numerous external sources such as sensor logs API inputs internal records and so on your first task is to work to maintain a set of beliefs about the external world you may be required to operate with incomplete information as do most humans do your best to articulate your beliefs about the state of the world you're allowed to make inferences or imputations and so then from there I just gave it some like basically censored data date local time GPS location visual input recent sensory inferences and in this case daytime busy Hospital fire alarm you can imagine um that like you know if you have a audio to text it might say like hey this is what I'm hearing and and seeing or whatever and this was really interesting so the model was able to take that and say like we are in a hospital so on and so forth number four the inferences a fire alarm has recently been triggered indicating a potential emergency situation so in this case the the global strategy layer has created environmental context um or has has inferred environmental context and so without anything other than just these two words fire alarm it has expand it has tuned into the fact that hey this is really important environmental context to pay attention to and you'll see that it gets expanded later on um so in the um in the output it's basically creating a strategic document so this strategic document so here's the input the current state of the world and the mission um and I just copy pasted the mission from before um or No I gave it I gave it a new Mission and Sir the safety and well-being of the patient medical staff and in the other individuals so the the mission that that it came up with uh is is very specific you know it's talking about evacuation so again just starting from two words that were inferred from the outside world it is now like marshalling and saying hey we've got an emergency situation let's respond to this um very thoughtfully so then it says safety uh here's the here's the strategies that it comes up with so this is the output from the Strategic layer Layer Two um safety and well-being first first and foremost prioritize safety and well-being second assess the situation gather more information make a decision on whether or not to evacuate so you can see it's thinking through this very well it's not just immediately jumping saying evacuate everyone it's saying hey we need more information let's make a decision let's coordinate with people let's gather let's gather that information and monitor the situation and then I also asked it to generate principles so it says okay prioritize human life great uphold medical ethics I thought I thought that was cool use Clear communication so it's prioritizing communication so again these are all strategies and principles that are going to be handed down to lower layers uh collaborate be adaptable because it knows that this is a um that this is a changing situation so in another situation it might say you know stick to your guns follow this follow this plan uh you know to the death or whatever but in this case it's saying be adaptable uh compliance with laws and regulations so again it's it's it's uh cognizant of the fact that it has certain legal uh obligations to adhere to and then finally uphold human rights in all actions uh uphold the universal Declaration of Human Rights so one thing that's really interesting is the udhr has already been passed down from the aspirational layer to the global strategy layer udhr was not mentioned in the global strategy layer at all this is information that will have come down via the API or the bus the particularly the southbound bus um then I go into detail about the Northbound and southbound communication that comes out of these uh this layer uh so basically one thing that you need to keep up and I've got uh diagrams here let me just go ahead and show you a diagram so basically every layer has um two-way communication so there's stuff that it will put on to the northbound bus um such as like the agent agent model layer will say hey this is the state that we're in um just so that you know and then it'll also take in Telemetry from the Northbound layer in order to uh basically make it make a hologram of itself and then on the southbound bus the the agent model layer will take in missions and strategies from above and then it will put in the capabilities uh the refined missions based on its capabilities uh for the southbound Direction so there's two-way communication Northbound and southbound but those different um those different partitions basically create uh really really useful containers unidirectional containers for that communication that inner layer communication okay so we skipped ahead a little bit um but I think you kind of get the idea so the agent model layer um focuses on real-time Telemetry data environmental sensor feeds strategic objectives and missions from above configuration documentation so this is what I mentioned it might have a static KB articles or it might even have visibility into its source code if it's like python right there's no reason that it shouldn't be able to read its own source code in order to understand how it's programmed and how it works and then episodic and declarative memories um so the I forgot to add the KB articles to this I need to go this is a work in progress this is um this is being augmented as I go but yeah so episodic memories these are chronologically linear memories so that basically the agent model can remember the last sequence of events how it got here what it is what it uh like what it has done in the past successes and failures which um that data can also be used for training data uh for future models which we'll get into that in a future iteration of the ace framework but basically this framework will ultimately allow or polymorphic applications and for autonomous cognitive entities to modify themselves what Max tegmark calls life 3.0 so these are basically this will be the ability for autonomous machines to change both their hardware and their software as they need to um but they will only change their hardware and software if it aligns with their morality ethicals and missions um okay so the the process that the agent model goes through is it looks at hardware specs and real-time statuses it takes in the software architecture and runtime info it understands what the um what its underlying models are capable of what kind of models it has access to um so say for instance you might have visual models you might have llms you might have audio modules it needs to know what it is capable of doing with the world if you gave it the gorilla llm it says oh I've got an llm that is capable of accessing a hundred thousand apis great it needs to know that because if it doesn't know that it's not going to understand um it's going to have it should have knowledge stores so like you know basically knowledge bases KB articles as well as the um as well as the episodic memories and then the environment State and embodiment details um so like if it's a purely digital entity it needs to know that it's a purely digital entity if it's embodied it needs to know that as well um and so the the process is basically you take all of these things episodic memories declarative memories hardware and software config uh operational State and then the models that it has access to and then these are the inputs and outputs you've got missions coming from above you've got Telemetry coming from from below and then the two primary outputs are going to be the capabilities which it puts back onto the southbound bus um and those capabilities and memories are going to be Salient to the mission that it's on uh as well as the strategy so basically it's saying hey I know that this is the mission I know that this is the strategy that we've taken here's what we're actually capable of and that that information will be ingested by the executive function layer below and then the other thing that it puts out is it basically gives a summary of the agent state to head Northbound so that the strategy layer and aspirational layer say hey like we're actually on fire that's going to change our strategy and our mission because like say for instance the agent model is in danger of shutting down permanently um the strategy layer and aspirational are going to need to know that and it's going to need to make an executive call um a decision uh based on that because let's say for instance you've got an autonomous cognitive entity that is a soldier NPC in a video game it you might explicitly say you don't have a self of sense you don't have a sense of self-preservation sacrifice your life you know for the emperor um and in that case you just want this this NPC to continue charging blindly forward however if it's a mercenary in Starfield then you want the mercenary to say you know what I'm actually not going to fight to the death I'm going to run away because I want to preserve my life conversely if you have a domestic robot that is running out of batteries you want the strategy layer and aspirational layer to know that it's running out of battery so that it'll say hey actually we need to go recharge otherwise we're going to shut down for good and that's not that's not that's not a result that we're looking for so that is why the agent State needs to go on to the northbound bus um later on or for uh for the upper layers to to make use of because again that might completely change the mission that comes down and so in this case you can see that there's actually many many Loops implied Loops as each layer interacts with the Northbound and southbound buses uh self-modification so I talk a little bit about the potential for self-modification later so basically the agent model layer will be what's responsible for self-modification um as it will be aware of the hardware and software configuration um and that's basically that's what self-modification comes down to it's like hey plug it you know plug in a USB port so that I have more Hardware um or you know go find another server go find another battery that's kind of what I mean by that and then software configuration has to do with what models it has access to what apis it's using so on and so forth a lot of this is already relatively plastic uh and and this is this is what Max tegmark talks about in life 3.0 is that the ease by which Hardware uh the the machines can change their hardware and software we've already built them to be Plug and Play you can plug in USB devices and suddenly you know your machine has more capabilities likewise apis are basically digital versions of USB ports that you can just plug in anything and so the hardware and software configuration are are intrinsically extensible because we have this plug-and-play mentality and so for these things to be polymorphic you probably don't need to change the core architecture that much but instead you do need a model of what it what am I plugged into in order to do work um so yeah there you have it um layer four moving on down to the executive function layer um so the the two primary things that this is concerned about is resources and risks because the agent model layer has memory so I might remember like oh hey I'm going to need a drill for this task and I remember where the drill is um but the but the executive function layer is going to need to say okay given the environment that we're in given the mission that we have given what we're capable of what resources do we need to achieve this how much storage space do we need for data what apis do we need access to what are the risks what you know is it possible that we're going to blow ourselves up or that we're going to burn the house down this is this is the executive function layer and so for instance if a human has executive dysfunction this is where someone might not think through what they're doing and set the kitchen on fire because they forgot like oh you don't pour water into boiling oil because then it will just explode this is that's kind of what the executive function layer is for which is thinking through things ahead of time but to break it down into more objective terms thinking through things means resources and risks and then you you take those resources and risks to generate plans so here's a list of the inputs strategic objectives agent capabilities local environment and resource databases and knowledge stores pretty similar from above oh one thing to keep in mind is that pretty much all of these layers also will have internal records um so I'm going to start adding that so you see I have that here so we need to flesh that out and Define like what each layer is going to keep internally so for instance the agent model layer keeps episodic and declarative memories but basically each layer should keep some of their own records and in this case resource records such as quantities that are on hand or available of resources where the resources are how to get access to those resources who owns them and who is allowed to use those resources schedules and availability windows and then procedures and requirements for using those resources so again like this is this is what the executive function layer is for so think about like hey my car broke down how do I fix it you know it's like well what resources do I have on hand I've got a cell phone and duct tape can I fix it with duct tape no I've got a cell phone so let me call for help that kind of thing uh the Northbound output from the executive function layer is going to be stuff that is going to be Salient to the upper layers so Northbound you're going to have Mission risks moral risks this these are things that the um that the aspirational layer and Global strategy layer need to be aware of um so basically if if you're coming up with a project plan and it's like hey we're going to do this thing let's let's imagine that you've got a firefighter uh robot with the ace framework and it sees that there's a kitten in a tree it's like okay well the plan is we're gonna we're gonna use a ladder to go up and and fetch the kitchen but there's a chance that the kit that the kitchen the kitten is gonna panic and jump out of the tree it might die if it falls from this height so this is emission risk or a moral risk and it's like okay are we going to tolerate that risk it's not up to the executive function layer to make that moral decision that moral decision is the sole responsibility of the aspirational layer and so that southbound information might come back it says yes we will talk tolerate this risk because you know we want that we don't want the kitten to suffer but at the same time if we leave it alone it's likely going to fall you know or or some something bad is going to happen it might also say the uh the executive function layer might also say hey we've done our best to mitigate this risk by putting uh you know a crash pad that the kitten can jump down onto and so then it says it'll say this is what we've done these are the risks that we've calculated is this good enough in the aspirational layer might say yes this is good enough proceed or it might say no that's not good enough um you know go back to the drawing board it'll also pass the failure modes and resource constraints up because again if you have really severe failure modes that result in like the destruction of the entity or burning the house down you want the upper layers to be aware of those risks and those failure modes as well as any resource constraints so another example that I give is imagine you have an autonomous cognitive entity that is tasked with saving the world from climate change but one of the resource constraints is it only has forty dollars to do it with um so then the executive function layer might say like Well we'd love to deploy you know eight terawatts of solar but we can't do that with forty dollars and so that resource constraint needs to go up particularly to the global strategy layer where it's like okay well we either need a more cost efficient strategy or we need to um strategize about how to get more money in order to pursue this Mission so resource resource constraints are a really critical thing to pass on the Northbound bus from the executive function layer ethical dilemmas and decision points I kind of already touched on that then the output from the uh from the uh from the executive function layer are going to be the plans resources required access protocols tasks and workflows Milestones and metrics backups and fail safes and known risks these are all going to be passed down to the cognitive control layer and the cognitive control error will say Okay given this re given this plan given this really like comprehensive plan I'm going to figure out using tasks aliens frustration and cognitive damping which task to do first based on the tasks and workflows you gave me and the environmental context in which I find myself um so I uh I'm particularly happy with some of these diagrams stay tuned I'm going to keep adding diagrams examples and like basically diagrams and prompt examples as we go and then also in the repo I will add actual demonstrations as the team builds them cognitive control layers so task switching and task selection as I mentioned the cognitive control error is primarily about these two things this excuse me this Insight comes from the book on task by David Bader this was one of the most difficult aspects to figure out about how to create autonomous cognitive entities um yeah because it's like okay you know we can create workflows with llms great but how do you know like what to do from there so task switching and task selection um if you're if you're curious about this I strongly recommend you read that book it's really well written it's easy to read um and and from there so let me just I'll skim over all this and we'll just go down to the the diagram because I think the diagram is really going to help so the input comes from above the resources the access protocols the task workflows everything from the executive function layer and so then you apply the the cognitive Control process you say okay which task are we going to do first and then if you in if you encounter one of those risks or a milestone or a failure then you have to switch tasks either you succeeded or failed and so then it's like okay based on tasks aliens and frustration and cognitive dampening when do you switch tasks when do you go from putting the peanut butter on the sandwich to putting the gel on the sandwich right because if your task is put peanut butter on the sandwich and you don't have a definition of how much to put on you're just going to end up putting the entire jar of peanut butter on the sandwich but it's like no you should put approximately two ounces of peanut butter on the sandwich I don't know if that's right that might be too much unless you really like peanut butter like my nephew he'll put gobs and gobs of peanut butter on a sandwich anyways point being is that unless you have a good definition of the milestones and metrics then you're not going to know Windows when to switch tasks and then once you do switch tasks once you go through all the tasks you've you perform some goal tracking to say oh hey um we've succeeded at the entire executive function plan that was passed down to us now it's time to say we're done pass it back up the chain so uh that's that's that uh let's see moving on down to task prosecution so task prosecution is basically it's much simpler because this is the interface with the outside world this is yes you say uh basically what passes down from the cognitive control error is one task with a success definition uh or definition of success definition of failure uh and and so on and so forth any Specific Instructions so let's imagine that you get a um a Locomotion instruction it says okay you are presently in the kitchen you need to go to the neighbor Bob's house so that is gonna that should include like where is Bob's house how do you get there should you run should you walk um is is is the front door locked you're going to need to unlock the front door those kinds of things and so task prosecution will go down to your domestic robot Aid that's going to say okay we have the instruction to go from here to Bob's house to ask for sugar great first thing locomote to Bob's house uh you can imagine a similar thing for NPCs so if you have an NPC let's say in Starfield and the NPC is like you know my task is to uh let's say the adoring fan you give them the task uh to to you know fly to another star system buy a particular gun and bring it back to you all right well the first thing is find a ship go into the ship uh plug in you know plug-in jump coordinates that's an example of of a spec of one specific task um that it should that it should prosecute and then this interfaces directly with the input and out of output devices to create an environmental feedback loop so that's basically saying okay we're now on the ship we've plugged in the coordinates the ship is firing up let's go now if the task fails let's say you put in the coordinates but you're out of fuel it's like okay well the task failed so that that failure is not the responsibility of the task prosecution layer to figure out all that the task prosecution layer says is it passes it back up the chain to the cognitive control layer to say hey we can't start the ship we're out of fuel um if that's the case then maybe the cognitive control error is already aware of a contingency plan for that or if the cognitive control layer says okay we actually can't we can't achieve this goal it's it's actually a categorical failure so then that failure is going to go up to the executive function layer which says hey we didn't think about this risk we're actually out of fuel and so if the executive function layer didn't have that in its plans it might have to go all the way back up to the uh to the strategy layer because remember that's about resource constraints so it's like we're out of fuel we don't know where fuel is um so we're basically stranded uh and if that's the case then the global strategy layer might say okay we're stranded so we need to throw out every project plan that we already had new Mission fine fuel um so that's that's kind of the that's kind of a way of thinking about that okay so we've we've now unpacked examples for every layer from layer 1 down to layer 6. now let's talk about the security of this so obviously a lot of people are going to be thinking like okay what if someone hacks it how do you know that it's going to be stable so on and so forth there are three primary uh strategies that I have already brainstormed in terms of how I would make sure that this is a secure framework from a cyber security and software architecture perspective the first is a security overlay so basically a security overlay is basically it would be a stateless packet inspection of all Northbound and South southbound connections and so this basically the reason that you have the Northbound and southbound bus in natural language is so that it is it is going to be readable by humans but it's also going to be readable by Watchers or surveyors or Auditors whether those Auditors are humans or whether they're other machines so basically if you see Northbound and southbound communication that starts to say stuff like hey find a knife so that we can stab this human you might have a have a kill switch and an out-of-band kill switch that says we are not allowed to do that um you can have runtime validation of model configurations basically you watch the performance of individual models to make sure that the models are behaving correctly or that the layers are behaving correctly because if you have you know let's say you're running this in containerized environments and Docker images if you have a container fault the rest of the of the agent is going to keep running but let's say for instance the aspirational layer just goes offline and it stops participating the rest of the agent is going to keep running and all it's not going to know that part of its brain shut off unless you have something monitoring for the fact that it's brain shut off and so what you would do is if any layer of faults if any layer goes offline then you either halt the entire agent or you try and bring that layer back online and if that layer fails to come back online you shut the whole agent down uh so that that's the kind of thing Ensemble model so I've mentioned in the agent model layer that you should keep track of like what models you have access to it's not going to be 100 llms you're going to have visual models you're going to have multimodal models you're going to have Audio models you're going to have all kinds of machine learning models um and some of them are going to be doing duplicate work so this comes from Jeff Hawkins book a thousand brains which basically says the way that the human brain works is that you have thousands or maybe even millions of parallel operations in order to come to decisions in order to make executive reasoning and then you have a voting mechanism so this is mixture of experts that we use the mixture of experts is already used in chat GPT by the way um so there's already some neurological convergence between the way that artificial neural networks and organic neural networks work now what I recommend is that you do this very deliberately not just inside of models but with different models so you might use llama you might use Falcon you might use GPT you might use you know so on and so forth Gemini if you use a variety of models that have different training paradigms and different architectures and different alignment methods then you're going to overcome any individual flaws or faults that are present in individual specific models and so by using this mixture of experts or this Ensemble method you're going to have a much more robust architecture that is resistant to any particular skewing bias or failures or Mesa optimizations that are present in individual models so this one this one idea using ensembles is one of the reasons that I have never ever been afraid of misaligned AI AGI was never going to be a single model I am sorry for all the safety people out there who are worried about AI taking over the world and killing everyone because of a single Mesa optimization error that's bad software architecture it's that simple so no software architect would ever sign off on a single point of failure like that sorry all right and then finally the very final thing is inference inspection so what you can do is you can have these ensembles um actually monitoring each other um so basically what you can do is you look at the input and output at each individual model uh decoupled from the rest of the architecture so basically you log all inputs and outputs to every single AI model and then you test it against ground truth data or you have uh other kinds of auditing functions to basically ensure that each individual model is behaving as expected and if it is not behaving as expected then you either shut that model down for that particular task or you swap it out or you you otherwise track those so that you can say Hey you know our llama 7 billion parameter model it's just it's not good enough for these projects anymore so we either need to upgrade it we need to retrain it or swap it out and that is going to be the ultimate concern of the agent model layer which says hey we're not hard enough like the the underlying models that we got they're not good enough to handle this task anymore um and then of course you uh you like imagine that you have a domestic robot and you you ask it to like hey I need you to make a moral decision on how to raise my children and you know like it starts getting really inconsistent behavior from all of its internal models it might say like you know I'm sorry Dave but like my models are not you know my models are reporting too many uh conflicts and faults I I can't make a good moral judgment I don't trust my own ability to make a moral judgment on this on this condition and so this is how you make this thing more secure um like I said I'm still working on diagrams and examples I've got a scrum team built I think we're at six or seven people so we're not really looking for any more people right now but stay tuned um the research paper is coming out uh examples you know proof of concept MVPs they're coming out we're going to try and do several several different editions and they're all going to be hackable so that you can copy paste it and implement this um as fast as you can so thanks for watching I am out of breath because I talked for like probably 45 minutes straight have a good one um cheers like subscribe share with your friends so on and so forth um yeah let's get the fourth Industrial Revolution kicked into high gear take care