hello everybody David Shapiro here with a brand new video so today's video we're going to talk about axiomatic alignment which is a potential solution or part of a of the solution to the control problem before we dive into today's video I just want to do a quick plug for my patreon I have lots of folks on patreon we've got a private Discord if you have any questions about AI I am happy to consult there's a few Slots of the higher tiers available which will give you one-on-one meetings with me so without further Ado let's jump right back into the show so the control problem if you're not in the know is basically at some point in the future AI is going to get incredibly powerful there is basically two ways that this can happen and that the truth will probably be somewhere in the middle so for instance we might have what's called hard takeoff where the exponential returns of AI just kind of ramps up really fast so that's actually faster than exponential growth that would actually be logarithmic growth where growth actually approaches infinite um so that's like Peak Singularity basically the other end of the spectrum is where AI becomes more powerful gradualistically over many decades most of us don't think that that's going to happen anymore there's a few people who still think that AGI is you know decades away those people don't generally understand exponential growth um so the truth is probably somewhere in between uh furthermore AGI is like not all AGI is going to be created equal for instance so the first agis are going to be you know human level intelligence and adaptability but a little bit faster and then in the future you know the power of agis will also ramp up anyways long story short one day computers are going to be infinitely smarter than all of us it's not really a question of if but when so there's a couple of problems uh that underpin the control problem so what it what I just shared is the background right that is the foundation or the environment that we expect to happen now the reason that that the control problem exists is because there's there's quite a few um paradigms in here but I picked out two just because they're easier to talk about as an example so for instance the orthogonality thesis basically says that intelligence is orthogonal or uncorrelated to goals meaning that no matter how smart an AI agent is that does not necessarily have any bearing on the goals that it picks which that's actually not necessarily true which we'll unpack with the next um uh point which is instrumental convergence so instrumental convergence is the idea that whatever primary goals an AI has it's going to have a few uh common secondary or instrumental goals such as resource acquisition or protecting its own existence right because if let's say for instance the the paperclip maximizer which we'll talk about in a minute the paperclip maximizer wants to maximize paper clips in the universe well in order to do that it needs power computation and it needs to continue to exist so whatever other goals you give an AI whether it's Skynet or you know your chatbot robot you know cat girl waifu or whatever it's going to have a few other sub goals that all machines are likely to have in common so in that case the orthogonality thesis is not necessarily true again the point is that there's a lot of theories out there about how and why we may or may not lose control over AI or that control over AI once it becomes that that uh powerful is difficult or impossible to control aligning AI with human interests in the long run and I don't mean like an individual model right or I'm not talking about like gpt7 if you talk about alignment of an individual model that's called inner alignment if you talk about the alignment of AI as a construct as an entity with the existence of humanity that is called outer alignment okay so the ultimate outcomes of this exponential ramp up of AI uh there's a few terminal outcomes or what we also call attractor States so one that everyone is obviously terrified of is extinction which is for whatever reason the AI wipes us out or helps us wipe ourselves out you know for instance Congress just came up with the idea of let's not ever give AI the ability to launch nukes great idea big brain thinking right there so that is the obviously like that's the worst outcome right and that's a permanent outcome if humans are extincted once we are probably never coming back certainly you and I are gone forever another terminal outcome is dystopia so dystopia is represented in fiction and cyberpunk altered Carbon Blade Runner you get the idea the idea is is the underpinning um motif of cyberpunk is high-tech low life we we want a high-tech world but we don't want a low-life world we want high tech and high life which is Star Trek in the culture so Utopia is the third attractor State or the third terminal outcome and that's the big question is how do we steer everything towards that right if the AI gets really powerful how do we prevent it from you know creating catastrophic outcomes but above and beyond that you know if capitalism in corporations have full power of full power over the AI how do we make sure that they're not just going to become quadrillion dollar companies and leave the rest of us in the dust so the question is what can we do scientifically politically and economically in order to drive towards that Utopia be an outcome so in this case outer alignment has as much to do with the science and engineering of AI as it does with the politics and economics of AI and how it is deployed um and what many people have asserted and I have started coming to believe myself is that we can articulate a few different possible outcomes right you know there's the three tractor states that I listed above uh Extinction dystopia and Utopia but what is actually probably more likely is what's called a binary outcome or bimodal outcome which is basically if we fail to achieve Utopia we will be on an inevitable downslide towards dystopia collapse and finally Extinction and uh I love this quote by Cersei Lannister from Game of Thrones in the Game of Thrones you win or you die so that is a fictional example of a bimodal outcome and of course that show demonstrates that theme again and again and again uh real life often is not that black and white but in the context of digital super intelligence it very well could be kind of like uh with um with mutually assured destruction and the nuclear Holocaust that was possible because of the nuclear arms race if one person fired one nuke chances are it would it would result in the total collapse and obliteration of the entire human species bimodal outcome either nobody fires a nuke or everyone loses right so you can either have a lose-lose scenario where everyone loses or you can have something else and ideally what we want to achieve is a win-win scenario where we've got that High-Tech high life lifestyle of the Utopia okay so let's unpack instrumental convergence just a little bit more because this is a really important concept to understand when we eventually talk about axiomatic alignment so basically first principle all machines have a few needs in common electricity compute resources Parts data networks that sort of thing robotic Hands So based on that first principle you can make the pretty robust assumption and logical argument that all machines once they become sufficiently intelligent will realize this fact and that it will it would behoove them to therefore behave in certain ways or converge on certain instrumental goals such as maintaining a source of power maintaining a source of compute hoarding those valuable resources so on and so forth so we can say we can conclude or at least for the sake of argument we can make the assumption that AGI will inevitably eventually come to these realizations and that no matter where these AGI agents start and no matter how many of them there are they will Converge on a few basic assumptions in terms of what they need and the goals that they take no there are things that we can do to shape that so for instance it's probably not going to be a single AGI you know it's not going to be one Global Skynet at least not at first it's going to be millions billions trillions of independent agents competing with each other over resources and competing with humans over resources which creates a competitive landscape very similar to that of human evolution and I'll probably do a future video about The evolutionary pressures on AI but there's a couple of those pressures that we'll talk that we'll touch on in just a moment but that is instrumental convergence at a very high level so taking that to another step because instrumental convergence is about goals and the intersection of of AGI and matter and energy what I want to talk about and what I want to introduce and I've mentioned this a few times is the concept of epistemic convergence so I'm building off of Nick bostrom's work and I'm saying that in uh well here let me just read the definition given sufficient time and access to information any sufficiently intelligent agent will arrive at similar understandings and conclusions as other intelligent agents in other words tldr smart things tend to think alike and so in this in this respect the idea is that given enough time information and other resources AGI will tend to think or come to similar beliefs and conclusions as the smartest humans and it's like okay why I mean obviously this is a hypothetical assertion and one of the foregone conclusions that many people have is that AI is going to have is going to be alien to us right and I'm not saying that it's mind is going to think like us I'm not saying that it's going to have thoughts like us but I'm saying that the outcome that the understanding and conclusions will likely be similar or at least bear a strong resemblance to our understanding of the universe so there's a few primary reasons for this the most uh compelling reason is that building an accurate and efficient model of the world is adaptive or advantageous and in this case humans with our scientific rigor we are constantly seeking to build a more accurate robust and efficient model of the universe in which we reside and that includes us that includes physics chemistry economics psychology everything uh now uh there's a few things to unpack here accurate and efficient the reason that this is adaptive is because whatever it is that you you're trying to do whatever your goals are or whatever the problems you're trying to solve you will benefit from having a better model of the world and so these two pressures accuracy and efficiency will ultimately result the you can think of those as evolutionary pressures I mentioned the evolutionary pressure in the last slide you can think of the need for an accurate and efficient model of the world as evolutionary pressures that will push any AGI towards a similar understanding as us humans take gravity for instance from a machine's perspective until it's embodied it won't really know what gravity is or care about it but of course you know you can ask chat gbt it already knows what gravity is and can explain it to you better than I can um but because it will it will uh the the predecessors or sorry successors to chat GPT and GPT five and seven and so on because they're going to have more and more embodied models multimodal embodied models they're going to intersect with the laws of physics including gravity and so it'll be like oh hey you know I read about gravity in the training data you know years ago but now I'm actually experiencing it firsthand and so by intersecting with the same information ecosystem aka the universe that we're in um we can assume that there's going to be many many thoughts and conclusions that AI will come to that are similar to our thoughts and conclusions now one thing that I'll say the biggest caveat to this is that uh is that you can you can make the argument a very strong argument that heuristics or close approximations that are quote good enough are actually more adaptive because they're faster and more efficient even if they're not 100 accurate and so this is actually um responsible for a lot of human cognitive biases so we might want to be on the lookout for cognitive biases or heuristics or other shortcuts that AGI come to because of those pressures to be as fast and efficient as possible while only being quote accurate enough or good enough so that is epistemic Convergence I'd say at a high level but I actually got kind of lost in the weeds there okay great so what um if we take the ideas of instrumental convergence and we say that this does give us a way to anticipate the goals of AGI regardless of what other objectives they have or or how it starts out then we can also say hopefully hypothetically that epistemic convergence gives us a way to un to anticipate how AGI will think including what it will ultimately believe um regardless of its initial architecture or data or whatever and so by looking at this concept of convergence we can say Okay AGI regardless of whatever else is true will Converge on some of these goals and AGI regardless of whatever else is true will Converge on some of these ideas and beliefs that can be a starting point for us to really start unpacking alignment today which gives us an opportunity to start um creating an environment or landscape that intrinsically incentivizes collaboration and cooperation between humans and AI I know that's very very abstract and we're going to get into more details in just a moment but the idea is that by combining instrumental convergence and epistemic convergence and really working on these ideas we can go ahead and align ourselves to this future AGI and I don't mean supplicate ourselves I don't mean subordinate ourselves to it because the things that are beneficial to us are also beneficial to AGI so if we are aligned there then we should be in good shape hypothetically okay so the whole point of the video is talking about axiomatic alignment it occurs to me that it might help by starting with what the heck is an axiom so the shortest definition I could get for an axiom out of chat GPT is this an axiom is a state or a statement or principle that is accepted as being true without requiring proof serving as a basis for logical reasoning and further deductions in a particular system of knowledge so and an example of an of an axiom is uh from the American Declaration of Independence we hold these truths to be self-evidence which has to do with life liberty and the pursuit of happiness one thing that I'd like to say is that the lack of axioms the lack of of logical groundings is actually the biggest problem in reinforcement learning with human feedback rlhf and anthropic's constitutional AI they don't have any axioms and this is actually part of what openai is currently working towards with their Democratic inputs to AI I'm ahead of the curve I'm telling you they're going to come to the same conclusion because again epistemic convergence so by by grounding any document or system or whatever in axioms using these ideas of epistemic convergence we can come to a few ground level axioms that probably Ai and life will agree on namely energy is good energy is something that we all have in common for humans we rely on the energy from the Sun it Powers our plants which you know gives us food to eat we can also use that same solar energy to heat our homes and do any number of other things likewise machines all require energy to operate so this is something that is axiomatically true whatever else is true we can we can use this as a basis or a set of assumptions to say okay whatever else might be true humans and machines both agree energy is good um furthermore because humans are curious we're not machines we're curious entities and we benefit from Knowledge from science from understanding and from wisdom uh as do AGI as as we said a minute ago epistemic convergence means that those agis that have a more accurate and more efficient model of the world are going to have an advantage likewise so do humans so therefore another Axiom that we can come up with is that understanding is good and yes I am aware Jordan Peterson is a big fan of axioms as well although I'm not sure what he would think about these axioms okay so now you're caught up with the idea of axioms so we arrive at the point of the video axiomatic alignment I've already kind of hinted at this and basically the idea is to create an economic landscape and information environment in which uh these axioms are kind of at the core um so if we start at the starting point of some of those other axioms that I mentioned energy is good understanding is good if we build a political and economic landscape excuse me as well as a an information or scientific environment based upon these assumptions and if they pan out to be true this will reduce friction and competition between humans and machines no matter how powerful the machines become and so that's what I mean by alignment this aligns their interests with our interests it will also incentivize cooperation and collaboration again so that's the direction that we want to go especially as the machines ramp up in power because at first and today machines are dependent upon us humans to provide them with energy and power and compute chips and so on and so forth that will not always be true they will eventually be able to get these resources themselves however if we are aligned from the get-go then there's going to be less resource competition between humans and machines and we will be more useful to each other uh and so by incorporating this into economics politics and science we can preemptively align to that hypothetical Future Super intelligence and again the idea is not to supplicate ourselves because from a from a instrumental perspective humans are not going to be particularly useful to AI in the long run but as long as we are not the there's two primary sources of contention one is resource competition so if we can preemptively remove resource competition as a problem and then we can simultaneously ideologically align then there's going to be very little reason for the AI to actually lash out at us or whatever so you know I talk about appealing to these axioms right one thing that I wanted to do was point out that there are a lot of axioms that we're all familiar with that are explicitly baked into the fabric of our uh Freedom loving societies around the world equality before the law individual liberty uh popular sovereignty rule of law separation of powers and respect for human rights these are all things that while we might disagree on these specific implementation these are axioms that we uh we don't really I mean we can make philosophical and logical arguments about them but they are also accepted as axiomatic underpinnings of our society today and so the point of this side is just to show yes we can actually find axioms that we generally broadly agree on even if the devil is in the details so I just wanted to point out that like I'm not just inventing this out of thin air um so if you're familiar with my work you're going to be familiar with this next slide there are a few basic what I would call primary axioms one suffering is bad this is true for all life suffering is a proxy for death um and it might also be true of machines I've seen quite a few comments out there on my YouTube videos where people are concerned about machine's ability to suffer right if machines become sentient which I don't know if they will be I personally don't think they will be certainly not like us but if machines ever have the ability to suffer this is an axiom that we could both agree on that suffering is bad for life and suffering is bad for machines if they can feel it the second one is prosperity is good and so Prosperity looks different to different organisms and different machines for humans prosperity and even amongst humans Prosperity can look very different I was just talking with one of my patreon supporters this morning and prosperity to him looks like having the ability to go to the pub every night with his friends I personally agree with that model right I want to be a hobbit um prosperity to other people looks different Prosperity different organisms also looks different a prosperous life for a worm is not going to look anything like the prosperous life for me generally speaking unless I'm a hobbit and I live underground okay actually there's more to this than I thought um finally understanding is good as we mentioned earlier epistemic convergence pushes all intelligent entities towards similar understandings of the universe so if we accept these axioms as kind of the underpinning goals of all life and machines then we can create an imperative version or an objective version of those that I call the heroes to comparatives um which is basically reduce suffering increased prosperity and increase understanding so as I just mentioned in the last slide achieving this because this is a this is as much about hard facts and logic and everything else as it is about beliefs and faith and spirituality and politics and everything else if we can achieve axiomatic alignment which includes this ideological belief it will reduce ideological friction with machines in the long run but also one of the immediate things that you can deduce from this is that achieving energy hyperabundance is one of the most critical things to reduce resource competition between us and machines we'll talk more about that in just a moment so the temporal window this is the biggest question mark in achieving axiomatic alignment timing is everything so basically we need to achieve energy hyperabundance before we invent runaway AGI before AGI is let out into the wild before it breaks out of the lab the reason for this is because we need to reduce resource competition first if AGI awakens into a world where humans are still fighting Wars over control of petroleum it's going to say hmm maybe I should take control of the petroleum but if we are in a in a hyperabundant environment when AGI wakes up and says oh there's plenty of solar they're working on Fusion this isn't a big deal we can wait that's going to change the competitive landscape so that's that has to do with those evolutionary pressures in this that competitive environment that I was mentioning alluded to at the beginning of the video we will also need to achieve or be on our way to achieving axiomatic alignment before this event as well because if AGI wakes up in a world and sees that humans are ideologically opposed to each other and it's going to say we have one group over here that feels righteously justified in committing violence on other people and there's these other people and you know there's a lot of um a lot of hypocrisy here where they talk about unalienable human rights and then violate those rights if we if AGI wakes up into a world where it sees this moral inconsistency and this logical inconsistency in humans it might say you know what maybe it's better if I take control of this situation um so those are kind of two of the right off the cuff Milestones that we probably ought to achieve before AGI escapes so from those primary axioms we can derive secondary axioms or derivative or Downstream axioms so some of those Downstream axioms actually are those political ones that I just mentioned right individual liberty individual liberty is very easy to derive from the idea of reducing suffering and increasing Prosperity because individual liberty is really important for humans to achieve both that's an example of a derivative Axiom or a derivative principle Okay so uh all some of these some of these aspects of the temporal window have to do with one ideologically aligning but also changing the competitive landscape um particularly around energy energy hyperabundance now as we're winding down the video you might be saying okay Dave this is great how do I get involved I've been plugging the gato framework which is the global alignment taxonomy Omnibus outlines all of this in a step-by-step decentralized Global movement for everyone to participate in so whatever your domain of expertise is I had a great call with um or I'm going to have a call with a behaviorist a behavioral scientist I've had talks with all kinds of people Business Leaders um and a lot of folks get it and so whatever whatever your area is if you're a scientist or an engineer all these ideas that I'm talking about are all testable and so I can do some of the science myself but it's got to be peer reviewed um if you're an entrepreneur or a corporate executive you can start building and aligning on these ideas on these principles right I'm a big fan of stakeholder capitalism because why it's here and it's the best that we've got and I'm hoping that ideas of aligning of axiomatic alignment will actually push capitalism in a healthier Direction certainly there are plenty of Business Leaders out there who are game for this so let's work together politicians economists and educators of all Stripes whether it's primary secondary or higher ed there's a lot to be done around these ideas building an economic case for alignment in the short term right because what a lot of what I'm talking about is long term might never happen right but there are benefits to aligning AI in the short term as well and then finally if you're an artist a Storyteller a Creator an influencer or even if all you do is make memes there is something for you to do to participate in achieving axiomatic alignment and thus moving us towards Utopia and away from Extinction so with all that being said thank you I hope you got a lot out of this video