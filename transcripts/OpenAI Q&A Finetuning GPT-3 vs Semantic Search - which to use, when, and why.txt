morning everybody David Shapiro here with another video so this is a question that I get a lot people ask how do I fine-tune uh gpt3 on my Corpus of data so that I can ask questions um so let's talk about fine tuning versus search uh when to use which and why so as I just mentioned I get a lot of questions about QA how do I how do I train my model on legal cases or statutes or contracts technical documentation like apis or KB articles medical records so on and so forth I get this question all the time um and and basically people want to be able to ask questions against their Corpus and they think that fine-tuning a model is the way to do it that all the knowledge will just be compressed into the model and you'll get good answers you don't that's not how it works uh next question uh okay fine I guess I'll explain this and unpack it um so first we have to talk about what is fine tuning fine tuning is a type of transfer learning so transfer learning was was originally created a few years back for uh image models earlier image recommend recognition models um but it also applies to NLP tasks now if you look up like what is fine tuning good for um you'll see stuff like you know classification sentiment analysis and named entity recognition this is old school um ignore all that that is like last generation NLP we are not we are no longer in in NLP NLP is Spacey nltk stuff like that we are in nlu natural language understanding and natural language Generation Um so pretty much if it's an NLP Benchmark ignore it if it's an NLP task ignore it there are cheaper faster simpler ways of doing that if you're using a uh a large language model for named entity recognition like that's using like nuclear fusion to power your house uh it's it's super it's way Overkill you can use smaller models like Curie and Babbage to do those if they're fine-tuned but we'll get into that at a later date maybe if y'all want um so the short answer though is that you're when when you find when you do fine tuning you're teaching it a new task you're not teaching it new information you're teaching it a new task it's kind of like tweaking a guitar just so that the final performance is a little bit better we'll unpack that more in just a minute what is semantic search so in order to compare the two you need to understand both of them so semantic search is also called neural search or vector search semantic search is just easier to say but if you hear neural search or vector searched it's the same thing and the the tldr of how it works is you use a semantic embedding which is just a string of numbers that represents the meaning of the text um and it'll this it allows new uh new um next-gen databases to scale very large very fast and also to search not just with keywords or indexes it allows them to search based on the semantic meaning right the actual content the context and the topics discussed in the actual records in your new database they can scale very large they're very fast and they're also very cheap at least in comparison to fine-tuned models so fine-tuning and semantic search sound drastically different they're entirely different Technologies the only similarity is that at some point they use semantic embeddings or or vector embeddings that's the point fine-tuning for QA is like using a hammer when you really needed a broom it is the wrong tool for the job it's that simple so how did we get here though why does everyone think that you can use fine tuning to teach it new information and that you should then be able to do QA just with a single model this is probably the biggest misconception about fine tuning because when you think like oh well gpt3 was trained on Wikipedia and now it knows all of Wikipedia so if I just fine tune it with a little bit more data then it should know that new data too right no wrong the reason is because it is transfer learning and so it's only unfreezing a small portion of the model it's not actually retraining the entire model there's a few other uh problems where uh you and we'll get to this in the next slide um where fine tuning does not overrule uh confabulation or hallucination it's the same thing um so basically what what you're doing is think of it like if you learn to tie your shoes you can tie pretty much any string that is an example of transfer learning you're not learning any fundamentally new physics you're not learning how to do anything different with your hands you're just saying I can tie my shoes now I can tie a string or if you can stack a wooden block then you can also stack bricks right that is an example of transfer learning where you take something that you've already learned to do and then you're just applying it to a slightly different task so fine tuning is just new tasks it's not new information it's not new knowledge just a new task fine-tuning is not the same as taking the large language model back to school to do that you have to unfreeze the entire model which is ludicrously expensive and even then it doesn't get rid of confabulation and hallucination so these are just fancy uh anthropomorphic terms for gaps and llm abilities I talked about this in another video the short the short version is um llms on their own do not have an episode epistemology they don't have a theory of knowledge they don't even have a theory of Mind really you can test it and like they can talk about mind but they don't they don't have in practice any uh epistemological Theory or abilities they don't know what they know and they don't know why they know it or or anything they just barf out a possibility um and so because of that fine tuning is not good as a knowledge door period end of story do not use it it is not reliable um now that being said you can't like so if you look at if you look at chat GPT like chat GPT you might say ah no chat GPT has uh has you know some theory of knowledge because it'll tell me like I don't know that um no it's just following a pattern it was told it was taught that if you ask certain questions it's supposed to tell you that it doesn't know but then you just ask a different way and it'll tell you right because it it one chat GPT does not have a any cognitive architecture it is just a fine-tuned model and this is one of the biggest gaps in open ai's approach they are a model only shop they don't as far as I can tell openai has not invested anything in understanding cognitive architecture or Neuroscience they seem to believe that all that they need is a bigger more powerful model um and and if they can do it great if they can create a Model A large language model that understands what it doesn't know um great but there are mathematicians out there that I've talked to that said a large language model a single a single model uh as we know it today will never have this ability it will never be able to understand what it does and does not know you will have to have a system or a fundamentally different kind of model in order to have any sense of epistemology so uh this is why fine tuning will probably never at least with the current Paradigm will probably never be good as an information store not as a reliable one at least um another problem fine-tuning is slow difficult and expensive I've seen really boneheaded claims out there and I'm not going to name names um but where people have said like oh it takes 200 000 samples to do fine tuning and even then it doesn't work and I'm like buddy that's saying more about yourself than anyone else anyways um that's BS I have dozens of videos of fine tuning and showing that it is successful but it does underscore an important Point most people don't get it fine tuning is is an entirely new discipline you know as far as I know there aren't even good books out there like I've even thought about writing a book on fine tuning but fine-tuning is one of my most valuable skills so it's like you know on the one hand I want to teach the world but on the other hand I also have a startup right so I've got a conflict of interest there but fine-tuning is difficult most people do not figure it out most people don't get it and even after I try and teach people most people still don't get it fine tuning I I what I usually say is fine tuning is 100 times more difficult than prompt engineering it's actually more like 10 000 times more difficult it is hard um it's also very expensive um fine tuning is so expensive that most other companies have not figured out how open AI does it which also means that open AI like they they the open AI is in the same boat as I am and I know that I criticize open Ai and so does everyone Elsewhere on the one hand they want to help the world uh but on the other hand they um they have their their profit motive to to think of and so like I realize I can't criticize them anymore because I am withholding information for my own benefit as well so I just want to put that out there that uh and after having some conversations with people in the alignment um field um this is an open question how much do we share and why because there are there are dangerous players out there that are just leeches um or that are using it for nefarious purposes so it's like okay maybe we don't share everything this is an open question so I am still uh you know a little bit skeptical of open AI but it is is what it is and I just wanted to add that little bit to be fair because like you know pot calls kettle black right like I'm withholding information there withholding information I can't really criticize them so I just wanted to address that elephant in the room okay so let's compare let's do a side by side of fine-tuning versus semantic search fine tuning is slow difficult and expensive semantic search is fast easy and cheap which one do you think is better um fine tuning prone to confabulation semantic excerpt semantic search recalls exact information fine tuning just teaches a new task not new information whereas semantic search it is very easy to add new information to your database or your index fine tuning requires constant retraining whenever you add a new document you need to retrain the entire model um and that is very expensive whereas with semantic search adding new elements is very easy fine tuning is not scalable the cost of fine tuning goes up proportional to the amount of data that you have whereas semantic search is I say infinitely scalable and technically it's not infinitely scalable but um like the the F the f a i s s the Facebook AI semantic search that scales to like a trillion elements so that's way more scalable than fine tuning um and I'll say that fine tuning does not work for QA with a little asterisk we'll talk about that on a second um and then semantic search solves half of QA and we'll get to the other half in just a moment but first I wanted to address what do I mean when I say fine fine tuning doesn't work for QA with an asterisk that is that you can use a fine-tuned model to help with the QA process so for instance when you're when you're creating an answer you have a question then you have a corpus or you know some some reference material and then the answer so there's three components and so when I talk about task answering a question from a given Corpus is a specific task and open AI actually had a QA endpoint but they deprecated it because it wasn't that good and nobody used it but what I will say is that the current instruct models are perfectly fine you just say here's my question here's the body of information is the answer here yes or no and um you know if so what is the answer uh so it can it can be a component of it but it's not necessary especially with the latest um instruct aligned models so in short using fine tuning for QA is like using a hammer to drive a screw through a board on your knee you could do it but you're gonna regret it and it's not going to work it's a wrong tool for the job okay so what is fine tuning good for then so I talked about how it teaches a new task so the correct way to think about fine-tuning is that it teaches it teaches uh teaches the model a pattern right what you're teaching it is not new information you're teaching it a pattern and so chat GPT for instance is a pattern the pattern is short user query long machine response writing emails is a pattern all kinds of code whether it's Json HTML C plus plus whatever those are all patterns right and and fiction even writing long form fiction is a pattern um and uh yes I trained Curie to write long form fiction I'm not going to show you how but I will show you in a minute just to prove that it's possible so basically the short answer is fine tuning is really good at teaching it a new uh a task or a pattern based task so without further Ado here is my Curie based uh um uh scene generator so I have a very short input and then it goes and writes several pages of a fictional scene because if you won't believe that I did this on Curie I don't care believe me or not I'm not going to show you how because this is incredibly valuable anyways okay so at this point you're probably saying yeah Dave I get it you know I get the point I get the concept so how do we do QA so there are uh well first how do you do QA how does a human do QA first you start with a question then you go forage for information then as you're foraging you compile a corpus of relevant data then you extract the Salient bits from that Corpus and then you finally produce an answer do you remember libraries so let's use a library analogy to do QA um so how do you find stuff in the library you use a Dewey Decimal System Dewey Decimal System is basically a human readable semantic embedding the Dewey Decimal System is a string of numbers that tell you roughly what's contained in the book um and so when you have a have a question you match your question to a Dewey decibel number and you say usually you ask a librarian what uh you know what where in the library do I need to go and it's like okay well you probably need like 541.26 and you also want some something from the 900s or whatever and so then you go you go get a stack of books this is this is your semantic search right you go get a stack of books you take them back to your desk so now you have a corpus you've gone from a huge amount of data to a much smaller subset that you're looking through then you skim them you look at their appendices you look at their chapters whatever and you say ah you so you zoom in you zoom in further and you say this page on this book has what I need right that's what the index is for is literally so that you can just say ah I'm looking for you know King George V or whatever he's mentioned on page 286. um so you go there you jot down some notes you compile all that data together you're not answering your question yet you're just getting all the relevant facts together finally once you get all the notes together then you have a a much shorter document you've gone from thousands of books tens of thousands to books to maybe you know a dozen books and you then you distill that further down into a page or two of relevant information and finally you can answer your excuse me answer your question so let's break this down into simple steps that you can do with a machine first you index your corpus with semantic embeddings this makes it searchable and this is whatever your Corpus is case law medical records whatever then once you have your question you use your large language model to generate relevant Search terms or queries relevant questions and then you also use an embedding which is a basically half of a language model say okay here's my search query generate a semantic embedding and then use that to go find use your semantic search engine to go find the most relevant documents that you have to that query so that's basically matching your Dewey Decimal System um to pull the most relevant Resorts and then you use the llm to quickly read and summarize the relevant bits of those documents and finally you compile it all together and you have your answer oh and by the way I already did a video on this like six months ago um walking you through this exact process it's called answer complex questions with with gpt3 and multiple documents sounds pretty much like what you're trying to achieve right okay that's the end you're welcome