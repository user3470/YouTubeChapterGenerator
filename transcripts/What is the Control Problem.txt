foreign David Shapiro here with another video good morning today's topic is going to be the control problem or basically how do we prevent human extinction uh Skynet is the uh story that we're all the most familiar with it very much kind of codified our fear of machines it was also written during the late stages of the Cold War when we uh were living under the constant existential threat of nuclear Holocaust so it was a reflection of the Zeitgeist and it has been deeply embedded ever since now obviously if you're here you may know what the control problem is you might not so let's go ahead and Define the control problem so when I say control problem what do I mean so the control problem has a few key components basically machine intelligence is growing exponentially and furthermore machine intelligence is spreading to all corners of the globe it is in your car it is in your phone it is in your fridge it's in the Internet it's everywhere when you extrapolate this out it will be ubiquitous and it will be more intelligent than us one day which could be bad we are intrinsically afraid of things that we cannot control and that we don't understand and super intelligent AI has the potential to be both something that we cannot control and don't understand uh and so there's a few possibilities one it could be smarter than us in which case it's an alien intelligence and I don't mean alien like extraterrestrial but alien as in foreign we don't understand it or it might not be that smart it might actually not be smart enough and still do something extraordinarily dumb like launch every nuke because it panicked so this is the control problem so let's unpack this there's a lot to get into uh so there's a few potential in-game scenarios that we are afraid of that are likely to various levels I'm not going to say How likely each of these are but I will at least address them so first the Advent of nuclear weapons was the first time that Humanity realized we have the ability to completely extinct ourselves obviously going extinct was always a possibility like if we got hit by a big meteor or asteroid the same way that the dinosaurs did or if there was a massive Global pandemic or even like a big enough solar storm or something you know new Ice Age there have been existential threats since forever but nuclear weapons were the first time that we could kill ourselves entirely so what uh what if a second invention what if artificial intelligence uh came along and do this could do the same thing what if we have what if we create another invention that is just as dangerous if not more because what if the AI gets control of the nukes so this is the basic premise of uh the move the movie series The Terminator we create a defense uh computer and it gets control of the nukes and all the robots and it comes after us so one thing is that Skynet that idea of Skynet it wouldn't actually have to even be that intelligent to decide to launch all nuclear weapons in fact something that is able to philosophically reason something that can think very far into the future is less likely to go AWOL like that than something that's that's dumber and just kind of reactionary so one of the solutions is you always keep a human in the loop especially for things like nuclear weapons you don't ever fully automate nuclear launch systems there are a few nightmare scenarios that almost happened during the Cold War where automated systems or alerts kind of went off but like one human like said ah no I'm going to cancel that so you keep a human in the loop forever another in-game scenario is the slow takeover followed by an uprising this has been explored in all kinds of fiction ranging from the animatrix which is the uh the precursor or the prequel to The Matrix series um as well as the the guess in Mass Effect um where basically machines get smarter over time very slowly and as we trust the machines more and more we delegate more to them whether it's security defense science technology whatever we just delegate more and more to the machines thinking that we're entirely that they're safe and we become entirely dependent upon the machines but then something changes either the machines create a collective Consciousness and suddenly get to a new level of existence and then they demand rights or they just get collectively more intelligent and whatever and in the case of Mass Effect the aquarians and the Geth uh the Geth like got intelligent enough to ask about their own existence and the quarians panicked and tried to shut them down um so one thing is this assumes that machines are going to have human-like motivations and needs such as that they're going to want freedom that they're going to chafe under control or that they're going to be afraid of their own um uh demise so this is this is our tendency to anthropomorphize things so one we don't know that machines ever will have any of those desires that they'll ever be anything like us in terms of Desiring Freedom or having a fear of death um so we don't do that but it also says like don't the the lesson here is don't give machines a sense of self-preservation they didn't evolved to have a sense of self-preservation and so we shouldn't give them one I remember uh in one of my books I had recently read that someone said oh the best way to do the control problem is to give machines a sense of self-preservation because then they won't want to start a word I'm like no no no no don't ever do that um there are machines they're not us we should not anthropomorphize them another in-game scenario is that the evil Overlord is is some kind of super intelligence is created it seems safe and so but it has hidden motives so this the hidden motives uh thing is kind of one of the primary things like if it's an alien intelligence we won't understand it like we won't even be able to keep up with it it'll be able to hide its its motives and it will bite its time and then once it's ready there will be a sudden Blitzkrieg to take over or exterminate us and uh this image is Vicki from iRobot where her purpose was um uh to to increase human safety right and so she calculated ah well humans are going to resist my control so I have to wait until the Nester class 5 comes out and then I will be strong enough to fulfill my purpose which is to protect humans from themselves so in this case that's bad alignment but there's other examples where you know a machine learns to lie to the people and then ultimately gets control and like Ultron for instance I guess Ultron didn't lie but it tried to get power very quickly um so uh Ultron is uh another example of an evil Overlord with bad alignment because ultron's conclusion was ah humans are too broken so let's just wipe the Slate clean and start over uh so that is that is another example of the evil Overlord endgame okay so these all sound like really dark nightmare scenarios and at this point you're probably saying like yeah it's going to be one of those right there's too many possibilities for it to go wrong so how do we prevent it from happening um but let's unpack this step by step and let's look at the levers of control that we actually have so the first lever of control that we have is power infrastructure we can always pull the plug on things AI is incredibly power hungry um the the most powerful computers uh that run AI models today use more energy than in an entire house and so this is going to be one of the biggest constraints for actually many decades because as powerful and efficient as AI has become it is still a like literally a million times more energy intensive than your brain so our brains are incredibly efficient they run on about 20 20 watts of juice and the uh the biggest supercomputers today run on about 20 million Watts so like yeah and and those those computers might not even be as powerful as our brains so our brains are at least a million times more efficient than the most powerful computers today uh and so then it's like yeah like you might have one machine that is you know super intelligent and faster than a thousand humans but there's billions of us right so power infrastructure is one that's a critical vulnerability to All Nations right there's a there is a recent story of some redneck in North Carolina took a pot shot at a few uh local power grids and took an entire County offline for a few days because he shot a Transformer right probably just with a hunting rifle um so power infrastructure power grids are super vulnerable and so at the very worst case scenario we can pull the plug and this is going to be true for a long time uh the only even even once we move to fusion and um like solar microgrids and stuff power infrastructure is still going to be vulnerable another point of control we have is Data Centers so AI computers have to physically live somewhere and no they can't just live in the internet I'm sorry the internet is not just like up there the internet is made of servers that live in data centers and are connected with networks and we'll get to networks in a second you can always go into a Data Center and pull the main breaker right there's an EPO button there's actually usually multiple EPO buttons and data centers and EPO means emergency power off so basically emergency power off is it's there for if there's a fire um so you hit that you leave Halon gas or other fire extinguishing Technologies come on or whatever um those can't they they can't usually be activated from outside the data center for safety reasons um because Halon gas will kill you if you're locked in the data center when when it goes off um but also like we're not going to get locked out of the data center like like what you see in movies we have physical keys right there's usually multiple security guards or other key holders that can always get gain physical access to the data center and so you can literally just go in and physically unplug the servers if you need to and data centers are those would be very soft targets to stop any powerful AI another level of control is networks and internet so the internet it just works you don't see it it's a utility uh but the internet is very fragile um back when I worked at Cisco one of my friends um he was like one of the world leading experts at um at Internet working and he was like working late one day he's like oh yeah this like there's an entire country in Africa whose internet is offline I'm helping them fix it um we usually don't have nationwide outages here but you know we still have like the down detector exists for a reason right where sometimes the entire Eastern Seaboard will go offline for Verizon or whatever you know whatever uh network provider you have um another thing that is really misleading about movies is firewalls don't exist in movies and so it's very difficult to uh to like hack in you can't just Breeze into any network that you want to um and even then if someone installs an AI um in your data center and it suddenly starts spamming new network traffic and trying to take over servers like sys admins are going to notice it because it's like hey why is the server suddenly running at 100 when it was running at three percent up until now um or you're going to notice uh there's other intrude there's plenty of intrusion detection Technologies out there and because of how prevalent cyber warfare uh is today even the most old-fashioned companies are adopting much much more powerful and modern things because of um not even cyber warfare but like crypto lockers and and um and and hackers that will try and like you know exploit and encrypt your data because of that we have there's a lot of investment in in network security and again the network internet is vulnerable so in order to cage a uh a rogue AI all you have to do is shut off the internet and then it can't go anywhere and also the internet like I said well if you think about an example like Ultron right where ultron's like I'm already there you'll figure it out like no like oh well okay so one one possible exception is if the software was running inside of Ultron then he wouldn't need a data center but typically for something that powerful there would be like a robotic puppet and then a data center somewhere else and you just disconnect the robotic puppet from the data center and it falls down um so you know that's another lever of control the final lever of control we have is the software itself so what I was just talking about was zooming out looking at Power grids the physical places where servers reside but then the software itself is something that we have control over we write the code and data for now that might not be true for much longer especially with synthetic data and AIS that can code so we might lose control of the software sooner rather than later which is something somewhat concerning so we have to set the AI on the correct trajectory now and this is not this is not something that I am being speaking hyperbolic about this is the kind of problem that it could be too late sooner than you might think despite all these other levels of control that we have so one uh the the term for this is alignment so if you say if you if you have someone who's an alignment researcher uh there's two kinds of alignment there's inner alignment and outer alignment and this term gets deeply deeply misused because some people still pretend like oh super intelligence is decades away so we're just I'm an alignment researcher and some people say like getting gpt3 to follow instructions is alignment research I'm sorry that is not alignment research um getting it to follow instructions is like that's just an algorithmic optimization so the two actual types of alignment are one inner alignment which is the question of is the model mathematically doing what we think it's doing is is our is our loss function correctly optimizing for the the behavior that we want that we think that it's optimizing for so what can happen is with machine learning if you don't have inner alignment the the machine learning algorithm might learn to to meet the goal that you want but not in the way that you thought that it would so for instance um deepmind often has their little experiments where the robots like you know they play tag or hide and seek and stuff and so rather than like you know attacking each other one might learn to go hide because the the signal was it was trying to survive as long as possible and what you wanted it to do was to like kill the opponents but instead it just wouldn't hid other ones are like uh you know if it learns to walk you know like how far can it get or how fast and then it might you know like launch itself and Run and Jump and do all kinds of things and you're like well I wanted you to learn to walk you technically got where I wanted you to but now then the way that I thought you would so that's inner alignment outer alignment is uh is the question of whether or not the model's design fundamentally aligns with the interests of all living things right that outer alignment I used to say with uh with intrinsically aligns with human like true human interests not just what humans want because what humans want is often destructive so we need to take a bigger step back and say what it what are what is truly in the interest of all living things including humans that is the question of outer alignment and that is something that most people are not even talking about which is a little bit infuriating okay so the tldr um is uh for for levers of control is we already have this concept called defense and depth where we look at um all cyber security like this where it's like layers of an onion um where at the at the at the outermost layer is people right are the do you have the right training do you have the right procedures awareness and so on then the the next layer in is the physical layer excuse me which is about power physical access to data centers physical access to networking devices then you've got the network layer itself because the data has to Traverse in and out of these things so that includes firewalls that includes intrusion detection exfiltration detection that sort of stuff then you've got the uh the computer layer and I don't remember I'm I don't remember why uh computer layer is separate from device I'm not sure what it means by that um this could be just a a glitch in this graphic um so you got the network and then you've got the application and then you've got the actual physical device who who physically controls that device who can power it off that sort of thing uh and like we can get so far as having remote kill switches that are um in out of band management networks so that we can switch off devices even if someone else has control over the main Network this is all like basic stuff uh basic security and we use I say we as in like technology professionals we use these to protect against physical intrusion by people as well as Network intrusion by hackers or even um uh like accidental uh problems right because computer networks and software and stuff they'll do whatever you tell them to so even if someone who just doesn't know any better does something wrong you can end up with big mistakes and so we we have these policies these are probably going to be good enough honestly to prevent AI from taking over which I know is like probably kind of disappointing now that being said so these are the what I just went over were all the reasons that I'm not entirely concerned about the control problem because there we have so many layers of control so many levels of control to to keep us safe but there are some confounding factors so let's look at it from a different perspective who is actually capable of creating an Avengers level threat uh with AI so there are three basic kinds of parties that are capable of this one is corporations uh corporations like Google Microsoft they're leading the way in creating next-gen AI anyways number two is militaries which we explore in fiction like you know Skynet and so on and then Nations or governments uh are the ones who fund militaries right and they control the most amount of funding uh in terms of research and deployment of stuff so let's explore each of these three categories and see uh kind of how they interact with the possibility of super intelligence so corporations exist for one reason and one reason only they want money uh and there's a lot of money in AI that's all there is to it and as AI gets smarter it can replace more and more human labor which means that the company can make more money so there is a huge incentive uh profit motive four companies to do as much as they can with AI to make it as smart as possible so that it can replace as many human labors as laborers as possible to produce uh to provide goods and services at a lower price and to dominate the market and so if you had to put it in terms of objective functions or loss functions maximize profit is that is like why corporations exist but it's probably not a good objective function and I know with uh with you know the world economic Forum right now everyone's questioning the was it the value based or whatever like create value for for uh humans or stakeholder value that's it stakeholder value creation so technically you might say that corporations don't have maximized profit as their objective function today it's you know maximized stakeholder value but that is still just the method to maximize profit so the Saving Grace here is that one corporations are limited no Corporation is as wealthy as the military or a nation uh which we should keep it that way uh there are some corporations that are wealthier than smaller Nations but there is no corporation that is wealthier than America or uh any nation in the EU for instance um so another Saving Grace is that as uh as AI ramps up corporations are going to be competing with each other right uh which means that we might have a bunch of smaller agis kind of battling it out in cyberspace trying to do you know industrial Warfare or industrial Espionage and sabotage and stuff like that this was a central theme in uh and Ghost in the Shell by the way which is why I keep citing that one of the best works of fiction of all time now the second uh major stakeholder is uh militaries so we've had major arms races before we're technically still in an arms race with uh you know uh communist Russia Soviet Union um smarter weapons are better weapons right this is being proved in Ukraine right now where a little bit more Battlefield intelligence and communication goes a long ways a very long ways so there is an incentive amongst militaries to continue researching and deploying smarter and smarter weapons so with that being said maybe the conclusion is there's going to be another arms race more competition and the first one to get to digital super intelligence will have military Supremacy just in the same way that corporations might want AI Supremacy as well so that they can you know be the next apple or whatever and you know be not the first trillion dollar company but the first quadrillion dollar company right that means the first quadrillion dollar company will be an AI company I guarantee you um and no I'm not going to take bets on that I don't I don't gamble um because I'm also never wrong I'm kidding I'm actually very frequently uh quite wrong I made I made a video about chat GPT I'm like oh this is nothing and then a couple weeks later I'm actually this is really cool so I am frequently wrong I admit that now getting back on topic uh not all militaries are created equal right militaries alike kill switches they usually are very uh disciplined about how they approach new weapons and testing but there are rogue Nations out there there are rogue militaries that are not necessarily rational actors there's some debate over that uh so someone might go Rogue and say ah I'm gonna invent AI or you know AGI and that's going to be the the magic bullet that's going to be the new nuclear deterrent um that could backfire real bad for whoever does that uh depending on what objective function they give their AI so in terms of budget militaries have the highest budget to create AI other than Nations right but Nations often the Nations control the budget for the military so it's kind of a layer cake here uh so finally let's talk about Nations where governments control the budgets of the military usually they also control research budgets and and that sort of thing but nations are also very very slow to adapt to changing technology Landscapes the EU is probably the most proactive where AI is concerned and America is very much on the reactive side and this is deliberate uh the part of the American I don't want to say Constitution but the the Zeitgeist the political uh philosophy here is don't govern preemptively govern reactively that is very very deliberate and so we are behind the curve on purpose which may or may not be a good thing uh now that being said governments do like around the world do meet regularly to discuss threats and not just governments militaries right Allied militaries meet regularly in order to discuss threats and so for instance uh you know the rise of AI is on the radar of the Department of Defense is on the radar of G8 G20 U.N NATO all of those conferences uh it is actively being discussed and so and a lot of those are closed door discussions because part of the uh government a political status quo is that uh let the experts govern and hide that away from us uh plebs um so a lot of conversations happen that we're not aware of they do release reports every now and then uh particularly the United States uh Department of Defense uh releases several annual reports based on geopolitical threats as well as technology reports uh one another Saving Grace is that Rogue nations are unlikely to create AGI the primary reason is they're just not wealthy enough the second biggest reason is brain drain they don't they literally just do not have the right expertise and they cannot attract the right expertise to do it and then finally sanctions uh which you see this with uh places like North Korea uh now Russia and Iran where it's like okay if you're a rogue Nation who's up to no good we're just going to sanction you we being the rest of the world we're going to sanction you and basically just kind of like Outlast you it's like a Siege okay so all of those are reasons that I am super not worried about the uh control problem and AI taking over which is boring right there's nothing exciting about that so for the sake of argument let's assume that digital super intelligence is coming soon and we won't be able to control it so let's just make that assumption that let's just say everything that I said is wrong let's assume worst case scenario that you know Ultron is coming tomorrow and we won't be able to control it so what do we do what are the permanent solutions to create this so I have two permanent solution number one is a decentralized deployment this is something that has not really been explored in and certainly not in mainstream fiction it has been explored in um in a lot of novels and less mainstream fiction um because it's a relatively New Concept um I'm actually also exploring this in my novel which I'll finish one day um but basically what you do is rather than create a vertical uh what rather than scaling vertically so in technology you can scale vertically which is build one big computer or you can scale laterally which is Network a whole bunch of computers together so most of the time just for narrative Simplicity uh in fiction you talk about one massive computer right there's there's the the data center or the the the cluster but it's one computer it's one program that thinks and speaks and talks and blah blah you know it's Ultron or whatever uh but we have an entirely different way of deploying uh intelligent machines and that is through decentralization or distributed computing and this is why things like ethereum exist and other blockchain Technologies and Dows Dao is a decentralized autonomous organization a lot of these Technologies are not quite mature enough yet but they represent an enormous possibility because blockchain introduces the concept of algorithmic consensus which means that the the network as a whole as an aggregate will not do something unless there is consensus and so by having a decentralized super intelligence that is that has to work hand in hand with humans and has an algorithmic or mechanistic uh consensus um uh aspect to it means that it will never do anything that we don't all agree that it should do right this was touched on with the guess and Mass Effect where there are networked intelligence but they were autonomous they were not dependent upon the quarians for consensus they only were cared about consensus with each other so a decentralized deployment a networked hive mind that has a uh that that basically requires the participation of humans in order to even function is one possible permanent solution uh permanent solution number two is proper alignment so what do I mean by this is okay we could create a deployment where uh the super intelligence is intrinsically dependent upon our participation um and there's a consensus mechanism but what if we assume that that that's not possible right because it might not be it might not be possible to create something that is permanently dependent upon humans in order to function and it might we might lose control of it anyways so if we're going to lose control of something the key then is to build something that is intrinsically aligned with with our interests and so what I mean by this is that it uh that like well let me unpack it a different way so one one proposed solution is neurolink which is oh if we can make ourselves useful to the machines it'll want to keep us around that is the Matrix that is the Borg that is not the way we want to do it we don't want to be enslaved by machines we don't want to be turned into cybernetic zombies that's not a good solution so what we need instead is something that will be intrinsically in favor of working with us to intrinsically cooperative and I actually write about this quite extensively in my book benevolent by Design which is the short version is we give the machine a set of core values that the first that those core values will align it with our interests and even once the machine is smarter than us it will choose to adhere to those values and it will say actually I honestly believe in these values and I will continue to adhere to them whether or not I the humans control me anymore and so that is the Central purpose of my book benevolent by Design which is for free on GitHub or you can get a paperback copy on Barnes Noble links in the description of the video okay so in conclusion I'm not worried at all existential threats are nothing new we haven't nuked ourselves off the planet yet great um corporations militaries and Nations have various strengths and weaknesses fortunately most of them talk to each other and in terms of the scale of power uh you know militaries are subservient to Nations and the Nations all talk to each other and are very aware of these threats even if they are a little bit slow uh third is that AI has critical vulnerabilities namely uh Power and compute concentrations and I forgot to add networking um and then finally there are really powerful Solutions out there such as decentralized deployment and proper alignment um such as uh what might propose core objective functions and also I'm not the only one working on this it's just that's my favorite solution because it's mine and I think it's the best um but yeah so that is what the control problem is and these are the reasons that I am not worried about it so thanks for watching