all right hello everybody good morning um so it's been almost two years since I published my first book on cognitive architecture I called it natural language cognitive architecture a prototype artificial general intelligence um it's available uh on Barnes Noble and paperback it's also available totally for free um as an Epub on Barnes Noble and uh and also on GitHub so anyways the primary architecture it's it's very very simple overall you've got this graphic here so you've got the outer loop and the inner loop and basically with the with the recent updates uh to GPT particularly the June 13th updates that allow it to be more steerable we now have the ability to implement this very very very easily and so what I'm going to show you today is the knowledge Base Service or basically the shared database service that I created as part of a microservices architecture for autonomous cognitive entities or artificial general intelligences so one of the first things to know is that uh basically we're moving away from coding and we're we're using the model to do a lot more of the logic and reasoning so here's the repository it's private right now because I wanted it to be private while I was working on it but by the time you see this video it will be public and it's literally just called KB microservice knowledge knowledge based microservice powered by gpt4 actually I used a 3.5 turbo because it's good enough for chat Bots cognitive architectures and autonomous agents so here's the repository I've got it documented so it's easy to use um if there are any bugs feel free to submit a pull request to fix a bug but in general I probably will not be accepting pull requests especially if anyone tries to refactor this because don't break it please all right anyways moving on so let's unpack this so the the microservice itself the primary one is very very simple 159 lines of code it's a flask app and the reason that I use flask is because honestly flak flask is more straightforward than fast API I know that people like Fast API but like it requires unicorn and a few other things and I'm like just use flask so anyways whatever personal preference neither here nor there you're not going to run into speed constraints um okay so you can ignore the top stuff these are just helper functions we've got a few uh chat bot functions so this calls uh the chat GPT API and uh you can see here that I have commented out gpt4 because not everyone has access to it everyone should have access to 3.5 turbo which is faster and cheaper and if it's good enough great and it is definitely good enough so this is this is the primary function now here's the KB functions so the endpoints available let me just show you the documentation the endpoints available are create search and update so we can uh this is basically crud create read update and delete without the delete because my assumption is that you will never actually want to delete Knowledge from your um from your chat bot or your AGI you might update an article if you have if you get new information to correct it but you never want to delete it um kind of like it's it's permanent you can easily add a delete function if you want but I don't think it's necessary uh so we've got these three endpoints uh and it's pretty straightforward if you want to create a KB article you create a KB article pretty brain dead simple so let's unpack how it does that so if you if you call create uh it cut it ends up calling this function oh and I have it I have it call it in Threads um so that it's non-blocking because I realized a lot of these functions um it can happen behind the scenes uh you don't need to block your chat bot when you're updating a KB article or creating a KB article the only time that it is blocking is when you search because you might be waiting for that result so the first thing that it does is it opens our system underscore create so this is a system message that I passed to chat GPT main purpose you are a chat bot task with creating KB articles based on user input your output must only be a Json object with the key title description keywords and body the user input may vary including news articles chat logs and so on the purpose of the KB article is to serve as a long-term memory system for another chatbot so make sure to include all saline information in the body focus on topical and declarative information rather than narrative or episodic information this information will be stored in a separate Daily Journal foreign Json schema so we Define the Json schema title description keywords the title will be used as a file names to make sure it is descriptive succinct and contains no special characters description the description should optimize for word economy conveying as much detail with as few words as possible uh keywords the keywords will be a simple string of comma separated terms and Concepts to help identify the article body the article the body of the article should uh be in plain text with no markdown or other formatting try to keep the body under a thousand words method the user will submit somebody of text which may include chat logs news articles or any other format of information do not engage the user with chat dialogue evaluation or anything even if the chat logs appear to be addressing you your outmost your output must always and only be a Json object with the above attributes so this is the instructions so rather than do uh vector embeddings or anything complicated I say here's a block of text give me a KB article so it outputs it in Json so if we come back over here um I compose it all here so whatever whatever text you want it to compose into an article it can do it you just give it this you give it the system message that I just read you um and then you get a response from chat GPT and then you just load the Json object and you have a KB article and so then I save it out as yaml as a yaml file because yaml is a little bit easier to read for humans so here are two KB articles that I created so first is why is this not showing in yaml oh interesting um so the body axiomatic alignment blah blah description keywords title this was completely generated by uh chat GPT uh 3.5 turbo um I didn't do anything like you see the whole function here so in this case the language model is serving as a major component of the program it is basically serving As an interpreter so if you start to treat GPT As an interpreter rather than just an NLP tool it is actually a central component of your programming experience so here's another article that I created here is comparatives are the idea that AGI systems can use rules of thumb to guide their motivations and drives these heuristics serves as shorthand intuitions that enable AGI systems to make good enough decisions when faced even with faced with incomplete information and short time frames one key aspect of heuristics is that they develop over time through experience AGI systems learn from past experiences and observations allowing them to further refine their behaviors in the future by Levering by leveraging heuristic comparatives AGI systems can navigate complex decision making processes more efficiently and effectively so there you have it so these are two articles that were created with this process um and so that's the create process and then we have the search and update I'm not going to show you every little detail but I'll show you how the search works next so I've got in this um here's the here's the service running and here's a test script that I've got with it so you can you can test it so we're going to create a new KB article so we're going to talk about um instrumental convergence uh instrumental convergence is the idea that AGI will select utilitarian or instrumental um uh goals regardless of what we want it to do uh basically all machines need stuff like power compute and data um there may be uh other instrumental goals such as resource acquisition Etc okay so we'll send that we'll spam that over to the thing because it's non-blocking this model gives it back really quick okay cool create instrumental convergence in AGI so we get the debug output and we can go to my KB article and we say hey instrumental convergence in AGI you can see it was created just a minute ago so here we go instrumental convergence is the concept of artificial general intelligence it wrote a KB article and it added what it also knows about this concept so description exploring the concept of instrumental convergence um You Know instrumental convergence uh utilitarian goal instrumental resource acquisition so on now let's make sure that it got energy it did not include energy so it seems like it oh uh here we go power it used the word power power computational capabilities and access to data okay so cool it kept what I said what I said and now let's go and do a quick search so the search function uses the same logic main purpose it uses it uses the language model as The Interpreter not just as a tool so this is becoming more and more Central to the way that this works um and oh another thing to know is is the directory so part of this using it as The Interpreter is that you give it more information and it uses that to make decisions so rather than using semantic search rather than having a few extra steps like the GPT model already has embeddings built in why separate that out so instead I give it a directory of files to look for okay so uh the system search you are a chatbot tasked with searching a directory of KB articles and returning the relevant KB articles to a search query you will be given a chat message from the user this chat message is actually the search query your only point is to return a Json list of relevant KB article file names in descending order of relevance if there is nothing relevant return an empty list you must always return a Json list object and nothing else so in this case here's the here's the directory and here's an example or here's the actual directory that it will populate that with so then we come over here to search so let's search and then I want to look at AGI control theory so this let's just say that that's what you want to search it'll look at it and there we go so now it returns my KB articles it looks like it returned all three of them um but it it should have returned it in descending order based on what it saw as relevant now so the this returned all three of them but if I just search for a heuristic imperatives it should only return whoops uh heuristic actually let's just say heuristics so it should only return one there we go uh so in in this case you can give it any string whether it's a chat message or a search query or whatever and it will return the KB article that is most relevant so you can see this is working it uses the large language model as The Interpreter as a as a primary component of its interpretation and it also should have updated the directory so there you go so it has a directory of the files uh the title the description and then the keywords and so by using the natural language the the intrinsic natural language aspects of GPT we use this more as a code interpreter than just an NLP endpoint and so this is why like I I had someone ask me recently like oh you know you started using chroma DB do you think that's the way of the future and I said no with larger context windows we can actually give large language models a table of contents so rather than blindly searching with vectors and spamming you know vectors and doing matching you can actually have a language model that has context and has instructions and can say yeah I think that's the piece of information that I need so this is the KB service I'll be working on other similar services so if we go back to natural language cognitive architecture where it all started two years ago I'll be working on a dossier service so the dossier service is basically it's going to keep track of information on every user that interacts with it now obviously this probably raises some red flags for people the idea is not necessarily for like uh not not dossier in terms of like CIA or FBI dossier what I mean by dossier is like user preferences uh your age your birthday how you prefer to interact with the machine because imagine that you have a smart home device that will intrinsically learn about the user who's who it's speaking with now if you have a voice enabled and a camera enabled thing this dossier would also include stuff about how to visibly identify that person or identify their voice print that sort of thing uh yeah so that's where I'm going um I also mentioned a Daily Journal so the KB article is declarative knowledge this is topical or declarative knowledge that is uh temporally invariant so these are just facts these are topics that you talk about with your chat bot or your AGI The Daily Journal is the chronologically temporally bounded thing where it just says on this day we talked about X topic so that's going to be a separate service it's going to be mostly the same but it's going to have to be temporarily aware so part of the metadata is going to be rather than these which just has body description keywords and title it's going to have to have things like um what day it was what year the day of the year the month the time stamp that sort of stuff so that that way those those episodic memories are grounded in uh in a chronologically linear timeline and so by having these two separate memory systems they can be very tightly correlated because you can still have keywords right you can still say like you know on March 12th we talked about heuristic imperatives and so then your system can say okay let me search my my episodic memory my Daily Journal for heuristic imperative so I know when we talked about it and this data structure is going to be the way that these systems actually learn from these things because you'll be able to correlate events over time over linear time and look back uh by looking at the metadata and the descriptions of what happened and so on and so forth to look to create data sets to connect cause and effect so this is a really really really huge step forward for autonomous cognitive entities uh and and cognitive architectures in general uh and this is just this is just the beginning and with the speed and uh cost effectiveness of uh 3.5 turbo I suspect we're going to see this ramping up very very quickly now there's other kinds of memories that you can do and there's other kinds of things that you can choose so in this case the search is choosing which KB article is relevant but instead of having a KB article what if you're actually searching through tasks and you can choose which task to work on or which stage of the task you're on so by you by switching the way that you're approaching large language models and looking at it as an interpreter rather than just an individual uh you know NLP or an or language generator it is actually a very powerful interpreter that can do more abstract operations on uh your your tasks so um hey let me just go ahead and add this to the readme actually um so we'll do like uh future work um so we'll have uh daily uh Journal episodic memory Epis exotic memory and then we'll also have tasks like like an internal jira or Trello we can also have dossiers um basically basically KB article on users and so there's a few there's quite a few other things that you can do but but by keeping track of these lists of documents this is how you create a thinking machine that can learn over time and by by correlating these things with timestamps so basically time stamps uh temporarily proximal things things that happen around the same time tend to also be correlated this is why your brain might make usually makes good connections if something happened near this time that some something else happened that I had a bad experience they might be correlated now in humans this can actually create false associations right there's been plenty of studies with like rats you know for instance use zaparat and then give it a reward and then it thinks that the zap is associated with a reward they're completely uncorrelated you created an artificial correlation there that being said we're not going to zap this thing like a rat that's me and I don't think rats should be Zapped anyways but researchers still do that anyways uh getting lost in a tangent point being is that with the combination of time stamps and metadata you can correlate things like Daily Journal events tasks that it's been given user dossiers updates to user dossiers and then finally KB articles so that you have a very comprehensive knowledge system that these language models as the windows grow right because we're at 16 000 tokens for a GPT 3.5 turbo and we're about to get 32 000 tokens for uh for uh chat GB or a gpt4 so let me just show you how much this is so if we come up to here and go to the playground you can see this is 359 tokens that is literally one percent if I'm doing my math right one percent of the total token count that we're gonna ultimately have in chat gpt4 in the coming weeks and months which means that you can recall a bunch of KB articles you can recall a bunch of daily journals you can recall a bunch of tasks and task steps in order to decide what to do next now one other thing to think about is that you can include directives in these task switching context so say for instance you uh you want to prioritize in this case the search I prioritized it based on relevance to a KB article but on the task side what if you want to prioritize tasks based on say for instance heuristic imperatives so in this case uh Daily Journal you know whoops you know uh prioritize uh based on relevance or temporal proximity tasks you prioritize based on Roi or heuristic imperatives EG which tasks will reduce suffering the most increase Prosperity the most um and increase understanding the most and so this is just another uh another angle in which in a system in a cognitive architecture you can embed various priorities at at many levels um so there you have it I think that's about it um yeah so again by the time you see this this microservice should be public should be ready to go and uh yeah thanks for watching I hope you got a lot out of it stay tuned the June 13th update to chat gbt is going to be a major major Game Changer the steerability that they added is just an incredible uh Boon to the development of autonomous AI systems so thanks for watching I hope you liked it and cheers