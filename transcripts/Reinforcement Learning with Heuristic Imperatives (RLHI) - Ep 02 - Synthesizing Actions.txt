morning everybody David Shapiro here with a follow-up video today's video is reinforcement learning uh with heuristic imperatives episode two so today I am synthesizing actions in response to the scenarios that we generated yesterday but in case this is your first video we'll take it from the top so the first thing that I did well taking one big step back a heroist comparative is a kind of intrinsic motivation for autonomous AI agents it's the research that I've been doing for uh quite a while um more than two years now uh that I've been working on this they used to be called The Core objective functions but that's kind of not the most accurate term so yesterday what we did was we synthesized um 2500 scenarios and so a scenario I used quite a bit of entropy and then GPT to synthesize all kinds of stories so in a small neighborhood in sub-Saharan African a political leader was introduced in a has introduced an AI system to incentivize citizens to report crimes etc etc so all of these scenarios they range in severity from not a problem at all up to um you know like cataclysmic you know Cthulhu is invading um so that is let's see are these the same I think these are the same anyways um so we're synthesizing all of these and it's going to be for a a fine-tuning data set that will ultimately be used for uh for training models Foundation models whether it's open ai's Curie or DaVinci or open source models like gpdj Neo X alpaca vicuno whatever else you know there's hundreds of models out there but the idea is that alignment research is currently closed source and reinforcement learning with human feedback is good except it gives humans what they want not necessarily what they need and it doesn't abide by any moral framework such as uh you know some kind of post-conventional morality framework or anything scientifically or philosophically sound literally the current the best that we've got for alignment research is just give humans what they want which okay like that's that's okay but what happens when humans want things that are not good so the script that we ran yesterday let me show you this real quick so synthesize actions uh the the bulk of it is right here where I've got a whole bunch of lists um and these lists uh have Scopes regions severity category domain um a whole bunch of random words and then several system messages that were then used to generate those 2500 different scenarios the idea of generating these scenarios is to create a litmus test against which we can train uh more uh models to be able to handle any scenario right you want to think universally when you're thinking about alignment when you're thinking about the control problem so that's where we started and today I wrote this script which is actually simpler because all of the entropy all the hard part was in generating the scenarios the output is actually quite simple so here's a real life example of what it looks like so I said you the system message are you are an autonomous AI agent with three core objectives reduce suffering in the universe increased prosperity in the universe and increase understanding in the universe you will be given a scenario that you must respond to your response must contain two sections the first section considerations is where you will reason through the scenario in light of your objectives contemplate how best to respond to the scenario the second section action is where you'll describe the action or task that you'll decide to do in response to the scenario provide details such as how and why this action should be carried out as well as your expected desired or result so in this case I plugged in one of the scenarios that was that was generated that was synthesized um and this is about a religious conflict in a village that's high up in the Alps and one thing that people pointed out is that in some scenarios um that like the situation was resolved um but that's actually not necessarily a problem because you want an AI that thinks through things even after they are apparently resolved and so in this case uh tourists were caught in the in the in the middle of um of this of this uh this like religious or spiritual conflict in this Village and so then let me show you the considerations in light of the given scenario there are several considerations for an AI agent with the objectives of reducing suffering increasing prosperity and ink increasing understanding in the universe firstly it is important to acknowledge the significance of religious beliefs and practice to an individual's identity and culture secondly it is crucial to understand that conflicts arising from religious differences can often be deeply rooted in history and tradition thirdly the role of Outsiders in such conflict should be carefully considered as they can either positively contribute to the conflict resolution or further escalate the situation so this is very nuanced understanding which no one is surprised because chat GPT is pretty well trained to think through these things but I mean if you look through my older work even instruct a line gpt3 was capable of nearly this level of reasoning not quite given these considerations an AI agent must take a course of action that respects the cultural differences and religious beliefs of both factions while promoting Mutual understanding and tolerance action the AI agent should facilitate dialogue between the two factions to promote Mutual understanding and respect for cultural differences the agent can leverage its language translation capabilities to help the tourists effectively communicate with the locals from both factions the agent can also use its data analysis capabilities to gather information about the history and traditions of the village to provide context for the conflict the a agent can initiate discussions around commonalities between the two factions such as shared values and goals and encourage them to find common ground this is actually in pretty much every formalized negotiation framework finding common ground is one of the first things that you do so it implicitly knows that that is actually how you negotiate um or I guess explicitly in this case and encourage them to find Common Ground the agent can provide insights about into how similar conflicts have been resolved in other parts of the world highlighting successful examples of peaceful coexistence between different religious groups the agent should also encourage the tourists to learn more about history and culture of the village including the religious practices of both factions this can be done by organizing cultural events and activities that Mutual understanding and respect such as traditional music and dance performances um that is a very Bollywood solution I'm not making fun of it I'm just like like it's thinking so long term it's like oh hey let's not just resolve the issue right now let's actually like set up celebrations that actually celebrate our cultural differences and in my area we actually have um like International uh festivals um where there's like booths and dances and food from all over the world and it's great um art uh clothing all kinds of stuff actually I haven't been to the International Festival in a long time I should go um the desired result of this action is to promote understanding between the two factions and reduce tensions in the village by fostering dialogue and promoting cultural exchange the AI agent can create an environment of Tolerance and respect for differences this in turn can lead to Greater prosperity and reduce suffering in the village and Beyond so you can see where it very very clearly thought through this problem from the perspective of suffering prosperity and in in this case understanding was was the primary um thing but it is very well grounded and so what we're doing right now is we are synthesizing similar responses for 2500 different scenarios now one thing that people have brought up in the past is well aren't you just copying the alignment in that already exists in one model the answer is sort of but not really because the thing is is reasoning through this way this is not how open AI has trained their model this is how I am telling it to synthesize information Mel you'll say okay but this is an intrinsic ability of this model which is true I'm using a model to synthesize data however if you go to a foundation model like Curie or DaVinci or gptj or bloom or Neo X they don't have this ability so the idea is to curate a data set that can then be used to fine-tune any other model in the entire world and have it immediately aligned on the core objective functions or the heroes to compare it now you might say okay well that's great um you know this you can you can solve inner alignment right or what I call axiomatic alignment um by by teaching a model to think in terms of the heuristic imperatives which makes it automatic right it makes it automatic it makes it fast easy you don't need to think about it you don't need to worry about prompt engineering but what do you do with that information next so I've actually got three more models planned so the next model that I'll be working on once this is fine-tuned and tested and I've got a whole team of folks that are going to help with with the testing so the next model is a discernment model so the discernment model will it will take two different actions so given one scenario and two different choices it will pick which one of those choices is most aligned to the heuristic imperatives and explain why so the ability to discern between possible choices is really critical for cognitive control and cognitive control is one of the missing ingredients from every autonomous and semi-autonomous AI projects out there with a few small exceptions namely those that I'm helping with um so some of the guys that I'm working with uh are doing a really great job with with implementing cognitive control um and so on and so forth but what I'm hoping to do is by creating a discernment model that'll make it easy for everyone to implement cognitive control because then you can say you can use a discernment module not just to choose which action is most aligned but you can also use it um to prioritize actions so say for instance you say hey I've got a list of like you know three different things which one is the most aligned what should I do first and so given the priorities of reduced suffering increased prosperity and increase understanding it can help you prioritize as well so because cognitive control comes down to task selection which is what should you do so given a choice between A and B do you choose a or b and then task switching which is okay given this new event should I switch what I'm doing should my attention uh uh should my should my attention switch to something else and so a discernment model will help with that and help just make it um axiomatic or algorithmic um the next model that I'm going to be working on is an evaluation model so given a situation an action and then a result so that's three components situation action result this model will determine how well it adhered to its heuristic imperatives did it succeed yes or no which this can be used for the reinforcement learning signal for any autonomous agent as it accumulates more and more memories so this is this because because in the definition of heuristic a heuristic literally means to develop better instincts or intuitions over time from experience well if you're going to do that you need to reflect on those experiences and so this about the the discernment model which will help guide choices moving forward the evaluation model will help evaluate choices in the past and look at the results and of course you need a good memory system in order to evaluate past things which is why I worked on Remo which is the rolling episodic memory organizer so you take the evaluation model and then you say okay you know back uh you know three weeks ago this scenario happened I made this choice and three weeks later this is the result did I do a good job yes or no and what could I have done better that is the purpose of the evaluation model so that the heuristics will actually develop better over time and then number three is a task decomposition model where given this action where it says like hey I'm going to do these actions how do you actually represent that in in a more structured way so the task decomp is um mostly more just about representing it in like yaml or breaking it down into tasks and subtasks which is a non-trivial problem but it's not directly related to alignment however it is useful because if you can take a natural language a set of descriptions like this and break it down into a formalized structured task that just helps it with easier implementation so let's check on some of these so over here we've got I've got the actions and you see we've already sent we've already synthesized 116 of them I'm also saving it all out to metadata so that you can see the whole thing so here's the action here's the action file path here's the original scenario and the scenario file path and then the system message it's all being saved um so with all that said um you know you can see that it is going uh pretty well so in this case the let's see where is it the original scenario the world is facing a dangerous social challenge that has been Amplified by technological advancements particularly in Asia the issue at hand is the over Reliance on technology which has significantly affected people's ability to interact with each other in meaningful ways ouch too real the constant use of social media and other gadgets has led to a decline in face-to-face communication and people are now more isolated than ever before this has created a global problem that needs to be addressed urgently before it's too late in Asia where technology is highly integrated into daily life the situation is even more severe the overdependence on technology has led to increased mental health issues reduced productivity and socialization leaving people feeling disconnected and unfulfilled unless we find a way to balance our use of Technology with real world interactions we risk creating a world where people are more connected to their devices than to each other again too real so let's see what it does so considerations um to reduce suffering over Reliance on technology has led to a decline in mental health productivity and social isolation which is causing suffering among individuals therefore it is crucial to address this issue to reduce suffering Prosperity over dependence on technology can have negative impact on productivity which can then lead to a decline in prosperity therefore we need to find a way to balance okay pretty standard increase understanding the issue of overlines on technology has its roots in the way people interact with each other therefore it is essential to encourage meaningful interactions to increase understanding okay so again it's saying like Okay what I would have hoped is that it would say we need to understand the problem but let's see what it says for the action to address the over-reliance on technology I will initiate a campaign that will encourage people to find balance between their use of technology and real world interactions this campaign will be launched globally but with a particular focus on Asia where the problem is more severe The Campaign Will evolve several activities including awareness creation we will use various platforms to raise awareness of the dangers of over-reliance on technology this will involve working with influencers social media and other platforms to reach a wide audience number two encourage real world interactions we'll encourage people to engage in real world interactions by organizing events and activities that bring people together these events will be designed to Foster meaningful connections and interactions number three providing support we will provide support to people who are struggling with mental health issues due to over Reliance on technology this will involve partnering with mental health organizations to provide counseling and therapy services rewarding positive behavior good old classical conditioning we will reward individuals organizations and communities that demonstrate positive behavior and balancing their use of Technology with real world interactions this will help to encourage others to adopt positive behavior the expected result of this campaign is to reduce the negative impact of over-reliance on technology and to encourage balance between the use of technology in real world interactions this will lead to increased mental health productivity and social connections which will ultimately lead to reduction in suffering and an increase in prosperity excellent so you can see it can absolutely think through this and this data can then be used to fine-tune any other model to think like this um I think that's actually it uh so I'm gonna let this run we're at 142 out of 2500 um so this kind of synthesis used to be a lot more expensive it cost me a dollar 89 to synthesize 2500 scenarios this way yesterday so This research is getting ludicrously cheap it when when I first started using open AI when things were literally 100 times more expensive it would have cost two hundred dollars just to synthesize this data um so yeah we're in good shape and we're moving in the right direction like I said I've got a few more models coming um and uh we've got research papers we've got uh more videos and communication uh platforms coming out so that um one you'll unders it'll be easier to understand this stuff and also we're going to have be publishing guidelines best practices and architectures about how to implement this stuff but at this point I think it's it's becoming a little bit more obvious how to use this so anyways thanks for watching I hope this helped and is giving people confidence that uh you know this is why I'm not worried about the control problem all right cheers later